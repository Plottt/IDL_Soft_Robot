{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmX6adzDL6IV",
        "outputId": "9e34b79d-dc10-4371-d1b2-e8cd2e324de2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so2Ihy3KKMb6",
        "outputId": "419b6eab-92f2-4ca1-9bf3-87d277b79e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import os\n",
        "from torchsummary import summary\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import wandb\n",
        "import torch.nn.functional as F\n",
        "import hashlib\n",
        "from typing import Dict, Tuple\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHpFZC4vKPdS",
        "outputId": "9aceb3a2-5c07-4960-e666-e5a55ca8c52f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vH6wKOAvKMb-"
      },
      "outputs": [],
      "source": [
        "# Configuration Dictionary\n",
        "config = {\n",
        "    'batch_size': 64,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 10,\n",
        "    'input_dim': 9,\n",
        "    'num_classes': 7,\n",
        "    'hidden_dim': 2048,\n",
        "    'num_blocks': 6,\n",
        "    'checkpoint_dir': \"/content/drive/MyDrive/IDL/Checkpoint\",\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5H88G2o5KMb-"
      },
      "outputs": [],
      "source": [
        "# Define category mapping\n",
        "CATEGORIES = {\n",
        "    'Blueball': 0,\n",
        "    'Box': 1,\n",
        "    'Pencilcase': 2,\n",
        "    'Pinkball': 3,\n",
        "    'StuffedAnimal': 4,\n",
        "    'Tennis': 5,\n",
        "    'Waterbottle': 6,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MuINLJVKKMb_"
      },
      "outputs": [],
      "source": [
        "# Path to the folder containing the dataset files\n",
        "folder_path = \"/content/drive/MyDrive/IDL/IDL_Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qXeSE84nKMb_"
      },
      "outputs": [],
      "source": [
        "# Stats trackers\n",
        "total_count = 0\n",
        "kept_count = 0\n",
        "valid_file_count = 0\n",
        "skipped_due_to_missing_waypoints = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FWAborBwKMb_"
      },
      "outputs": [],
      "source": [
        "WAYPOINTS = [\n",
        "    (30, -30), (30, 30), (15, -30), (15, 30),\n",
        "    (0, -30), (0, 30), (-15, -30), (-15, 30),\n",
        "    (-30, -30), (-30, 30), (-30, -30), (30, -30),\n",
        "    (30, 30), (-30, 30)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zVuq0D5TKMb_"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load and label dataset\n",
        "def load_and_label_file(file_path, file_name):\n",
        "    global total_count\n",
        "    category = next((key for key in CATEGORIES if key in file_name), None)\n",
        "    if category is None:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    data = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            if len(parts) == 10:\n",
        "                try:\n",
        "                    timestamp = parts[0]\n",
        "                    microsec = int(parts[1])\n",
        "                    x = float(parts[2])\n",
        "                    y = float(parts[3])\n",
        "                    x_target = float(parts[4])\n",
        "                    y_target = float(parts[5])\n",
        "                    pwm1 = int(parts[6])\n",
        "                    pwm2 = int(parts[7])\n",
        "                    pwm3 = int(parts[8])\n",
        "                    pwm4 = int(parts[9])\n",
        "                    total_count += 1\n",
        "\n",
        "                    data.append([\n",
        "                        timestamp, microsec, x, y, x_target, y_target,\n",
        "                        pwm1, pwm2, pwm3, pwm4, category, CATEGORIES[category]\n",
        "                    ])\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(data, columns=[\n",
        "        \"timestamp\", \"microseconds\", \"x\", \"y\", \"x_target\", \"y_target\",\n",
        "        \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"category\", \"label\"\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "naFDKYoGKMb_"
      },
      "outputs": [],
      "source": [
        "# Step 2: Assign sequential waypoint numbers\n",
        "def assign_sequential_waypoints(df, tol=1.0):\n",
        "    df = df.reset_index(drop=True)\n",
        "    wp_index = 0\n",
        "    assigned_wp = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        x_t, y_t = df.loc[i, \"x_target\"], df.loc[i, \"y_target\"]\n",
        "        current_expected = WAYPOINTS[wp_index]\n",
        "\n",
        "        if np.isclose(x_t, current_expected[0], atol=tol) and np.isclose(y_t, current_expected[1], atol=tol):\n",
        "            assigned_wp.append(wp_index)\n",
        "        else:\n",
        "            if wp_index + 1 < len(WAYPOINTS):\n",
        "                next_expected = WAYPOINTS[wp_index + 1]\n",
        "                if np.isclose(x_t, next_expected[0], atol=tol) and np.isclose(y_t, next_expected[1], atol=tol):\n",
        "                    wp_index += 1\n",
        "                    assigned_wp.append(wp_index)\n",
        "                else:\n",
        "                    assigned_wp.append(wp_index)\n",
        "            else:\n",
        "                assigned_wp.append(wp_index)\n",
        "\n",
        "    df[\"waypoint_number\"] = assigned_wp\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MfXzeHC4KMb_"
      },
      "outputs": [],
      "source": [
        "# Step 3: Filter out rows where y <= 0\n",
        "def filter_by_y(df):\n",
        "    global kept_count\n",
        "    filtered = df[df[\"y\"] > 0].reset_index(drop=True)\n",
        "    kept_count += len(filtered)\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ogu08rbDKMcA"
      },
      "outputs": [],
      "source": [
        "def process_file(file_path, file_name):\n",
        "    global valid_file_count\n",
        "\n",
        "    # Step 1: Load and label\n",
        "    df = load_and_label_file(file_path, file_name)\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Step 2: Assign waypoint numbers\n",
        "    df = assign_sequential_waypoints(df)\n",
        "\n",
        "    # 📌 Show how many waypoints existed before filtering\n",
        "    waypoint_count_before = df[\"waypoint_number\"].nunique()\n",
        "    print(f\"\\n📌 {file_name} → {waypoint_count_before} waypoints BEFORE filtering\")\n",
        "\n",
        "    # Step 3: Filter out rows where y ≤ 0\n",
        "    df = filter_by_y(df)\n",
        "\n",
        "    # 📌 Show how many remain after filtering\n",
        "    waypoint_count_after = df[\"waypoint_number\"].nunique()\n",
        "    print(f\"📌 {file_name} → {waypoint_count_after} waypoints AFTER filtering\")\n",
        "\n",
        "    # Count as valid if any data was kept\n",
        "    if not df.empty:\n",
        "        valid_file_count += 1\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CBO_ta8KMcA",
        "outputId": "148751d4-e158-4ced-ca21-772d271953a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📌 Pinkball2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball2.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball6.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball6.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle2.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle4.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal7.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball5.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase5.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball11.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball11.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal4.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal4.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball5.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis4.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis1.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis2.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis3.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball11.txt → 10 waypoints BEFORE filtering\n",
            "📌 Blueball11.txt → 9 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase3.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Box7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal6.txt → 13 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box2.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Box5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box5.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball1.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle5.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box4.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal8.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball3.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase4.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase2.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal10.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball2.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal5.txt → 10 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal5.txt → 10 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box3.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball3.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball4.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle3.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal9.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball4.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal1.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal1.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal2.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal2.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal3.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal3.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis5.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle1.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📄 Summary:\n",
            "Total files scanned: 71\n",
            "✅ Files with 14 valid waypoints: 71\n",
            "⚠️ Skipped due to missing waypoints: 0\n",
            "📊 Data points before filtering: 360590\n",
            "✅ Data points after filtering: 268721\n",
            "🚫 Dropped data points: 91869\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Process all .txt files\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if not file_name.endswith(\".txt\") or file_name.startswith(\".\"):\n",
        "        continue\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = process_file(file_path, file_name)\n",
        "    if not df.empty:\n",
        "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "\n",
        "# Step 6: Summary\n",
        "print(\"\\n📄 Summary:\")\n",
        "print(f\"Total files scanned: {len([f for f in os.listdir(folder_path) if f.endswith('.txt')])}\")\n",
        "print(f\"✅ Files with 14 valid waypoints: {valid_file_count}\")\n",
        "print(f\"⚠️ Skipped due to missing waypoints: {skipped_due_to_missing_waypoints}\")\n",
        "print(f\"📊 Data points before filtering: {total_count}\")\n",
        "print(f\"✅ Data points after filtering: {kept_count}\")\n",
        "print(f\"🚫 Dropped data points: {total_count - kept_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8u06Sh1KMcA",
        "outputId": "d321200d-8099-4b75-c857-8239ffe526b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                timestamp  microseconds     x      y  x_target  y_target  \\\n",
            "0     2025-02-28 14:25:35     328046152 -1.27   1.84      30.0     -30.0   \n",
            "1     2025-02-28 14:25:35     328099628 -1.27   1.84      30.0     -30.0   \n",
            "2     2025-02-28 14:25:35            68 -1.27   1.84      30.0     -30.0   \n",
            "3     2025-02-28 14:25:35     328046152 -1.27   1.84      30.0     -30.0   \n",
            "4     2025-02-28 14:25:35     328099628 -1.27   1.84      30.0     -30.0   \n",
            "...                   ...           ...   ...    ...       ...       ...   \n",
            "1345  2025-02-28 14:26:40      64985356  0.64  27.98       0.0      30.0   \n",
            "1346  2025-02-28 14:26:40      65007184  0.64  27.98       0.0      30.0   \n",
            "1347  2025-02-28 14:26:40      65029016  0.64  27.98       0.0      30.0   \n",
            "1348  2025-02-28 14:26:40      65050844  0.64  27.98       0.0      30.0   \n",
            "1349  2025-02-28 14:26:41      65072664  0.64  27.98       0.0      30.0   \n",
            "\n",
            "      pwm1  pwm2  pwm3  pwm4  category  label  waypoint_number  \n",
            "0       80     0    80     0  Pinkball      3                0  \n",
            "1       80     0    80     0  Pinkball      3                0  \n",
            "2       80     0    80     0  Pinkball      3                0  \n",
            "3       80     0    80     0  Pinkball      3                0  \n",
            "4       80     0    80     0  Pinkball      3                0  \n",
            "...    ...   ...   ...   ...       ...    ...              ...  \n",
            "1345     0    40     0    40  Pinkball      3                5  \n",
            "1346     0    40     0    40  Pinkball      3                5  \n",
            "1347     0    40     0    40  Pinkball      3                5  \n",
            "1348     0    40     0    40  Pinkball      3                5  \n",
            "1349     0    40     0    40  Pinkball      3                5  \n",
            "\n",
            "[1350 rows x 13 columns]\n"
          ]
        }
      ],
      "source": [
        "print(all_data.head(1350))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yuHhrzRwKMcB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6qjmfJ-JKMcB"
      },
      "outputs": [],
      "source": [
        "class ObjectSensorDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        features = df[[\"x\", \"y\", \"x_target\", \"y_target\", \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"microseconds\"]].values\n",
        "        labels = df[\"label\"].values\n",
        "\n",
        "        self.X = torch.tensor(features, dtype=torch.float32)\n",
        "        self.y = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO8htFsSKMcB",
        "outputId": "57b9cc54-6bbd-4373-eea6-75b8bffe02e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔁 Training: 100%|██████████| 2940/2940 [00:01<00:00, 2080.04it/s]\n",
            "🔍 Validating: 100%|██████████| 630/630 [00:00<00:00, 2275.95it/s]\n",
            "🧪 Testing: 100%|██████████| 630/630 [00:00<00:00, 1522.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Dataset Sizes:\n",
            "🧠 Training set: 188104 samples\n",
            "🧪 Validation set: 40308 samples\n",
            "🧾 Test set: 40309 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Normalize features before split\n",
        "features_to_scale = [\"x\", \"y\", \"x_target\", \"y_target\", \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"microseconds\"]\n",
        "scaler = StandardScaler()\n",
        "all_data[features_to_scale] = scaler.fit_transform(all_data[features_to_scale])\n",
        "\n",
        "# 🧪 Split into train/val/test with stratified sampling\n",
        "train_df, temp_df = train_test_split(\n",
        "    all_data, test_size=0.3, stratify=all_data[\"label\"], random_state=42\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42\n",
        ")\n",
        "\n",
        "# 📦 Create datasets\n",
        "train_dataset = ObjectSensorDataset(train_df)\n",
        "val_dataset = ObjectSensorDataset(val_df)\n",
        "test_dataset = ObjectSensorDataset(test_df)\n",
        "\n",
        "# 📦 Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# 🧪 Example usage with tqdm (dummy training loop)\n",
        "for X_batch, y_batch in tqdm(train_loader, desc=\"🔁 Training\"):\n",
        "    # Simulate training step\n",
        "    pass\n",
        "\n",
        "for X_batch, y_batch in tqdm(val_loader, desc=\"🔍 Validating\"):\n",
        "    # Simulate validation step\n",
        "    pass\n",
        "\n",
        "for X_batch, y_batch in tqdm(test_loader, desc=\"🧪 Testing\"):\n",
        "    # Simulate test step\n",
        "    pass\n",
        "\n",
        "# 📊 Summary of dataset sizes\n",
        "print(\"\\n📊 Dataset Sizes:\")\n",
        "print(f\"🧠 Training set: {len(train_dataset)} samples\")\n",
        "print(f\"🧪 Validation set: {len(val_dataset)} samples\")\n",
        "print(f\"🧾 Test set: {len(test_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvgzj8OfKMcB",
        "outputId": "3013b13f-0b79-45b0-fff0-654178ec9438"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResidualMLPClassifier                    [64, 7]                   --\n",
              "├─Sequential: 1-1                        [64, 2048]                --\n",
              "│    └─Linear: 2-1                       [64, 2048]                20,480\n",
              "│    └─BatchNorm1d: 2-2                  [64, 2048]                4,096\n",
              "│    └─ReLU: 2-3                         [64, 2048]                --\n",
              "├─Sequential: 1-2                        [64, 2048]                --\n",
              "│    └─ResidualBlock: 2-4                [64, 2048]                --\n",
              "│    │    └─Sequential: 3-1              [64, 2048]                8,400,896\n",
              "│    │    └─ReLU: 3-2                    [64, 2048]                --\n",
              "│    └─ResidualBlock: 2-5                [64, 2048]                --\n",
              "│    │    └─Sequential: 3-3              [64, 2048]                8,400,896\n",
              "│    │    └─ReLU: 3-4                    [64, 2048]                --\n",
              "│    └─ResidualBlock: 2-6                [64, 2048]                --\n",
              "│    │    └─Sequential: 3-5              [64, 2048]                8,400,896\n",
              "│    │    └─ReLU: 3-6                    [64, 2048]                --\n",
              "│    └─ResidualBlock: 2-7                [64, 2048]                --\n",
              "│    │    └─Sequential: 3-7              [64, 2048]                8,400,896\n",
              "│    │    └─ReLU: 3-8                    [64, 2048]                --\n",
              "│    └─ResidualBlock: 2-8                [64, 2048]                --\n",
              "│    │    └─Sequential: 3-9              [64, 2048]                8,400,896\n",
              "│    │    └─ReLU: 3-10                   [64, 2048]                --\n",
              "│    └─ResidualBlock: 2-9                [64, 2048]                --\n",
              "│    │    └─Sequential: 3-11             [64, 2048]                8,400,896\n",
              "│    │    └─ReLU: 3-12                   [64, 2048]                --\n",
              "├─Sequential: 1-3                        [64, 7]                   --\n",
              "│    └─Dropout: 2-10                     [64, 2048]                --\n",
              "│    └─Linear: 2-11                      [64, 512]                 1,049,088\n",
              "│    └─BatchNorm1d: 2-12                 [64, 512]                 1,024\n",
              "│    └─ReLU: 2-13                        [64, 512]                 --\n",
              "│    └─Linear: 2-14                      [64, 7]                   3,591\n",
              "==========================================================================================\n",
              "Total params: 51,483,655\n",
              "Trainable params: 51,483,655\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 3.29\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 27.79\n",
              "Params size (MB): 205.93\n",
              "Estimated Total Size (MB): 233.73\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.BatchNorm1d(dim),\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.block(x)\n",
        "        out += identity\n",
        "        return self.relu(out)\n",
        "\n",
        "\n",
        "class ResidualMLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, hidden_dim=2048, num_blocks=6):\n",
        "        super(ResidualMLPClassifier, self).__init__()\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.res_blocks = nn.Sequential(*[ResidualBlock(hidden_dim) for _ in range(num_blocks)])\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.res_blocks(x)\n",
        "        feats = x\n",
        "        out = self.classifier(x)\n",
        "        return {\"feats\": feats, \"out\": out}\n",
        "\n",
        "model = ResidualMLPClassifier(input_dim=9, num_classes=7, hidden_dim = config['hidden_dim'], num_blocks = config['num_blocks']).to(config['device'])\n",
        "summary(model, input_size=(64, 9))  # for batch size 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2VElzMZvKMcB"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EiPYAO1zKMcB"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "DLwyZLQLKMcB"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, y = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs['out'], y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = accuracy(outputs['out'], y)[0].item()\n",
        "        loss_m.update(loss.item())\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(loss_m.avg)),\n",
        "            acc=\"{:.04f}%\".format(float(acc_m.avg)),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    return loss_m.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VZO5iNDzKMcC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate_model(model, val_loader, criterion, class_names, device):\n",
        "    model.eval()\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "        x, y = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs['out'], y)\n",
        "\n",
        "        acc = accuracy(outputs['out'], y)[0].item()\n",
        "\n",
        "        _, predicted = torch.max(outputs['out'], 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "        loss_m.update(loss.item())\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(loss_m.avg)),\n",
        "            acc=\"{:.04f}%\".format(float(acc_m.avg))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    if class_names:\n",
        "        print(\"\\nPer-class Validation Accuracy:\")\n",
        "        per_class_acc = {}\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_mask = (np.array(all_targets) == i)\n",
        "            if np.sum(class_mask) > 0:\n",
        "                class_correct = np.sum((np.array(all_preds)[class_mask] == i))\n",
        "                class_total = np.sum(class_mask)\n",
        "                acc_percent = 100 * class_correct / class_total\n",
        "                print(f\"  {class_name}: {acc_percent:.4f}% ({class_correct}/{class_total})\")\n",
        "                per_class_acc[f\"val_acc_{class_name}\"] = acc_percent\n",
        "\n",
        "    return loss_m.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "g0vNzj8FKMcC"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'metrics': metrics\n",
        "    }, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3naM25ADKMcC",
        "outputId": "3ca72727-c6fa-49bf-c32d-ff3aa05d2997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define CrossEntropyLoss as the criterion\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    label_smoothing=0.1\n",
        ")\n",
        "\n",
        "# Initialize optimizer with AdamW\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    weight_decay=1e-5\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWEXTMbuKMcC",
        "outputId": "5d77835d-b300-425c-a950-826b58b23a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdonggul\u001b[0m (\u001b[33mdonggul-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Intialize wandb\n",
        "wandb.login(key=\"78d5988d9f05a421bc74d044c3cd9afc3b918020\") # API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "gon3xS9EKMcC",
        "outputId": "b1d24c6b-7fcf-41ed-c660-48733b2f8da1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250330_230612-ewb6z296</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/ewb6z296' target=\"_blank\">15run</a></strong> to <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/ewb6z296' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/ewb6z296</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"15run\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = False, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id = \"\", ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"object_classification\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8hA-tExKMcC",
        "outputId": "c07cdb17-3113-4063-c4aa-98981c05c075"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1262, Train Accuracy: 64.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 63.9127% (3721/5822)\n",
            "  Box: 59.3361% (3289/5543)\n",
            "  Pencilcase: 59.1996% (3195/5397)\n",
            "  Pinkball: 52.7701% (3067/5812)\n",
            "  StuffedAnimal: 60.8573% (3677/6042)\n",
            "  Tennis: 64.1971% (3830/5966)\n",
            "  Waterbottle: 57.0730% (3268/5726)\n",
            "Validation Loss: 1.2424, Validation Accuracy: 59.66%\n",
            "Saved best model with validation loss: 1.2424 and accuracy: 59.66%\n",
            "Saved model for epoch 1\n",
            "End of Epoch 1/10\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   4%|▍         | 124/2940 [00:06<02:14, 20.98it/s, acc=65.5368%, loss=1.0994, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0997, Train Accuracy: 66.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 45.3624% (2641/5822)\n",
            "  Box: 36.7851% (2039/5543)\n",
            "  Pencilcase: 45.9515% (2480/5397)\n",
            "  Pinkball: 41.8100% (2430/5812)\n",
            "  StuffedAnimal: 47.9311% (2896/6042)\n",
            "  Tennis: 42.0717% (2510/5966)\n",
            "  Waterbottle: 46.5246% (2664/5726)\n",
            "Validation Loss: 1.7116, Validation Accuracy: 43.81%\n",
            "Saved model for epoch 2\n",
            "End of Epoch 2/10\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   3%|▎         | 83/2940 [00:03<01:46, 26.83it/s, acc=69.7477%, loss=1.0330, lr=0.000500]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0142, Train Accuracy: 70.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 69.1000% (4023/5822)\n",
            "  Box: 64.9287% (3599/5543)\n",
            "  Pencilcase: 65.3326% (3526/5397)\n",
            "  Pinkball: 70.1480% (4077/5812)\n",
            "  StuffedAnimal: 70.9202% (4285/6042)\n",
            "  Tennis: 73.4831% (4384/5966)\n",
            "  Waterbottle: 67.6039% (3871/5726)\n",
            "Validation Loss: 1.0444, Validation Accuracy: 68.88%\n",
            "Saved best model with validation loss: 1.0444 and accuracy: 68.88%\n",
            "Saved model for epoch 3\n",
            "End of Epoch 3/10\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   0%|          | 4/2940 [00:00<03:38, 13.44it/s, acc=74.3750%, loss=0.9438, lr=0.000500]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9949, Train Accuracy: 71.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 68.0179% (3960/5822)\n",
            "  Box: 66.6787% (3696/5543)\n",
            "  Pencilcase: 69.5572% (3754/5397)\n",
            "  Pinkball: 66.7584% (3880/5812)\n",
            "  StuffedAnimal: 67.7921% (4096/6042)\n",
            "  Tennis: 67.3148% (4016/5966)\n",
            "  Waterbottle: 68.9487% (3948/5726)\n",
            "Validation Loss: 1.0825, Validation Accuracy: 67.85%\n",
            "Saved model for epoch 4\n",
            "End of Epoch 4/10\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   6%|▌         | 181/2940 [00:08<01:41, 27.14it/s, acc=72.6992%, loss=0.9751, lr=0.000500]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9867, Train Accuracy: 72.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 67.5541% (3933/5822)\n",
            "  Box: 62.5293% (3466/5543)\n",
            "  Pencilcase: 64.0170% (3455/5397)\n",
            "  Pinkball: 70.1652% (4078/5812)\n",
            "  StuffedAnimal: 66.4019% (4012/6042)\n",
            "  Tennis: 63.6775% (3799/5966)\n",
            "  Waterbottle: 65.5781% (3755/5726)\n",
            "Validation Loss: 1.1283, Validation Accuracy: 65.74%\n",
            "Saved model for epoch 5\n",
            "End of Epoch 5/10\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   7%|▋         | 204/2940 [00:09<02:16, 20.11it/s, acc=72.3269%, loss=0.9858, lr=0.000500]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9719, Train Accuracy: 73.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 70.5084% (4105/5822)\n",
            "  Box: 63.2870% (3508/5543)\n",
            "  Pencilcase: 63.3130% (3417/5397)\n",
            "  Pinkball: 65.9842% (3835/5812)\n",
            "  StuffedAnimal: 66.4846% (4017/6042)\n",
            "  Tennis: 67.7673% (4043/5966)\n",
            "  Waterbottle: 60.1816% (3446/5726)\n",
            "Validation Loss: 1.1541, Validation Accuracy: 65.42%\n",
            "Saved model for epoch 6\n",
            "End of Epoch 6/10\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   0%|          | 14/2940 [00:00<03:22, 14.43it/s, acc=71.7634%, loss=0.9829, lr=0.000500]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9662, Train Accuracy: 73.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 58.3476% (3397/5822)\n",
            "  Box: 44.3803% (2460/5543)\n",
            "  Pencilcase: 42.4866% (2293/5397)\n",
            "  Pinkball: 41.6552% (2421/5812)\n",
            "  StuffedAnimal: 51.7047% (3124/6042)\n",
            "  Tennis: 60.9286% (3635/5966)\n",
            "  Waterbottle: 50.3667% (2884/5726)\n",
            "Validation Loss: 1.6209, Validation Accuracy: 50.15%\n",
            "Saved model for epoch 7\n",
            "End of Epoch 7/10\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   1%|▏         | 39/2940 [00:01<02:19, 20.76it/s, acc=75.5208%, loss=0.9177, lr=0.000250]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9143, Train Accuracy: 76.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 49.6221% (2889/5822)\n",
            "  Box: 47.6998% (2644/5543)\n",
            "  Pencilcase: 48.5640% (2621/5397)\n",
            "  Pinkball: 39.4357% (2292/5812)\n",
            "  StuffedAnimal: 48.7918% (2948/6042)\n",
            "  Tennis: 51.3912% (3066/5966)\n",
            "  Waterbottle: 49.8603% (2855/5726)\n",
            "Validation Loss: 1.7095, Validation Accuracy: 47.92%\n",
            "Saved model for epoch 8\n",
            "End of Epoch 8/10\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   2%|▏         | 52/2940 [00:03<02:02, 23.63it/s, acc=77.3738%, loss=0.8969, lr=0.000250]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9067, Train Accuracy: 76.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 49.0210% (2854/5822)\n",
            "  Box: 44.3442% (2458/5543)\n",
            "  Pencilcase: 47.6932% (2574/5397)\n",
            "  Pinkball: 41.3283% (2402/5812)\n",
            "  StuffedAnimal: 48.0636% (2904/6042)\n",
            "  Tennis: 55.7828% (3328/5966)\n",
            "  Waterbottle: 48.1663% (2758/5726)\n",
            "Validation Loss: 1.7840, Validation Accuracy: 47.82%\n",
            "Saved model for epoch 9\n",
            "End of Epoch 9/10\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   3%|▎         | 86/2940 [00:04<02:40, 17.74it/s, acc=77.6401%, loss=0.8855, lr=0.000250]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 8 that is less than the current step 9. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8987, Train Accuracy: 76.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 71.7108% (4175/5822)\n",
            "  Box: 69.0240% (3826/5543)\n",
            "  Pencilcase: 67.9452% (3667/5397)\n",
            "  Pinkball: 71.8858% (4178/5812)\n",
            "  StuffedAnimal: 72.4429% (4377/6042)\n",
            "  Tennis: 71.7901% (4283/5966)\n",
            "  Waterbottle: 69.5599% (3983/5726)\n",
            "Validation Loss: 1.0556, Validation Accuracy: 70.68%\n",
            "Saved model for epoch 10\n",
            "End of Epoch 10/10\n",
            "\n",
            "Training complete! Best validation accuracy: 68.88%\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "best_val_loss = float('inf')\n",
        "best_val_acc = 0\n",
        "class_names = list(CATEGORIES.keys())\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "\n",
        "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, config['device'])\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    val_loss, val_acc = validate_model(model, val_loader, criterion, class_names, config['device'])\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    curr_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_val_acc = val_acc\n",
        "        best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "        }, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(f\"Saved best model with validation loss: {best_val_loss:.4f} and accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    last_model_path = os.path.join(config['checkpoint_dir'], f'model_epoch_{epoch+1}.pth')\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "    wandb.save(last_model_path)\n",
        "    print(f\"Saved model for epoch {epoch+1}\")\n",
        "\n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'learning_rate': curr_lr\n",
        "    }, step=epoch)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}/{config['epochs']}\")\n",
        "\n",
        "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "prB3uz0aKMcC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_model(model, test_loader, criterion, class_names, device, checkpoint_dir=None):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "    class_correct = {class_name: 0 for class_name in class_names}\n",
        "    class_total = {class_name: 0 for class_name in class_names}\n",
        "    test_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\", ncols=100)\n",
        "\n",
        "    for data in test_bar:\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        outputs_for_loss = outputs['out'] if isinstance(outputs, dict) and 'out' in outputs else outputs\n",
        "        loss = criterion(outputs_for_loss, targets)\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        probs = torch.nn.functional.softmax(outputs_for_loss, dim=1)\n",
        "        _, predicted = torch.max(outputs_for_loss, 1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "        for i in range(targets.size(0)):\n",
        "            label = targets[i].item()\n",
        "            pred = predicted[i].item()\n",
        "            class_name = class_names[label]\n",
        "            class_total[class_name] += 1\n",
        "            if pred == label:\n",
        "                class_correct[class_name] += 1\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "        test_bar.set_postfix({\n",
        "            'loss': f\"{test_loss/total:.4f}\",\n",
        "            'acc': f\"{100.0*correct/total:.2f}%\"\n",
        "        })\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = correct / total\n",
        "\n",
        "    class_accuracy = {name: class_correct[name]/class_total[name] if class_total[name] > 0 else 0 for name in class_names}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f} ({correct}/{total})\")\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    for class_name in class_names:\n",
        "        print(f\" {class_name}: {class_accuracy[class_name]:.4f} ({class_correct[class_name]}/{class_total[class_name]})\")\n",
        "\n",
        "    return {\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_acc,\n",
        "        'class_accuracy': class_accuracy,\n",
        "        'predictions': all_preds,\n",
        "        'targets': all_targets,\n",
        "        'probabilities': all_probs\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyEM7gzIKMcD",
        "outputId": "d613ff10-ad02-481c-cffb-6cbe7513c3fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best model from epoch 2\n"
          ]
        }
      ],
      "source": [
        "best_model_path = f\"{config['checkpoint_dir']}/best_model.pth\"\n",
        "if os.path.exists(best_model_path):\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint.get('epoch', 'unknown')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm_YytY1KMcD",
        "outputId": "97226e0d-a9fe-4f43-c3d6-57b1f9eaa48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|███████████████████████| 630/630 [00:04<00:00, 138.35batch/s, loss=1.0455, acc=68.74%]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TEST RESULTS\n",
            "==================================================\n",
            "Test Loss: 1.0455\n",
            "Test Accuracy: 0.6874 (27708/40309)\n",
            "\n",
            "Per-Class Accuracy:\n",
            " Blueball: 0.6846 (3986/5822)\n",
            " Box: 0.6533 (3621/5543)\n",
            " Pencilcase: 0.6717 (3625/5397)\n",
            " Pinkball: 0.6999 (4068/5812)\n",
            " StuffedAnimal: 0.6925 (4184/6042)\n",
            " Tennis: 0.7295 (4352/5966)\n",
            " Waterbottle: 0.6761 (3872/5727)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on test set\n",
        "test_results = test_model(\n",
        "    model=model,\n",
        "    test_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    class_names=class_names,\n",
        "    device=device,\n",
        "    checkpoint_dir=config['checkpoint_dir']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mldq_w9VKMcD"
      },
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}