{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SIbafSTASQH"
      },
      "source": [
        "# Contact-based Object Classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hXmX1r8gAVj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8552ac8-6e91-4ca2-a2c0-7c1a1dc4a83e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "uzYH1vB7ASQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c224a5af-4f72-456b-89f8-55adb7be175b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import os\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import wandb\n",
        "import torch.nn.functional as F\n",
        "import hashlib\n",
        "from typing import Dict, Tuple\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TQ2pWDvyASQI"
      },
      "outputs": [],
      "source": [
        "# Categories for classification\n",
        "CATEGORIES = {\n",
        "    'Blueball': 0,\n",
        "    'Box': 1,\n",
        "    'Pencilcase': 2,\n",
        "    'Pinkball': 3,\n",
        "    'StuffedAnimal': 4,\n",
        "    'Tennis': 5,\n",
        "    'Waterbottle': 6,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CxXpK7nZASQI"
      },
      "outputs": [],
      "source": [
        "# Configuration Dictionary\n",
        "config = {\n",
        "    'batch_size': 32,\n",
        "    'lr': 0.001,\n",
        "    'epochs': 20,\n",
        "    'data_dir': \"/content/drive/MyDrive/IDL/IDL_Data\",\n",
        "    'checkpoint_dir': \"/content/drive/MyDrive/IDL/Checkpoint\",\n",
        "    'cleaned_dir': \"/content/drive/MyDrive/IDL/IDL_Cleaned\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class ContactDatapointDataset(Dataset):\n",
        "#     def __init__(self, data_dir: str, labels: dict = None):\n",
        "#         self.data_dir = Path(data_dir)\n",
        "#         self.file_paths = list(self.data_dir.glob(\"*.txt\"))\n",
        "#         self.label_map = labels or {}\n",
        "\n",
        "#         self.features_list = []\n",
        "#         self.labels_list = []\n",
        "#         self.file_indices = []\n",
        "\n",
        "#         print(\"Processing files and extracting datapoints:\")\n",
        "#         for file_idx, file_path in enumerate(self.file_paths):\n",
        "#             try:\n",
        "#                 category = re.sub(r\"\\d+\", \"\", file_path.stem)\n",
        "#                 label = self.label_map.get(category, -1)\n",
        "\n",
        "#                 if label == -1:\n",
        "#                     print(f\"⚠️ Skipping unknown category: {file_path.name} (parsed: '{category}')\")\n",
        "#                     continue\n",
        "\n",
        "#                 df = self._parse_file(file_path)\n",
        "\n",
        "#                 for i in range(len(df)):\n",
        "#                     features = self._extract_features(df.iloc[i])\n",
        "#                     self.features_list.append(features)\n",
        "#                     self.labels_list.append(label)\n",
        "#                     self.file_indices.append(file_idx)\n",
        "\n",
        "#                 print(f\"  {file_path.name}: {len(df)} datapoints\")\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"❌ Error processing {file_path.name}: {e}\")\n",
        "\n",
        "#         self.features = torch.FloatTensor(self.features_list)\n",
        "#         self.labels = torch.LongTensor(self.labels_list)\n",
        "#         self.file_indices = torch.LongTensor(self.file_indices)\n",
        "#         self._print_dataset_stats()\n",
        "\n",
        "#     def _parse_file(self, file_path: Path) -> pd.DataFrame:\n",
        "#         df = pd.read_csv(file_path, header=None, skiprows=1)\n",
        "#         columns = [\n",
        "#             'timestamp_pc', 'timestamp_micro', 'x', 'y', 'angle_1', 'angle_2',\n",
        "#             'contact_1_left', 'contact_1_right', 'contact_2_left', 'contact_2_right'\n",
        "#         ]\n",
        "#         df = pd.DataFrame(df.values, columns=columns)\n",
        "#         df = df[df['y'] >= 0].reset_index(drop=True)\n",
        "#         return df\n",
        "\n",
        "#     def _extract_features(self, row: pd.Series) -> np.ndarray:\n",
        "#         return np.array([\n",
        "#             row['contact_1_left'], row['contact_1_right'],\n",
        "#             row['contact_2_left'], row['contact_2_right'],\n",
        "#             row['x'], row['y'], row['angle_1'], row['angle_2']\n",
        "#         ])\n",
        "\n",
        "#     def _print_dataset_stats(self):\n",
        "#         total = len(self.features)\n",
        "#         unique_files = len(torch.unique(self.file_indices))\n",
        "#         category_counts = dict(Counter(self.labels_list))\n",
        "#         print(\"\\nDataset Statistics:\")\n",
        "#         print(f\"Total datapoints: {total}\")\n",
        "#         print(f\"Total files: {unique_files}\")\n",
        "#         for label, count in category_counts.items():\n",
        "#             name = [k for k, v in self.label_map.items() if v == label][0]\n",
        "#             print(f\"  {name}: {count} datapoints\")\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.features)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "_xxt8FuAUrVe"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def split_dataset_by_files_robust(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
        "#     random.seed(random_state)\n",
        "#     np.random.seed(random_state)\n",
        "\n",
        "#     assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10\n",
        "\n",
        "#     unique_file_indices = torch.unique(dataset.file_indices).numpy()\n",
        "#     file_to_label = {}\n",
        "#     for file_idx in unique_file_indices:\n",
        "#         mask = dataset.file_indices == file_idx\n",
        "#         window_indices = torch.where(mask)[0]\n",
        "#         labels = dataset.labels[window_indices]\n",
        "#         unique_labels, counts = torch.unique(labels, return_counts=True)\n",
        "#         most_common_label = unique_labels[torch.argmax(counts)].item()\n",
        "#         file_to_label[file_idx] = most_common_label\n",
        "\n",
        "#     label_to_files = {}\n",
        "#     for file_idx, label in file_to_label.items():\n",
        "#         if label not in label_to_files:\n",
        "#             label_to_files[label] = []\n",
        "#         label_to_files[label].append(file_idx)\n",
        "\n",
        "#     train_files, val_files, test_files = [], [], []\n",
        "#     for label, files in label_to_files.items():\n",
        "#         random.shuffle(files)\n",
        "#         num_files = len(files)\n",
        "\n",
        "#         if num_files >= 3:\n",
        "#             num_train = max(1, int(num_files * train_ratio))\n",
        "#             num_val = max(1, int(num_files * val_ratio))\n",
        "#             num_test = max(1, num_files - num_train - num_val)\n",
        "\n",
        "#             if num_train + num_val + num_test > num_files:\n",
        "#                 if num_files == 2:\n",
        "#                     num_train, num_val, num_test = 1, 0, 1\n",
        "#                 else:\n",
        "#                     num_train = max(1, num_files - 2)\n",
        "#                     num_val, num_test = 1, 1\n",
        "\n",
        "#             train_files.extend(files[:num_train])\n",
        "#             val_files.extend(files[num_train:num_train+num_val])\n",
        "#             test_files.extend(files[num_train+num_val:])\n",
        "\n",
        "#         elif num_files == 2:\n",
        "#             train_files.append(files[0])\n",
        "#             val_files.append(files[1])\n",
        "\n",
        "#         elif num_files == 1:\n",
        "#             train_files.append(files[0])\n",
        "\n",
        "#     train_indices, val_indices, test_indices = [], [], []\n",
        "#     for idx in range(len(dataset)):\n",
        "#         file_idx = dataset.file_indices[idx].item()\n",
        "#         if file_idx in train_files:\n",
        "#             train_indices.append(idx)\n",
        "#         elif file_idx in val_files:\n",
        "#             val_indices.append(idx)\n",
        "#         elif file_idx in test_files:\n",
        "#             test_indices.append(idx)\n",
        "\n",
        "#     return train_indices, val_indices, test_indices"
      ],
      "metadata": {
        "id": "a1fKSI-wamz5"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_dataloaders(data_dir, categories, batch_size=64, seed=42):\n",
        "#     dataset = ContactDatapointDataset(data_dir=data_dir, labels=categories)\n",
        "#     train_indices, val_indices, test_indices = split_dataset_by_files_robust(dataset, random_state=seed)\n",
        "\n",
        "#     train_subset = Subset(dataset, train_indices)\n",
        "#     train_labels = [dataset.labels[i] for i in train_indices]\n",
        "#     class_sample_count = torch.tensor([train_labels.count(i) for i in range(len(categories))], dtype=torch.float)\n",
        "#     weights = 1. / class_sample_count\n",
        "#     samples_weight = torch.tensor([weights[t] for t in train_labels])\n",
        "#     sampler = torch.utils.data.WeightedRandomSampler(weights=samples_weight, num_samples=len(samples_weight), replacement=True)\n",
        "\n",
        "#     train_loader = DataLoader(train_subset, batch_size=batch_size, sampler=sampler)\n",
        "#     val_loader = DataLoader(Subset(dataset, val_indices), batch_size=batch_size)\n",
        "#     test_loader = DataLoader(Subset(dataset, test_indices), batch_size=batch_size)\n",
        "\n",
        "#     print(f\"\\nFinal dataset splits:\")\n",
        "#     print(f\"Train: {len(train_indices)} samples\")\n",
        "#     print(f\"Validation: {len(val_indices)} samples\")\n",
        "#     print(f\"Test: {len(test_indices)} samples\")\n",
        "\n",
        "#     return train_loader, val_loader, test_loader, weights.to(torch.float32)"
      ],
      "metadata": {
        "id": "FfZfG-a0Zk9C"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader, val_loader, test_loader, class_weights = create_dataloaders(\n",
        "#     data_dir=config['data_dir'],\n",
        "#     categories=CATEGORIES,         # Your label mapping dictionary\n",
        "#     batch_size=config['batch_size'],\n",
        "# )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8edmmqAsVRYR",
        "outputId": "217ed14e-770a-4ee6-97b0-13cde733a761"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing files and extracting datapoints:\n",
            "  Pinkball2.txt: 3418 datapoints\n",
            "  Pinkball6.txt: 3396 datapoints\n",
            "  Blueball6.txt: 3504 datapoints\n",
            "  Waterbottle2.txt: 3343 datapoints\n",
            "  Waterbottle4.txt: 4129 datapoints\n",
            "  Blueball10.txt: 4315 datapoints\n",
            "  StuffedAnimal7.txt: 3852 datapoints\n",
            "  Pinkball5.txt: 3547 datapoints\n",
            "  Waterbottle10.txt: 3888 datapoints\n",
            "  Tennis7.txt: 3886 datapoints\n",
            "  Waterbottle8.txt: 3638 datapoints\n",
            "  Pencilcase5.txt: 4046 datapoints\n",
            "  Pinkball11.txt: 4054 datapoints\n",
            "  StuffedAnimal4.txt: 3760 datapoints\n",
            "  Blueball5.txt: 3512 datapoints\n",
            "  Pencilcase10.txt: 4014 datapoints\n",
            "  Tennis4.txt: 4058 datapoints\n",
            "  Tennis9.txt: 3508 datapoints\n",
            "  Blueball7.txt: 3613 datapoints\n",
            "  Tennis1.txt: 3805 datapoints\n",
            "  Waterbottle7.txt: 3780 datapoints\n",
            "  Tennis2.txt: 5052 datapoints\n",
            "  Blueball9.txt: 3498 datapoints\n",
            "  Tennis6.txt: 3729 datapoints\n",
            "  Box10.txt: 3373 datapoints\n",
            "  Tennis3.txt: 3764 datapoints\n",
            "  Pinkball9.txt: 3389 datapoints\n",
            "  Blueball11.txt: 3064 datapoints\n",
            "  Pencilcase3.txt: 4660 datapoints\n",
            "  Box7.txt: 4062 datapoints\n",
            "  StuffedAnimal6.txt: 4963 datapoints\n",
            "  Box8.txt: 3559 datapoints\n",
            "  Waterbottle9.txt: 4405 datapoints\n",
            "  Box2.txt: 3467 datapoints\n",
            "  Box5.txt: 4090 datapoints\n",
            "  Pencilcase8.txt: 4080 datapoints\n",
            "  Blueball1.txt: 3316 datapoints\n",
            "  Waterbottle5.txt: 3578 datapoints\n",
            "  Pencilcase9.txt: 3387 datapoints\n",
            "  Pinkball8.txt: 3542 datapoints\n",
            "  Box4.txt: 3635 datapoints\n",
            "  Blueball8.txt: 3569 datapoints\n",
            "  Tennis10.txt: 4417 datapoints\n",
            "  StuffedAnimal8.txt: 4261 datapoints\n",
            "  Box1.txt: 3770 datapoints\n",
            "  Blueball3.txt: 3395 datapoints\n",
            "  Pencilcase4.txt: 3596 datapoints\n",
            "  Pencilcase2.txt: 4770 datapoints\n",
            "  Pinkball7.txt: 3537 datapoints\n",
            "  StuffedAnimal10.txt: 4575 datapoints\n",
            "  Pinkball1.txt: 3544 datapoints\n",
            "  Blueball2.txt: 3512 datapoints\n",
            "  StuffedAnimal5.txt: 2670 datapoints\n",
            "  Pinkball10.txt: 3472 datapoints\n",
            "  Box6.txt: 3871 datapoints\n",
            "  Tennis8.txt: 4044 datapoints\n",
            "  Box3.txt: 3643 datapoints\n",
            "  Pencilcase6.txt: 3731 datapoints\n",
            "  Box9.txt: 3530 datapoints\n",
            "  Waterbottle6.txt: 4479 datapoints\n",
            "  Pinkball3.txt: 3491 datapoints\n",
            "  Blueball4.txt: 3529 datapoints\n",
            "  Waterbottle3.txt: 3420 datapoints\n",
            "  StuffedAnimal9.txt: 4869 datapoints\n",
            "  Pinkball4.txt: 3380 datapoints\n",
            "  StuffedAnimal1.txt: 3489 datapoints\n",
            "  StuffedAnimal2.txt: 3913 datapoints\n",
            "  Pencilcase1.txt: 3760 datapoints\n",
            "  StuffedAnimal3.txt: 3941 datapoints\n",
            "  Tennis5.txt: 3526 datapoints\n",
            "  Waterbottle1.txt: 3538 datapoints\n",
            "\n",
            "Dataset Statistics:\n",
            "Total datapoints: 268921\n",
            "Total files: 71\n",
            "  Pinkball: 38770 datapoints\n",
            "  Blueball: 38827 datapoints\n",
            "  Waterbottle: 38198 datapoints\n",
            "  StuffedAnimal: 40293 datapoints\n",
            "  Tennis: 39789 datapoints\n",
            "  Pencilcase: 36044 datapoints\n",
            "  Box: 37000 datapoints\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final dataset splits:\n",
            "Train: 183641 samples\n",
            "Validation: 28674 samples\n",
            "Test: 56606 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "tGK67AajASQI"
      },
      "outputs": [],
      "source": [
        "#Dataset Creation with window\n",
        "class ContactWindowDataset(Dataset):\n",
        "    def __init__(self, data_dir: str, labels: Dict[str, int] = None, window_size: int = 50, step_size: int = 10):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Directory containing the .txt files\n",
        "            labels (Dict[str, int]): Dictionary mapping categories to class labels\n",
        "            window_size (int): Size of the sliding window (smaller = more samples)\n",
        "            step_size (int): Step size for the sliding window (smaller = more samples)\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.file_paths = list(self.data_dir.glob(\"*.txt\"))\n",
        "        self.labels = labels or {}\n",
        "        self.window_size = window_size\n",
        "        self.step_size = step_size\n",
        "\n",
        "        # Lists to store all windows and their labels\n",
        "        self.features_list = []\n",
        "        self.labels_list = []\n",
        "        self.file_indices = []  # To keep track of which file each window came from\n",
        "\n",
        "        # Process all files\n",
        "        print(\"Processing files and extracting windows:\")\n",
        "        for file_idx, file_path in enumerate(self.file_paths):\n",
        "            try:\n",
        "                category = re.sub(r\"\\d+\", \"\", file_path.stem)\n",
        "                label = self.labels.get(category, -1)\n",
        "\n",
        "                if label == -1:\n",
        "                    print(f\"⚠️ Skipping file with unknown category: {file_path.name} (parsed category: '{category}')\")\n",
        "                    continue  # Skip this file entirely\n",
        "\n",
        "                df = self._parse_file(file_path)\n",
        "\n",
        "                windows_from_file = 0\n",
        "                for start_idx in range(0, len(df) - self.window_size + 1, self.step_size):\n",
        "                    window = df.iloc[start_idx:start_idx + self.window_size]\n",
        "                    features = self._extract_features(window)\n",
        "                    self.features_list.append(features)\n",
        "                    self.labels_list.append(label)\n",
        "                    self.file_indices.append(file_idx)\n",
        "                    windows_from_file += 1\n",
        "\n",
        "                print(f\"  {file_path.name}: {windows_from_file} windows generated from {len(df)} datapoints\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing file: {file_path.name}\")\n",
        "                print(f\"   Error: {e}\")\n",
        "\n",
        "        # Convert lists to tensors for efficiency\n",
        "        self.features = torch.FloatTensor(self.features_list)\n",
        "        self.labels = torch.LongTensor(self.labels_list)\n",
        "        self.file_indices = torch.LongTensor(self.file_indices)\n",
        "\n",
        "        # Print summary statistics\n",
        "        self._print_dataset_stats()\n",
        "\n",
        "    def _parse_file(self, file_path: Path) -> pd.DataFrame:\n",
        "        \"\"\"Parse a single data file and filter out rows where y < 0\"\"\"\n",
        "        df = pd.read_csv(file_path, header=None, skiprows=1)\n",
        "        columns = [\n",
        "            'timestamp_pc', 'timestamp_micro',\n",
        "            'x', 'y', 'angle_1', 'angle_2',\n",
        "            'contact_1_left', 'contact_1_right',\n",
        "            'contact_2_left', 'contact_2_right'\n",
        "        ]\n",
        "        df = pd.DataFrame(df.values, columns=columns)\n",
        "\n",
        "        # Filter out rows where y < 0\n",
        "        df = df[df['y'] >= 0].reset_index(drop=True)\n",
        "        return df\n",
        "\n",
        "    def _extract_features(self, window: pd.DataFrame) -> np.ndarray:\n",
        "        \"\"\"Extract features from a window of data\"\"\"\n",
        "        # Basic statistical features\n",
        "        features = np.array([\n",
        "            window['contact_1_left'].mean(),\n",
        "            window['contact_1_right'].mean(),\n",
        "            window['contact_2_left'].mean(),\n",
        "            window['contact_2_right'].mean(),\n",
        "            window['x'].max() - window['x'].min(),\n",
        "            window['y'].max() - window['y'].min(),\n",
        "            window['angle_1'].std(),\n",
        "            window['angle_2'].std(),\n",
        "            # Additional features for more information\n",
        "            window['contact_1_left'].std(),\n",
        "            window['contact_1_right'].std(),\n",
        "            window['contact_2_left'].std(),\n",
        "            window['contact_2_right'].std(),\n",
        "            window['angle_1'].mean(),\n",
        "            window['angle_2'].mean()\n",
        "        ])\n",
        "        return features\n",
        "\n",
        "    def _print_dataset_stats(self):\n",
        "        \"\"\"Print statistics about the dataset\"\"\"\n",
        "        total_windows = len(self.features)\n",
        "        unique_files = len(torch.unique(self.file_indices))\n",
        "\n",
        "        # Count samples per category\n",
        "        category_counts = {}\n",
        "        for label in self.labels_list:\n",
        "            category_name = list(CATEGORIES.keys())[list(CATEGORIES.values()).index(label)]\n",
        "            category_counts[category_name] = category_counts.get(category_name, 0) + 1\n",
        "\n",
        "        print(\"\\nDataset Statistics:\")\n",
        "        print(f\"Total number of windows: {total_windows}\")\n",
        "        print(f\"Total number of files: {unique_files}\")\n",
        "        print(f\"Average windows per file: {total_windows / unique_files:.2f}\")\n",
        "        print(\"\\nSamples per category:\")\n",
        "        for category, count in category_counts.items():\n",
        "            print(f\"  {category}: {count} windows\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
        "        return self.features[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "pDNQQznuASQI"
      },
      "outputs": [],
      "source": [
        "def split_dataset_by_files_robust(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
        "    \"\"\"\n",
        "    Split the dataset by files while trying to ensure all classes are represented in each split.\n",
        "    This function handles edge cases where some classes have very few examples.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataset : ContactWindowDataset\n",
        "        The dataset to split\n",
        "    train_ratio : float\n",
        "        Ratio of data for training\n",
        "    val_ratio : float\n",
        "        Ratio of data for validation\n",
        "    test_ratio : float\n",
        "        Ratio of data for testing\n",
        "    random_state : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (train_indices, val_indices, test_indices)\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    import random\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(random_state)\n",
        "    np.random.seed(random_state)\n",
        "\n",
        "    # Verify ratios sum to 1\n",
        "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10, \"Ratios must sum to 1\"\n",
        "\n",
        "    # Get unique files and their labels\n",
        "    unique_file_indices = torch.unique(dataset.file_indices).numpy()\n",
        "\n",
        "    # Create mapping from file index to most common label in that file\n",
        "    file_to_label = {}\n",
        "    for file_idx in unique_file_indices:\n",
        "        # Get all windows from this file\n",
        "        mask = dataset.file_indices == file_idx\n",
        "        window_indices = torch.where(mask)[0]\n",
        "\n",
        "        # Get the labels for these windows\n",
        "        labels = dataset.labels[window_indices]\n",
        "\n",
        "        # Use the most common label for this file\n",
        "        unique_labels, counts = torch.unique(labels, return_counts=True)\n",
        "        most_common_label = unique_labels[torch.argmax(counts)].item()\n",
        "        file_to_label[file_idx] = most_common_label\n",
        "\n",
        "    # Group files by label\n",
        "    label_to_files = {}\n",
        "    for file_idx, label in file_to_label.items():\n",
        "        if label not in label_to_files:\n",
        "            label_to_files[label] = []\n",
        "        label_to_files[label].append(file_idx)\n",
        "\n",
        "    # Print how many files per label\n",
        "    print(\"\\nFiles per label:\")\n",
        "    for label, files in label_to_files.items():\n",
        "        class_name = list(CATEGORIES.keys())[list(CATEGORIES.values()).index(label)]\n",
        "        print(f\"  {class_name}: {len(files)} files\")\n",
        "\n",
        "    # Manual split to ensure all classes are in all splits\n",
        "    train_files = []\n",
        "    val_files = []\n",
        "    test_files = []\n",
        "\n",
        "    for label, files in label_to_files.items():\n",
        "        # Shuffle files for this label\n",
        "        random.shuffle(files)\n",
        "\n",
        "        # Calculate how many files should go to each split\n",
        "        num_files = len(files)\n",
        "\n",
        "        if num_files >= 3:\n",
        "            # If we have enough files, use the ratios\n",
        "            num_train = max(1, int(num_files * train_ratio))\n",
        "            num_val = max(1, int(num_files * val_ratio))\n",
        "            num_test = max(1, num_files - num_train - num_val)\n",
        "\n",
        "            # If we don't have enough files for all splits, prioritize train\n",
        "            if num_train + num_val + num_test > num_files:\n",
        "                # If we have 2 files, put one in train and one in test\n",
        "                if num_files == 2:\n",
        "                    num_train, num_val, num_test = 1, 0, 1\n",
        "                else:\n",
        "                    num_train = max(1, num_files - 2)\n",
        "                    num_val = 1\n",
        "                    num_test = 1\n",
        "\n",
        "            # Split files\n",
        "            train_files.extend(files[:num_train])\n",
        "            val_files.extend(files[num_train:num_train+num_val])\n",
        "            test_files.extend(files[num_train+num_val:])\n",
        "\n",
        "        elif num_files == 2:\n",
        "            # For classes with only 2 files, put one in train and one in val\n",
        "            train_files.append(files[0])\n",
        "            val_files.append(files[1])\n",
        "            # We'll handle test set separately\n",
        "\n",
        "        elif num_files == 1:\n",
        "            # For classes with only 1 file, duplicate windows\n",
        "            # Put the file in the train set\n",
        "            train_files.append(files[0])\n",
        "            print(f\"Warning: Class {label} has only one file, putting it in train set\")\n",
        "\n",
        "    # Now map back to window indices\n",
        "    train_indices = []\n",
        "    val_indices = []\n",
        "    test_indices = []\n",
        "\n",
        "    for idx in range(len(dataset)):\n",
        "        file_idx = dataset.file_indices[idx].item()\n",
        "        if file_idx in train_files:\n",
        "            train_indices.append(idx)\n",
        "        elif file_idx in val_files:\n",
        "            val_indices.append(idx)\n",
        "        elif file_idx in test_files:\n",
        "            test_indices.append(idx)\n",
        "\n",
        "    # Special handling for classes with only one or two files\n",
        "    # For these classes, create synthetic test examples\n",
        "    for label, files in label_to_files.items():\n",
        "        if len(files) <= 2:\n",
        "            class_indices = []\n",
        "            for idx in range(len(dataset)):\n",
        "                if dataset.labels[idx].item() == label:\n",
        "                    class_indices.append(idx)\n",
        "\n",
        "            if len(files) == 2 and len(test_indices) == 0:\n",
        "                # If we have 2 files but none in test, move some val windows to test\n",
        "                class_val_indices = [idx for idx in class_indices if idx in val_indices]\n",
        "                if class_val_indices:\n",
        "                    # Move half of val windows to test\n",
        "                    half_point = len(class_val_indices) // 2\n",
        "                    for idx in class_val_indices[half_point:]:\n",
        "                        val_indices.remove(idx)\n",
        "                        test_indices.append(idx)\n",
        "\n",
        "            elif len(files) == 1:\n",
        "                # If we have only 1 file, duplicate some train windows to val and test\n",
        "                class_train_indices = [idx for idx in class_indices if idx in train_indices]\n",
        "\n",
        "                # Select random indices to duplicate (without removing from train)\n",
        "                random.shuffle(class_train_indices)\n",
        "                third_point = max(1, len(class_train_indices) // 3)\n",
        "\n",
        "                # Add to val and test\n",
        "                val_indices.extend(class_train_indices[:third_point])\n",
        "                test_indices.extend(class_train_indices[third_point:2*third_point])\n",
        "\n",
        "    # Print class distribution in each split\n",
        "    print(\"\\nClass distribution:\")\n",
        "    for split_name, indices in [\n",
        "        (\"Train\", train_indices),\n",
        "        (\"Validation\", val_indices),\n",
        "        (\"Test\", test_indices)\n",
        "    ]:\n",
        "        class_counts = {}\n",
        "        for idx in indices:\n",
        "            label = dataset.labels[idx].item()\n",
        "            class_name = list(CATEGORIES.keys())[list(CATEGORIES.values()).index(label)]\n",
        "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
        "\n",
        "        print(f\"\\n{split_name} set:\")\n",
        "        for class_name, count in sorted(class_counts.items()):\n",
        "            print(f\"  {class_name}: {count} windows\")\n",
        "\n",
        "    return train_indices, val_indices, test_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLlzvWiiASQI",
        "outputId": "874de60e-5e81-49da-f415-4b90cb671c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing files and extracting windows:\n",
            "  Pinkball2.txt: 664 windows generated from 3418 datapoints\n",
            "  Pinkball6.txt: 660 windows generated from 3396 datapoints\n",
            "  Blueball6.txt: 681 windows generated from 3504 datapoints\n",
            "  Waterbottle2.txt: 649 windows generated from 3343 datapoints\n",
            "  Waterbottle4.txt: 806 windows generated from 4129 datapoints\n",
            "  Blueball10.txt: 844 windows generated from 4315 datapoints\n",
            "  StuffedAnimal7.txt: 751 windows generated from 3852 datapoints\n",
            "  Pinkball5.txt: 690 windows generated from 3547 datapoints\n",
            "  Waterbottle10.txt: 758 windows generated from 3888 datapoints\n",
            "  Tennis7.txt: 758 windows generated from 3886 datapoints\n",
            "  Waterbottle8.txt: 708 windows generated from 3638 datapoints\n",
            "  Pencilcase5.txt: 790 windows generated from 4046 datapoints\n",
            "  Pinkball11.txt: 791 windows generated from 4054 datapoints\n",
            "  StuffedAnimal4.txt: 733 windows generated from 3760 datapoints\n",
            "  Blueball5.txt: 683 windows generated from 3512 datapoints\n",
            "  Pencilcase10.txt: 783 windows generated from 4014 datapoints\n",
            "  Tennis4.txt: 792 windows generated from 4058 datapoints\n",
            "  Tennis9.txt: 682 windows generated from 3508 datapoints\n",
            "  Blueball7.txt: 703 windows generated from 3613 datapoints\n",
            "  Tennis1.txt: 742 windows generated from 3805 datapoints\n",
            "  Waterbottle7.txt: 737 windows generated from 3780 datapoints\n",
            "  Tennis2.txt: 991 windows generated from 5052 datapoints\n",
            "  Blueball9.txt: 680 windows generated from 3498 datapoints\n",
            "  Tennis6.txt: 726 windows generated from 3729 datapoints\n",
            "  Box10.txt: 655 windows generated from 3373 datapoints\n",
            "  Tennis3.txt: 733 windows generated from 3764 datapoints\n",
            "  Pinkball9.txt: 658 windows generated from 3389 datapoints\n",
            "  Blueball11.txt: 593 windows generated from 3064 datapoints\n",
            "  Pencilcase3.txt: 913 windows generated from 4660 datapoints\n",
            "  Box7.txt: 793 windows generated from 4062 datapoints\n",
            "  StuffedAnimal6.txt: 973 windows generated from 4963 datapoints\n",
            "  Box8.txt: 692 windows generated from 3559 datapoints\n",
            "  Waterbottle9.txt: 862 windows generated from 4405 datapoints\n",
            "  Box2.txt: 674 windows generated from 3467 datapoints\n",
            "  Box5.txt: 799 windows generated from 4090 datapoints\n",
            "  Pencilcase8.txt: 797 windows generated from 4080 datapoints\n",
            "  Blueball1.txt: 644 windows generated from 3316 datapoints\n",
            "  Waterbottle5.txt: 696 windows generated from 3578 datapoints\n",
            "  Pencilcase9.txt: 658 windows generated from 3387 datapoints\n",
            "  Pinkball8.txt: 689 windows generated from 3542 datapoints\n",
            "  Box4.txt: 708 windows generated from 3635 datapoints\n",
            "  Blueball8.txt: 694 windows generated from 3569 datapoints\n",
            "  Tennis10.txt: 864 windows generated from 4417 datapoints\n",
            "  StuffedAnimal8.txt: 833 windows generated from 4261 datapoints\n",
            "  Box1.txt: 735 windows generated from 3770 datapoints\n",
            "  Blueball3.txt: 660 windows generated from 3395 datapoints\n",
            "  Pencilcase4.txt: 700 windows generated from 3596 datapoints\n",
            "  Pencilcase2.txt: 935 windows generated from 4770 datapoints\n",
            "  Pinkball7.txt: 688 windows generated from 3537 datapoints\n",
            "  StuffedAnimal10.txt: 896 windows generated from 4575 datapoints\n",
            "  Pinkball1.txt: 689 windows generated from 3544 datapoints\n",
            "  Blueball2.txt: 683 windows generated from 3512 datapoints\n",
            "  StuffedAnimal5.txt: 515 windows generated from 2670 datapoints\n",
            "  Pinkball10.txt: 675 windows generated from 3472 datapoints\n",
            "  Box6.txt: 755 windows generated from 3871 datapoints\n",
            "  Tennis8.txt: 789 windows generated from 4044 datapoints\n",
            "  Box3.txt: 709 windows generated from 3643 datapoints\n",
            "  Pencilcase6.txt: 727 windows generated from 3731 datapoints\n",
            "  Box9.txt: 687 windows generated from 3530 datapoints\n",
            "  Waterbottle6.txt: 876 windows generated from 4479 datapoints\n",
            "  Pinkball3.txt: 679 windows generated from 3491 datapoints\n",
            "  Blueball4.txt: 686 windows generated from 3529 datapoints\n",
            "  Waterbottle3.txt: 665 windows generated from 3420 datapoints\n",
            "  StuffedAnimal9.txt: 954 windows generated from 4869 datapoints\n",
            "  Pinkball4.txt: 657 windows generated from 3380 datapoints\n",
            "  StuffedAnimal1.txt: 678 windows generated from 3489 datapoints\n",
            "  StuffedAnimal2.txt: 763 windows generated from 3913 datapoints\n",
            "  Pencilcase1.txt: 733 windows generated from 3760 datapoints\n",
            "  StuffedAnimal3.txt: 769 windows generated from 3941 datapoints\n",
            "  Tennis5.txt: 686 windows generated from 3526 datapoints\n",
            "  Waterbottle1.txt: 688 windows generated from 3538 datapoints\n",
            "\n",
            "Dataset Statistics:\n",
            "Total number of windows: 52407\n",
            "Total number of files: 71\n",
            "Average windows per file: 738.13\n",
            "\n",
            "Samples per category:\n",
            "  Pinkball: 7540 windows\n",
            "  Blueball: 7551 windows\n",
            "  Waterbottle: 7445 windows\n",
            "  StuffedAnimal: 7865 windows\n",
            "  Tennis: 7763 windows\n",
            "  Pencilcase: 7036 windows\n",
            "  Box: 7207 windows\n"
          ]
        }
      ],
      "source": [
        "# Create the dataset with smaller window and step size to maximize samples\n",
        "window_size = 100   # Smaller window size generates more samples\n",
        "step_size = 5     # Smaller step size creates more overlapping windows\n",
        "dataset = ContactWindowDataset(\n",
        "    data_dir=config[\"data_dir\"],\n",
        "    labels=CATEGORIES,\n",
        "    window_size=window_size,\n",
        "    step_size=step_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZMqKKG6ASQJ",
        "outputId": "7e57678d-affc-40f4-d186-6e0e6cd01ed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files per label:\n",
            "  Pinkball: 11 files\n",
            "  Blueball: 11 files\n",
            "  Waterbottle: 10 files\n",
            "  StuffedAnimal: 10 files\n",
            "  Tennis: 10 files\n",
            "  Pencilcase: 9 files\n",
            "  Box: 10 files\n",
            "\n",
            "Class distribution:\n",
            "\n",
            "Train set:\n",
            "  Blueball: 4883 windows\n",
            "  Box: 5085 windows\n",
            "  Pencilcase: 4710 windows\n",
            "  Pinkball: 4901 windows\n",
            "  StuffedAnimal: 5481 windows\n",
            "  Tennis: 5360 windows\n",
            "  Waterbottle: 5376 windows\n",
            "\n",
            "Validation set:\n",
            "  Blueball: 660 windows\n",
            "  Box: 674 windows\n",
            "  Pencilcase: 935 windows\n",
            "  Pinkball: 658 windows\n",
            "  StuffedAnimal: 973 windows\n",
            "  Tennis: 991 windows\n",
            "  Waterbottle: 708 windows\n",
            "\n",
            "Test set:\n",
            "  Blueball: 2008 windows\n",
            "  Box: 1448 windows\n",
            "  Pencilcase: 1391 windows\n",
            "  Pinkball: 1981 windows\n",
            "  StuffedAnimal: 1411 windows\n",
            "  Tennis: 1412 windows\n",
            "  Waterbottle: 1361 windows\n",
            "\n",
            "Dataset Splits:\n",
            "Training set size: 35796 windows\n",
            "Validation set size: 5599 windows\n",
            "Test set size: 11012 windows\n",
            "\n",
            "Sample batch:\n",
            "Batch features shape: torch.Size([32, 14])\n",
            "Batch labels shape: torch.Size([32])\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset by files to prevent data leakage\n",
        "train_indices, val_indices, test_indices = split_dataset_by_files_robust(dataset)\n",
        "\n",
        "# Create subset datasets\n",
        "from torch.utils.data import Subset\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
        "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'])\n",
        "\n",
        "# Print split information\n",
        "print(\"\\nDataset Splits:\")\n",
        "print(f\"Training set size: {len(train_dataset)} windows\")\n",
        "print(f\"Validation set size: {len(val_dataset)} windows\")\n",
        "print(f\"Test set size: {len(test_dataset)} windows\")\n",
        "\n",
        "# Example of accessing a batch\n",
        "for features, labels in train_loader:\n",
        "    print(f\"\\nSample batch:\")\n",
        "    print(f\"Batch features shape: {features.shape}\")\n",
        "    print(f\"Batch labels shape: {labels.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vEKjnVmASQJ",
        "outputId": "a0586c52-c613-425c-e5f5-f7f79e3f263c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 64]             960\n",
            "       BatchNorm1d-2                   [-1, 64]             128\n",
            "              ReLU-3                   [-1, 64]               0\n",
            "            Conv1d-4              [-1, 128, 50]          57,472\n",
            "       BatchNorm1d-5              [-1, 128, 50]             256\n",
            "              ReLU-6              [-1, 128, 50]               0\n",
            "         MaxPool1d-7              [-1, 128, 25]               0\n",
            "            Conv1d-8              [-1, 128, 25]          49,280\n",
            "       BatchNorm1d-9              [-1, 128, 25]             256\n",
            "             ReLU-10              [-1, 128, 25]               0\n",
            "           Conv1d-11              [-1, 128, 25]          49,280\n",
            "      BatchNorm1d-12              [-1, 128, 25]             256\n",
            "             ReLU-13              [-1, 128, 25]               0\n",
            "  Temporal1DBlock-14              [-1, 128, 25]               0\n",
            "        MaxPool1d-15              [-1, 128, 12]               0\n",
            "           Conv1d-16              [-1, 256, 12]          33,024\n",
            "      BatchNorm1d-17              [-1, 256, 12]             512\n",
            "           Conv1d-18              [-1, 256, 12]          98,560\n",
            "      BatchNorm1d-19              [-1, 256, 12]             512\n",
            "             ReLU-20              [-1, 256, 12]               0\n",
            "           Conv1d-21              [-1, 256, 12]         196,864\n",
            "      BatchNorm1d-22              [-1, 256, 12]             512\n",
            "             ReLU-23              [-1, 256, 12]               0\n",
            "  Temporal1DBlock-24              [-1, 256, 12]               0\n",
            "        MaxPool1d-25               [-1, 256, 6]               0\n",
            "           Conv1d-26               [-1, 512, 6]         131,584\n",
            "      BatchNorm1d-27               [-1, 512, 6]           1,024\n",
            "           Conv1d-28               [-1, 512, 6]         393,728\n",
            "      BatchNorm1d-29               [-1, 512, 6]           1,024\n",
            "             ReLU-30               [-1, 512, 6]               0\n",
            "           Conv1d-31               [-1, 512, 6]         786,944\n",
            "      BatchNorm1d-32               [-1, 512, 6]           1,024\n",
            "             ReLU-33               [-1, 512, 6]               0\n",
            "  Temporal1DBlock-34               [-1, 512, 6]               0\n",
            "AdaptiveAvgPool1d-35               [-1, 512, 1]               0\n",
            "          Dropout-36                  [-1, 512]               0\n",
            "           Linear-37                  [-1, 128]          65,664\n",
            "      BatchNorm1d-38                  [-1, 128]             256\n",
            "           Linear-39                    [-1, 7]             903\n",
            "================================================================\n",
            "Total params: 1,870,023\n",
            "Trainable params: 1,870,023\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.80\n",
            "Params size (MB): 7.13\n",
            "Estimated Total Size (MB): 7.93\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "class Temporal1DBlock(nn.Module):\n",
        "    \"\"\"Residual block for 1D time series data with skip connection\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1):\n",
        "        super(Temporal1DBlock, self).__init__()\n",
        "        padding = dilation * (kernel_size - 1) // 2\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1),\n",
        "                nn.BatchNorm1d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        return self.relu(out)\n",
        "\n",
        "\n",
        "class TimeSeriesClassifier(nn.Module):\n",
        "    def __init__(self, input_features, num_classes, window_size=50):\n",
        "        super(TimeSeriesClassifier, self).__init__()\n",
        "\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.feature_embedding = nn.Sequential(\n",
        "            nn.Linear(input_features, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=7, stride=1, padding=3),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2, padding=1)  # 128-dim\n",
        "        )\n",
        "\n",
        "        self.res1 = Temporal1DBlock(128, 128, dilation=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.res2 = Temporal1DBlock(128, 256, dilation=2)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.res3 = Temporal1DBlock(256, 512, dilation=4)\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(512, 128)\n",
        "        self.bn = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(-1)\n",
        "            x = x.transpose(1, 2)\n",
        "            x = x.view(x.shape[0], -1)\n",
        "            x = self.feature_embedding(x)\n",
        "            x = x.unsqueeze(-1)\n",
        "            x = x.repeat(1, 1, self.window_size)\n",
        "        elif len(x.shape) == 3:\n",
        "            batch_size, features, time_steps = x.shape\n",
        "            x = x.transpose(1, 2).reshape(-1, features)\n",
        "            x = self.feature_embedding(x)\n",
        "            x = x.view(batch_size, time_steps, -1).transpose(1, 2)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.res1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.res2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.res3(x)\n",
        "\n",
        "        x = self.global_pool(x).squeeze(-1)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn(x)\n",
        "        feats = x\n",
        "        out = self.fc2(x)\n",
        "\n",
        "        return {\"feats\": feats, \"out\": out}\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "input_features = 14  # Number of features in your dataset\n",
        "window_size = 50  # Size of time window\n",
        "num_classes = len(CATEGORIES)\n",
        "model = TimeSeriesClassifier(input_features, num_classes, window_size)\n",
        "model = model.to(device)\n",
        "\n",
        "summary(model, input_size=(input_features, window_size))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class Temporal1DBlock(nn.Module):\n",
        "#     def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1):\n",
        "#         super(Temporal1DBlock, self).__init__()\n",
        "#         padding = dilation * (kernel_size - 1) // 2\n",
        "\n",
        "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation)\n",
        "#         self.bn1 = nn.BatchNorm1d(out_channels)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation)\n",
        "#         self.bn2 = nn.BatchNorm1d(out_channels)\n",
        "\n",
        "#         self.downsample = nn.Sequential()\n",
        "#         if in_channels != out_channels:\n",
        "#             self.downsample = nn.Sequential(\n",
        "#                 nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "#                 nn.BatchNorm1d(out_channels)\n",
        "#             )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         identity = self.downsample(x)\n",
        "#         out = self.relu(self.bn1(self.conv1(x)))\n",
        "#         out = self.bn2(self.conv2(out))\n",
        "#         out += identity\n",
        "#         return self.relu(out)\n",
        "\n",
        "# class TimeSeriesClassifier(nn.Module):\n",
        "#     def __init__(self, input_features, num_classes, simulated_timesteps=10):\n",
        "#         super(TimeSeriesClassifier, self).__init__()\n",
        "\n",
        "#         reduced_dim = 64\n",
        "#         self.simulated_timesteps = simulated_timesteps\n",
        "\n",
        "#         self.feature_embedding = nn.Sequential(\n",
        "#             nn.Linear(input_features, reduced_dim),\n",
        "#             nn.ReLU(inplace=True)\n",
        "#         )\n",
        "\n",
        "#         self.conv1 = nn.Sequential(\n",
        "#             nn.Conv1d(reduced_dim, 128, kernel_size=3, padding=1),\n",
        "#             nn.BatchNorm1d(128),\n",
        "#             nn.ReLU(inplace=True),\n",
        "#             nn.MaxPool1d(kernel_size=2)\n",
        "#         )\n",
        "\n",
        "#         self.res_blocks = nn.Sequential(\n",
        "#             Temporal1DBlock(128, 256, dilation=1),\n",
        "#             Temporal1DBlock(256, 512, dilation=2),\n",
        "#             Temporal1DBlock(512, 1024, dilation=4)\n",
        "#         )\n",
        "\n",
        "#         self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
        "#         self.classifier = nn.Sequential(\n",
        "#             nn.Dropout(0.5),\n",
        "#             nn.Linear(1024, num_classes)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         if len(x.shape) == 2:\n",
        "#             x = self.feature_embedding(x)\n",
        "#             x = torch.nan_to_num(x)  # Replace NaNs\n",
        "#             x = x.unsqueeze(-1).repeat(1, 1, self.simulated_timesteps)\n",
        "#         elif len(x.shape) == 3:\n",
        "#             batch_size, features, time_steps = x.shape\n",
        "#             x = x.transpose(1, 2).reshape(-1, features)\n",
        "#             x = self.feature_embedding(x)\n",
        "#             x = torch.nan_to_num(x)  # Replace NaNs\n",
        "#             x = x.view(batch_size, time_steps, -1).transpose(1, 2)\n",
        "\n",
        "#         x = self.conv1(x)\n",
        "#         x = self.res_blocks(x)\n",
        "#         x = self.global_pool(x).squeeze(-1)\n",
        "#         out = self.classifier(x)\n",
        "#         return {\"feats\": x, \"out\": out}\n",
        "\n",
        "# # Initialize the model\n",
        "# input_features = 8  # Number of features in your dataset\n",
        "# window_size = 50  # Size of time window\n",
        "# num_classes = len(CATEGORIES)\n",
        "# model = TimeSeriesClassifier(input_features, num_classes, window_size)\n",
        "# model = model.to(device)\n",
        "\n",
        "# summary(model, input_size=(input_features, window_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m10y_w81QIUK",
        "outputId": "8c7cf6f2-91e8-4b20-9f53-418b0b8f7d08"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 64]             576\n",
            "              ReLU-2                   [-1, 64]               0\n",
            "            Conv1d-3              [-1, 128, 50]          24,704\n",
            "       BatchNorm1d-4              [-1, 128, 50]             256\n",
            "              ReLU-5              [-1, 128, 50]               0\n",
            "         MaxPool1d-6              [-1, 128, 25]               0\n",
            "            Conv1d-7              [-1, 256, 25]          33,024\n",
            "       BatchNorm1d-8              [-1, 256, 25]             512\n",
            "            Conv1d-9              [-1, 256, 25]          98,560\n",
            "      BatchNorm1d-10              [-1, 256, 25]             512\n",
            "             ReLU-11              [-1, 256, 25]               0\n",
            "           Conv1d-12              [-1, 256, 25]         196,864\n",
            "      BatchNorm1d-13              [-1, 256, 25]             512\n",
            "             ReLU-14              [-1, 256, 25]               0\n",
            "  Temporal1DBlock-15              [-1, 256, 25]               0\n",
            "           Conv1d-16              [-1, 512, 25]         131,584\n",
            "      BatchNorm1d-17              [-1, 512, 25]           1,024\n",
            "           Conv1d-18              [-1, 512, 25]         393,728\n",
            "      BatchNorm1d-19              [-1, 512, 25]           1,024\n",
            "             ReLU-20              [-1, 512, 25]               0\n",
            "           Conv1d-21              [-1, 512, 25]         786,944\n",
            "      BatchNorm1d-22              [-1, 512, 25]           1,024\n",
            "             ReLU-23              [-1, 512, 25]               0\n",
            "  Temporal1DBlock-24              [-1, 512, 25]               0\n",
            "           Conv1d-25             [-1, 1024, 25]         525,312\n",
            "      BatchNorm1d-26             [-1, 1024, 25]           2,048\n",
            "           Conv1d-27             [-1, 1024, 25]       1,573,888\n",
            "      BatchNorm1d-28             [-1, 1024, 25]           2,048\n",
            "             ReLU-29             [-1, 1024, 25]               0\n",
            "           Conv1d-30             [-1, 1024, 25]       3,146,752\n",
            "      BatchNorm1d-31             [-1, 1024, 25]           2,048\n",
            "             ReLU-32             [-1, 1024, 25]               0\n",
            "  Temporal1DBlock-33             [-1, 1024, 25]               0\n",
            "AdaptiveAvgPool1d-34              [-1, 1024, 1]               0\n",
            "          Dropout-35                 [-1, 1024]               0\n",
            "           Linear-36                    [-1, 7]           7,175\n",
            "================================================================\n",
            "Total params: 6,930,119\n",
            "Trainable params: 6,930,119\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 3.26\n",
            "Params size (MB): 26.44\n",
            "Estimated Total Size (MB): 29.70\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "x58jxxMDASQJ"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "wIwp9zitASQJ"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the top-k accuracy\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "OHYjqyauASQJ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    # Metric meters\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "\n",
        "    # Progress Bar\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()  # Zero gradients\n",
        "\n",
        "        # Get the data\n",
        "        x, y = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs['out'], y)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        acc = accuracy(outputs['out'], y)[0].item()\n",
        "\n",
        "        # Update meters\n",
        "        loss_m.update(loss.item())\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        # Update progress bar\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(loss_m.avg)),\n",
        "            acc=\"{:.04f}%\".format(float(acc_m.avg)),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        # Memory management\n",
        "        del x, y, outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    return loss_m.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "fhJE9SqXASQJ"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate_model(model, val_loader, criterion, class_names, device):\n",
        "    model.eval()\n",
        "    # Metric meters\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "\n",
        "    # Progress Bar\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "        x, y = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs['out'], y)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        acc = accuracy(outputs['out'], y)[0].item()\n",
        "\n",
        "        # Store predictions and targets for confusion matrix\n",
        "        _, predicted = torch.max(outputs['out'], 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "        # Update meters\n",
        "        loss_m.update(loss.item())\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        # Update progress bar\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(loss_m.avg)),\n",
        "            acc=\"{:.04f}%\".format(float(acc_m.avg))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        # Memory management\n",
        "        del x, y, outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    # Calculate per-class accuracy\n",
        "    if class_names:\n",
        "        print(\"\\nPer-class Validation Accuracy:\")\n",
        "        per_class_acc = {}\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_mask = (np.array(all_targets) == i)\n",
        "            if np.sum(class_mask) > 0:\n",
        "                class_correct = np.sum((np.array(all_preds)[class_mask] == i))\n",
        "                class_total = np.sum(class_mask)\n",
        "                acc_percent = 100 * class_correct / class_total\n",
        "                print(f\"  {class_name}: {acc_percent:.4f}% ({class_correct}/{class_total})\")\n",
        "                per_class_acc[f\"val_acc_{class_name}\"] = acc_percent\n",
        "\n",
        "\n",
        "    return loss_m.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "tBwbY2qqASQK"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'metrics': metrics\n",
        "    }, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fx-XvfDASQK",
        "outputId": "c1783f8a-afe7-4903-f6db-68cdf92e2fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Define CrossEntropyLoss as the criterion\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    weight=class_weights.to(device),\n",
        "    label_smoothing=0.1\n",
        ")\n",
        "\n",
        "# Initialize optimizer with AdamW\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    weight_decay=1e-5\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=5,\n",
        "    min_lr=1e-6,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNd6D1c-ASQK",
        "outputId": "9681129c-5d66-4080-c7e4-747f645b6d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "wandb.login(key=\"78d5988d9f05a421bc74d044c3cd9afc3b918020\") # API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "-c1MRAeqASQK"
      },
      "outputs": [],
      "source": [
        "# Initialize wandb\n",
        "run = wandb.init(\n",
        "    name = \"14run\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = False, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id = \"\", ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"object_classification\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ31h1FjASQK",
        "outputId": "0296b063-c240-435d-c9c0-4f4d4efba76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.9171, Train Accuracy: 20.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 48.7879% (322/660)\n",
            "  Box: 42.8783% (289/674)\n",
            "  Pencilcase: 28.7701% (269/935)\n",
            "  Pinkball: 28.1155% (185/658)\n",
            "  StuffedAnimal: 11.0997% (108/973)\n",
            "  Tennis: 18.8698% (187/991)\n",
            "  Waterbottle: 0.0000% (0/708)\n",
            "Validation Loss: 2.0207, Validation Accuracy: 24.29%\n",
            "Saved best model with validation loss: 2.0207 and accuracy: 24.29%\n",
            "Saved model for epoch 1\n",
            "End of Epoch 1/20\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  48%|████▊     | 542/1119 [00:09<00:11, 50.57it/s, acc=23.8893%, loss=1.8718, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 0 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.8674, Train Accuracy: 24.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 54.2424% (358/660)\n",
            "  Box: 36.3501% (245/674)\n",
            "  Pencilcase: 28.5561% (267/935)\n",
            "  Pinkball: 20.8207% (137/658)\n",
            "  StuffedAnimal: 12.0247% (117/973)\n",
            "  Tennis: 6.6599% (66/991)\n",
            "  Waterbottle: 0.0000% (0/708)\n",
            "Validation Loss: 2.0622, Validation Accuracy: 21.26%\n",
            "Saved model for epoch 2\n",
            "End of Epoch 2/20\n",
            "\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  40%|████      | 451/1119 [00:08<00:11, 58.02it/s, acc=25.7951%, loss=1.8401, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.8368, Train Accuracy: 25.89%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 22.5758% (149/660)\n",
            "  Box: 57.7151% (389/674)\n",
            "  Pencilcase: 31.6578% (296/935)\n",
            "  Pinkball: 53.0395% (349/658)\n",
            "  StuffedAnimal: 13.6691% (133/973)\n",
            "  Tennis: 13.3199% (132/991)\n",
            "  Waterbottle: 0.7062% (5/708)\n",
            "Validation Loss: 2.0203, Validation Accuracy: 25.95%\n",
            "Saved best model with validation loss: 2.0203 and accuracy: 25.95%\n",
            "Saved model for epoch 3\n",
            "End of Epoch 3/20\n",
            "\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  34%|███▍      | 384/1119 [00:06<00:12, 58.06it/s, acc=26.0552%, loss=1.8234, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 2 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.8172, Train Accuracy: 26.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 25.9091% (171/660)\n",
            "  Box: 33.5312% (226/674)\n",
            "  Pencilcase: 32.7273% (306/935)\n",
            "  Pinkball: 20.0608% (132/658)\n",
            "  StuffedAnimal: 21.0689% (205/973)\n",
            "  Tennis: 28.7588% (285/991)\n",
            "  Waterbottle: 1.4124% (10/708)\n",
            "Validation Loss: 2.0175, Validation Accuracy: 23.85%\n",
            "Saved best model with validation loss: 2.0175 and accuracy: 23.85%\n",
            "Saved model for epoch 4\n",
            "End of Epoch 4/20\n",
            "\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  24%|██▍       | 266/1119 [00:05<00:15, 53.50it/s, acc=28.2303%, loss=1.8033, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 3 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7987, Train Accuracy: 28.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 19.3939% (128/660)\n",
            "  Box: 26.7062% (180/674)\n",
            "  Pencilcase: 33.2620% (311/935)\n",
            "  Pinkball: 38.1459% (251/658)\n",
            "  StuffedAnimal: 14.1829% (138/973)\n",
            "  Tennis: 19.5762% (194/991)\n",
            "  Waterbottle: 0.5650% (4/708)\n",
            "Validation Loss: 2.1325, Validation Accuracy: 21.54%\n",
            "Saved model for epoch 5\n",
            "End of Epoch 5/20\n",
            "\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  18%|█▊        | 204/1119 [00:03<00:15, 57.62it/s, acc=28.6585%, loss=1.7830, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 4 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7836, Train Accuracy: 28.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 23.7879% (157/660)\n",
            "  Box: 18.9911% (128/674)\n",
            "  Pencilcase: 32.5134% (304/935)\n",
            "  Pinkball: 42.7052% (281/658)\n",
            "  StuffedAnimal: 13.4635% (131/973)\n",
            "  Tennis: 35.1160% (348/991)\n",
            "  Waterbottle: 0.7062% (5/708)\n",
            "Validation Loss: 2.0751, Validation Accuracy: 24.18%\n",
            "Saved model for epoch 6\n",
            "End of Epoch 6/20\n",
            "\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  11%|█         | 118/1119 [00:02<00:20, 48.01it/s, acc=29.8582%, loss=1.7751, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 5 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7692, Train Accuracy: 29.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 21.6667% (143/660)\n",
            "  Box: 26.4095% (178/674)\n",
            "  Pencilcase: 31.3369% (293/935)\n",
            "  Pinkball: 44.5289% (293/658)\n",
            "  StuffedAnimal: 14.2857% (139/973)\n",
            "  Tennis: 25.0252% (248/991)\n",
            "  Waterbottle: 3.2486% (23/708)\n",
            "Validation Loss: 2.1038, Validation Accuracy: 23.53%\n",
            "Saved model for epoch 7\n",
            "End of Epoch 7/20\n",
            "\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   4%|▍         | 45/1119 [00:00<00:18, 57.98it/s, acc=29.4158%, loss=1.7614, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 6 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7538, Train Accuracy: 30.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 19.0909% (126/660)\n",
            "  Box: 51.3353% (346/674)\n",
            "  Pencilcase: 30.3743% (284/935)\n",
            "  Pinkball: 31.4590% (207/658)\n",
            "  StuffedAnimal: 17.8828% (174/973)\n",
            "  Tennis: 30.6761% (304/991)\n",
            "  Waterbottle: 1.8362% (13/708)\n",
            "Validation Loss: 2.0881, Validation Accuracy: 25.97%\n",
            "Saved model for epoch 8\n",
            "End of Epoch 8/20\n",
            "\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  45%|████▌     | 508/1119 [00:09<00:10, 57.54it/s, acc=31.0351%, loss=1.7444, lr=0.001000]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 7 that is less than the current step 8. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7419, Train Accuracy: 31.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 41.5152% (274/660)\n",
            "  Box: 34.1246% (230/674)\n",
            "  Pencilcase: 35.5080% (332/935)\n",
            "  Pinkball: 14.4377% (95/658)\n",
            "  StuffedAnimal: 15.9301% (155/973)\n",
            "  Tennis: 26.1352% (259/991)\n",
            "  Waterbottle: 1.4124% (10/708)\n",
            "Validation Loss: 2.1729, Validation Accuracy: 24.21%\n",
            "Saved model for epoch 9\n",
            "End of Epoch 9/20\n",
            "\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7301, Train Accuracy: 31.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 24.8485% (164/660)\n",
            "  Box: 26.8546% (181/674)\n",
            "  Pencilcase: 34.9733% (327/935)\n",
            "  Pinkball: 35.5623% (234/658)\n",
            "  StuffedAnimal: 17.8828% (174/973)\n",
            "  Tennis: 26.6398% (264/991)\n",
            "  Waterbottle: 9.0395% (64/708)\n",
            "Validation Loss: 2.1016, Validation Accuracy: 25.15%\n",
            "Saved model for epoch 10\n",
            "End of Epoch 10/20\n",
            "\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7015, Train Accuracy: 33.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 28.4848% (188/660)\n",
            "  Box: 46.2908% (312/674)\n",
            "  Pencilcase: 34.7594% (325/935)\n",
            "  Pinkball: 36.0182% (237/658)\n",
            "  StuffedAnimal: 16.4440% (160/973)\n",
            "  Tennis: 24.7225% (245/991)\n",
            "  Waterbottle: 3.8136% (27/708)\n",
            "Validation Loss: 2.1639, Validation Accuracy: 26.69%\n",
            "Saved model for epoch 11\n",
            "End of Epoch 11/20\n",
            "\n",
            "Epoch 12/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6909, Train Accuracy: 34.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 24.6970% (163/660)\n",
            "  Box: 54.1543% (365/674)\n",
            "  Pencilcase: 36.5775% (342/935)\n",
            "  Pinkball: 33.5866% (221/658)\n",
            "  StuffedAnimal: 19.1161% (186/973)\n",
            "  Tennis: 29.7679% (295/991)\n",
            "  Waterbottle: 4.2373% (30/708)\n",
            "Validation Loss: 2.0782, Validation Accuracy: 28.61%\n",
            "Saved model for epoch 12\n",
            "End of Epoch 12/20\n",
            "\n",
            "Epoch 13/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6806, Train Accuracy: 34.19%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 26.2121% (173/660)\n",
            "  Box: 55.1929% (372/674)\n",
            "  Pencilcase: 36.6845% (343/935)\n",
            "  Pinkball: 25.8359% (170/658)\n",
            "  StuffedAnimal: 18.8078% (183/973)\n",
            "  Tennis: 23.5116% (233/991)\n",
            "  Waterbottle: 8.8983% (63/708)\n",
            "Validation Loss: 2.1280, Validation Accuracy: 27.45%\n",
            "Saved model for epoch 13\n",
            "End of Epoch 13/20\n",
            "\n",
            "Epoch 14/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6749, Train Accuracy: 34.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 24.2424% (160/660)\n",
            "  Box: 25.6677% (173/674)\n",
            "  Pencilcase: 35.2941% (330/935)\n",
            "  Pinkball: 50.7599% (334/658)\n",
            "  StuffedAnimal: 21.8911% (213/973)\n",
            "  Tennis: 22.3007% (221/991)\n",
            "  Waterbottle: 8.0508% (57/708)\n",
            "Validation Loss: 2.2119, Validation Accuracy: 26.58%\n",
            "Saved model for epoch 14\n",
            "End of Epoch 14/20\n",
            "\n",
            "Epoch 15/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6654, Train Accuracy: 35.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 26.8182% (177/660)\n",
            "  Box: 28.4866% (192/674)\n",
            "  Pencilcase: 35.9358% (336/935)\n",
            "  Pinkball: 40.2736% (265/658)\n",
            "  StuffedAnimal: 19.0134% (185/973)\n",
            "  Tennis: 21.7962% (216/991)\n",
            "  Waterbottle: 8.0508% (57/708)\n",
            "Validation Loss: 2.1249, Validation Accuracy: 25.51%\n",
            "Saved model for epoch 15\n",
            "End of Epoch 15/20\n",
            "\n",
            "Epoch 16/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6617, Train Accuracy: 35.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 26.0606% (172/660)\n",
            "  Box: 56.0831% (378/674)\n",
            "  Pencilcase: 35.6150% (333/935)\n",
            "  Pinkball: 25.2280% (166/658)\n",
            "  StuffedAnimal: 21.6855% (211/973)\n",
            "  Tennis: 26.9425% (267/991)\n",
            "  Waterbottle: 6.9209% (49/708)\n",
            "Validation Loss: 2.1719, Validation Accuracy: 28.15%\n",
            "Saved model for epoch 16\n",
            "End of Epoch 16/20\n",
            "\n",
            "Epoch 17/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6370, Train Accuracy: 36.61%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 27.5758% (182/660)\n",
            "  Box: 56.8249% (383/674)\n",
            "  Pencilcase: 37.0053% (346/935)\n",
            "  Pinkball: 25.0760% (165/658)\n",
            "  StuffedAnimal: 19.8356% (193/973)\n",
            "  Tennis: 27.2452% (270/991)\n",
            "  Waterbottle: 5.9322% (42/708)\n",
            "Validation Loss: 2.1802, Validation Accuracy: 28.24%\n",
            "Saved model for epoch 17\n",
            "End of Epoch 17/20\n",
            "\n",
            "Epoch 18/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6326, Train Accuracy: 37.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 25.7576% (170/660)\n",
            "  Box: 58.1602% (392/674)\n",
            "  Pencilcase: 37.5401% (351/935)\n",
            "  Pinkball: 27.9635% (184/658)\n",
            "  StuffedAnimal: 17.3690% (169/973)\n",
            "  Tennis: 23.4107% (232/991)\n",
            "  Waterbottle: 5.0847% (36/708)\n",
            "Validation Loss: 2.1803, Validation Accuracy: 27.40%\n",
            "Saved model for epoch 18\n",
            "End of Epoch 18/20\n",
            "\n",
            "Epoch 19/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:  34%|███▎      | 376/1119 [00:07<00:12, 57.57it/s, acc=37.4834%, loss=1.6227, lr=0.000250]"
          ]
        }
      ],
      "source": [
        "# Create checkpoint directory if it doesn't exist\n",
        "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
        "\n",
        "# Initialize best metrics tracking\n",
        "best_val_loss = float('inf')\n",
        "best_val_acc = 0\n",
        "class_names = list(CATEGORIES.keys())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config['epochs']):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss, val_acc = validate_model(model, val_loader, criterion, class_names, device)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Update learning rate scheduler\n",
        "    scheduler.step(val_loss)\n",
        "    curr_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Save best model based on validation loss\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_val_acc = val_acc\n",
        "\n",
        "        # Save best model\n",
        "        best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "        }, best_model_path)\n",
        "        wandb.save(best_model_path)  # Save the model to WandB\n",
        "        print(f\"Saved best model with validation loss: {best_val_loss:.4f} and accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Save the model for every epoch\n",
        "    last_model_path = os.path.join(config['checkpoint_dir'], f'model_epoch_{epoch+1}.pth')\n",
        "    torch.save(model.state_dict(), last_model_path)\n",
        "    wandb.save(last_model_path)  # Save the model to WandB\n",
        "    print(f\"Saved model for epoch {epoch+1}\")\n",
        "\n",
        "    # Logging metrics to WandB\n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'learning_rate': curr_lr\n",
        "    }, step=epoch)\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}/{config['epochs']}\")\n",
        "\n",
        "# Final message\n",
        "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "RaHhO_4GASQK"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader, criterion, class_names, device, checkpoint_dir=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test dataset and generate detailed performance metrics\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    # Initialize metrics\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Store all predictions and ground truth for analysis\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = [] # Store probabilities for confidence analysis\n",
        "    # Per-class statistics\n",
        "    class_correct = {class_name: 0 for class_name in class_names}\n",
        "    class_total = {class_name: 0 for class_name in class_names}\n",
        "    # Create progress bar\n",
        "    test_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\", ncols=100)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_bar:\n",
        "            # Get inputs and labels\n",
        "            inputs, targets = data\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Handle different output formats (dictionary vs. tensor)\n",
        "            if isinstance(outputs, dict) and 'out' in outputs:\n",
        "                outputs_for_loss = outputs['out']\n",
        "            else:\n",
        "                outputs_for_loss = outputs\n",
        "\n",
        "            loss = criterion(outputs_for_loss, targets)\n",
        "\n",
        "            # Calculate loss and accuracy\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Get predictions and probabilities\n",
        "            if isinstance(outputs, dict) and 'out' in outputs:\n",
        "                probs = torch.nn.functional.softmax(outputs['out'], dim=1)\n",
        "                _, predicted = torch.max(outputs['out'], 1)\n",
        "            else:\n",
        "                probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Update total counts\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "            # Update per-class counts\n",
        "            for i in range(targets.size(0)):\n",
        "                label = targets[i].item()\n",
        "                pred = predicted[i].item()\n",
        "                class_name = class_names[label]\n",
        "                class_total[class_name] += 1\n",
        "                if pred == label:\n",
        "                    class_correct[class_name] += 1\n",
        "\n",
        "            # Store predictions and targets for later analysis\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            test_bar.set_postfix({\n",
        "                'loss': f\"{test_loss/total:.4f}\",\n",
        "                'acc': f\"{100.0*correct/total:.2f}%\"\n",
        "            })\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = correct / total\n",
        "\n",
        "    # Calculate per-class accuracy\n",
        "    class_accuracy = {name: class_correct[name]/class_total[name] if class_total[name] > 0 else 0\n",
        "                    for name in class_names}\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f} ({correct}/{total})\")\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    for class_name in class_names:\n",
        "        print(f\" {class_name}: {class_accuracy[class_name]:.4f} ({class_correct[class_name]}/{class_total[class_name]})\")\n",
        "\n",
        "    # Return comprehensive metrics dictionary\n",
        "    return {\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_acc,\n",
        "        'class_accuracy': class_accuracy,\n",
        "        'predictions': all_preds,\n",
        "        'targets': all_targets,\n",
        "        'probabilities': all_probs\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model (optional - if you saved a checkpoint)\n",
        "best_model_path = f\"{config['checkpoint_dir']}/best_model.pth\"\n",
        "if os.path.exists(best_model_path):\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint.get('epoch', 'unknown')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrET4H_jISc-",
        "outputId": "c19b71cc-c1bf-4a1c-8abe-374880c2e099"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded best model from epoch 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW5hIq_XASQK",
        "outputId": "175ba265-3747-4c4a-8f14-27e988453774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Testing:   0%|                                                           | 0/345 [00:00<?, ?batch/s]\u001b[A\n",
            "Testing:   0%|                                  | 0/345 [00:00<?, ?batch/s, loss=1.6382, acc=12.50%]\u001b[A\n",
            "Testing:   0%|                                   | 0/345 [00:00<?, ?batch/s, loss=1.8131, acc=6.25%]\u001b[A\n",
            "Testing:   0%|                                   | 0/345 [00:00<?, ?batch/s, loss=1.8679, acc=4.17%]\u001b[A\n",
            "Testing:   0%|                                  | 0/345 [00:00<?, ?batch/s, loss=1.6960, acc=18.75%]\u001b[A\n",
            "Testing:   0%|                                  | 0/345 [00:00<?, ?batch/s, loss=2.0016, acc=15.00%]\u001b[A\n",
            "Testing:   0%|                                  | 0/345 [00:00<?, ?batch/s, loss=2.1837, acc=12.50%]\u001b[A\n",
            "Testing:   0%|                                  | 0/345 [00:00<?, ?batch/s, loss=2.3176, acc=10.71%]\u001b[A\n",
            "Testing:   0%|                                  | 0/345 [00:00<?, ?batch/s, loss=2.2876, acc=14.45%]\u001b[A\n",
            "Testing:   0%|                                  | 0/345 [00:00<?, ?batch/s, loss=2.2238, acc=22.22%]\u001b[A\n",
            "Testing:   0%|                                  | 0/345 [00:00<?, ?batch/s, loss=2.1723, acc=30.00%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.1723, acc=30.00%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.1090, acc=33.52%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.1286, acc=32.29%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.1092, acc=29.81%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.0926, acc=27.68%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.0514, acc=28.75%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.0177, acc=29.30%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.0248, acc=27.57%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.0204, acc=26.74%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.0019, acc=27.14%]\u001b[A\n",
            "Testing:   3%|▋                        | 10/345 [00:00<00:03, 96.49batch/s, loss=2.0528, acc=26.09%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.0528, acc=26.09%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.0638, acc=25.30%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.1607, acc=24.15%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.2579, acc=23.10%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.3461, acc=22.14%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.3432, acc=21.50%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.3319, acc=20.67%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.3220, acc=19.91%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.3336, acc=19.20%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.3297, acc=19.61%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.3091, acc=22.29%]\u001b[A\n",
            "Testing:   6%|█▍                       | 20/345 [00:00<00:03, 95.45batch/s, loss=2.2893, acc=24.70%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2893, acc=24.70%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2774, acc=24.12%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2621, acc=23.39%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2498, acc=22.70%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2345, acc=22.41%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2070, acc=23.18%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2052, acc=22.55%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2034, acc=21.96%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2084, acc=21.71%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.2023, acc=22.11%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.1958, acc=22.10%]\u001b[A\n",
            "Testing:   9%|██▏                      | 31/345 [00:00<00:03, 98.05batch/s, loss=2.1913, acc=21.58%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.1913, acc=21.58%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.1957, acc=21.08%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.1999, acc=20.60%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2414, acc=20.14%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2521, acc=20.04%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2462, acc=19.68%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2375, acc=19.27%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2529, acc=18.88%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2792, acc=18.50%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2835, acc=18.14%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2772, acc=17.79%]\u001b[A\n",
            "Testing:  12%|██▉                     | 42/345 [00:00<00:02, 101.21batch/s, loss=2.2769, acc=17.45%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.2769, acc=17.45%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.3056, acc=17.13%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.3465, acc=16.82%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.3858, acc=16.52%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.4125, acc=16.23%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.3945, acc=16.76%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.3898, acc=16.47%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.3852, acc=16.20%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.3844, acc=16.03%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.4053, acc=15.88%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.4115, acc=15.62%]\u001b[A\n",
            "Testing:  15%|███▋                    | 53/345 [00:00<00:02, 101.03batch/s, loss=2.4052, acc=15.38%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.4052, acc=15.38%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3995, acc=15.14%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3868, acc=15.25%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3731, acc=15.62%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3728, acc=15.53%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3716, acc=15.31%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3704, acc=15.09%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3586, acc=15.54%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3789, acc=15.45%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3926, acc=15.28%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3842, acc=15.08%]\u001b[A\n",
            "Testing:  19%|████▍                   | 64/345 [00:00<00:02, 100.96batch/s, loss=2.3761, acc=14.88%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3761, acc=14.88%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3667, acc=15.30%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3591, acc=15.30%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3533, acc=15.10%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3477, acc=14.91%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3657, acc=14.73%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3579, acc=15.20%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3501, acc=16.23%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3428, acc=17.13%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3360, acc=17.08%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3344, acc=17.02%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3377, acc=17.15%]\u001b[A\n",
            "Testing:  22%|█████▏                  | 75/345 [00:00<00:02, 100.93batch/s, loss=2.3561, acc=16.95%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3561, acc=16.95%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3740, acc=16.76%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3910, acc=16.57%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3895, acc=16.74%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3809, acc=17.65%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3726, acc=18.55%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3612, acc=18.78%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3614, acc=18.58%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3552, acc=18.39%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3518, acc=18.20%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3619, acc=18.01%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3594, acc=17.83%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3563, acc=17.65%]\u001b[A\n",
            "Testing:  25%|██████                  | 87/345 [00:00<00:02, 106.10batch/s, loss=2.3531, acc=17.50%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:00<00:02, 110.78batch/s, loss=2.3531, acc=17.50%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:00<00:02, 110.78batch/s, loss=2.3568, acc=17.33%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:00<00:02, 110.78batch/s, loss=2.3568, acc=17.16%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:00<00:02, 110.78batch/s, loss=2.3572, acc=16.99%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:00<00:02, 110.78batch/s, loss=2.3577, acc=16.83%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:01<00:02, 110.78batch/s, loss=2.3602, acc=16.73%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:01<00:02, 110.78batch/s, loss=2.3649, acc=16.60%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:01<00:02, 110.78batch/s, loss=2.3571, acc=17.14%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:01<00:02, 110.78batch/s, loss=2.3494, acc=17.91%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:01<00:02, 110.78batch/s, loss=2.3483, acc=18.35%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:01<00:02, 110.78batch/s, loss=2.3480, acc=18.30%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:01<00:02, 110.78batch/s, loss=2.3520, acc=18.13%]\u001b[A\n",
            "Testing:  29%|██████▋                | 100/345 [00:01<00:02, 110.78batch/s, loss=2.3462, acc=18.25%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3462, acc=18.25%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3367, acc=18.42%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3298, acc=18.26%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3243, acc=18.10%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3211, acc=18.29%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3183, acc=18.19%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3189, acc=18.03%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3143, acc=17.88%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3086, acc=17.89%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3045, acc=18.10%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3085, acc=17.96%]\u001b[A\n",
            "Testing:  32%|███████▍               | 112/345 [00:01<00:02, 107.44batch/s, loss=2.3064, acc=17.81%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3064, acc=17.81%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3044, acc=17.67%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3103, acc=17.55%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3096, acc=17.41%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3077, acc=17.27%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3063, acc=17.14%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3063, acc=17.01%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3047, acc=17.14%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3101, acc=17.18%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.3015, acc=17.80%]\u001b[A\n",
            "Testing:  36%|████████▌               | 123/345 [00:01<00:02, 98.26batch/s, loss=2.2943, acc=18.42%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2943, acc=18.42%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2909, acc=18.80%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2886, acc=18.75%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2872, acc=18.61%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2858, acc=18.48%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2925, acc=18.39%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2917, acc=18.32%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2893, acc=18.19%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2867, acc=18.09%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.2870, acc=18.02%]\u001b[A\n",
            "Testing:  39%|█████████▎              | 133/345 [00:01<00:02, 93.66batch/s, loss=2.3034, acc=17.90%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3034, acc=17.90%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3190, acc=17.77%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3332, acc=17.65%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3383, acc=17.68%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3501, acc=17.56%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3619, acc=17.44%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3695, acc=17.39%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3733, acc=17.33%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3731, acc=17.32%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3699, acc=17.21%]\u001b[A\n",
            "Testing:  41%|█████████▉              | 143/345 [00:01<00:02, 89.84batch/s, loss=2.3676, acc=17.10%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3676, acc=17.10%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3653, acc=17.00%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3620, acc=17.08%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3645, acc=16.97%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3622, acc=16.86%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3600, acc=16.75%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3605, acc=16.65%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3691, acc=16.58%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.3881, acc=16.48%]\u001b[A\n",
            "Testing:  44%|██████████▋             | 153/345 [00:01<00:02, 84.21batch/s, loss=2.4069, acc=16.38%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4069, acc=16.38%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4154, acc=16.37%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4267, acc=16.29%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4389, acc=16.19%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4513, acc=16.10%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4563, acc=16.09%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4615, acc=16.00%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4628, acc=15.90%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4648, acc=15.81%]\u001b[A\n",
            "Testing:  47%|███████████▎            | 162/345 [00:01<00:02, 83.74batch/s, loss=2.4620, acc=15.94%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4620, acc=15.94%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4588, acc=15.92%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4589, acc=15.93%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4584, acc=15.84%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4578, acc=15.75%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4634, acc=15.66%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4648, acc=15.57%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4649, acc=15.48%]\u001b[A\n",
            "Testing:  50%|███████████▉            | 171/345 [00:01<00:02, 79.30batch/s, loss=2.4650, acc=15.40%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:01<00:02, 77.92batch/s, loss=2.4650, acc=15.40%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:01<00:02, 77.92batch/s, loss=2.4697, acc=15.31%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:01<00:02, 77.92batch/s, loss=2.4721, acc=15.23%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:01<00:02, 77.92batch/s, loss=2.4713, acc=15.14%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:01<00:02, 77.92batch/s, loss=2.4705, acc=15.06%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:01<00:02, 77.92batch/s, loss=2.4720, acc=15.03%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:02<00:02, 77.92batch/s, loss=2.4688, acc=14.95%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:02<00:02, 77.92batch/s, loss=2.4657, acc=14.87%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:02<00:02, 77.92batch/s, loss=2.4629, acc=14.79%]\u001b[A\n",
            "Testing:  52%|████████████▍           | 179/345 [00:02<00:02, 77.92batch/s, loss=2.4606, acc=14.71%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4606, acc=14.71%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4568, acc=14.68%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4530, acc=14.61%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4494, acc=14.56%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4530, acc=14.49%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4558, acc=14.48%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4591, acc=14.42%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4582, acc=14.34%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4577, acc=14.27%]\u001b[A\n",
            "Testing:  54%|█████████████           | 188/345 [00:02<00:01, 78.81batch/s, loss=2.4581, acc=14.21%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4581, acc=14.21%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4600, acc=14.24%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4574, acc=14.16%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4540, acc=14.09%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4486, acc=14.23%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4438, acc=14.51%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4450, acc=14.52%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4500, acc=14.45%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4588, acc=14.38%]\u001b[A\n",
            "Testing:  57%|█████████████▋          | 197/345 [00:02<00:01, 81.70batch/s, loss=2.4659, acc=14.35%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4659, acc=14.35%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4618, acc=14.57%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4556, acc=14.87%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4623, acc=14.80%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4699, acc=14.73%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4767, acc=14.66%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4735, acc=14.78%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4660, acc=15.08%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4635, acc=15.01%]\u001b[A\n",
            "Testing:  60%|██████████████▎         | 206/345 [00:02<00:01, 80.92batch/s, loss=2.4610, acc=14.94%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4610, acc=14.94%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4594, acc=14.99%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4676, acc=14.92%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4648, acc=14.95%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4575, acc=15.28%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4559, acc=15.36%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4535, acc=15.29%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4589, acc=15.22%]\u001b[A\n",
            "Testing:  62%|██████████████▉         | 215/345 [00:02<00:01, 79.26batch/s, loss=2.4654, acc=15.15%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4654, acc=15.15%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4718, acc=15.08%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4769, acc=15.01%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4795, acc=14.95%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4778, acc=14.88%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4760, acc=14.82%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4693, acc=15.07%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4677, acc=15.00%]\u001b[A\n",
            "Testing:  65%|███████████████▌        | 223/345 [00:02<00:01, 78.25batch/s, loss=2.4660, acc=14.94%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4660, acc=14.94%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4648, acc=14.94%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4685, acc=14.90%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4767, acc=14.84%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4848, acc=14.77%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4884, acc=14.78%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4874, acc=14.74%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4847, acc=14.68%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4819, acc=14.62%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4801, acc=14.56%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4789, acc=14.60%]\u001b[A\n",
            "Testing:  67%|████████████████        | 231/345 [00:02<00:01, 75.70batch/s, loss=2.4762, acc=14.54%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4762, acc=14.54%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4778, acc=14.48%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4772, acc=14.42%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4764, acc=14.36%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4757, acc=14.32%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4781, acc=14.31%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4761, acc=14.25%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4741, acc=14.19%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4732, acc=14.29%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4750, acc=14.23%]\u001b[A\n",
            "Testing:  70%|████████████████▊       | 242/345 [00:02<00:01, 83.77batch/s, loss=2.4756, acc=14.17%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.4756, acc=14.17%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.4764, acc=14.12%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.4840, acc=14.07%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.4903, acc=14.02%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.4965, acc=13.96%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.5023, acc=13.91%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.5080, acc=13.86%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.5083, acc=13.84%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.5140, acc=13.79%]\u001b[A\n",
            "Testing:  73%|█████████████████▌      | 252/345 [00:02<00:01, 87.94batch/s, loss=2.5198, acc=13.73%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:02<00:00, 84.23batch/s, loss=2.5198, acc=13.73%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:02<00:00, 84.23batch/s, loss=2.5244, acc=13.68%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:02<00:00, 84.23batch/s, loss=2.5272, acc=13.64%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:02<00:00, 84.23batch/s, loss=2.5257, acc=13.59%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:02<00:00, 84.23batch/s, loss=2.5240, acc=13.54%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:02<00:00, 84.23batch/s, loss=2.5206, acc=13.63%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:03<00:00, 84.23batch/s, loss=2.5162, acc=13.72%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:03<00:00, 84.23batch/s, loss=2.5134, acc=13.67%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:03<00:00, 84.23batch/s, loss=2.5107, acc=13.62%]\u001b[A\n",
            "Testing:  76%|██████████████████▏     | 261/345 [00:03<00:00, 84.23batch/s, loss=2.5107, acc=13.61%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5107, acc=13.61%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5112, acc=13.56%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5097, acc=13.51%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5081, acc=13.46%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5057, acc=13.50%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5079, acc=13.45%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5085, acc=13.41%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5110, acc=13.36%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5098, acc=13.55%]\u001b[A\n",
            "Testing:  78%|██████████████████▊     | 270/345 [00:03<00:00, 82.50batch/s, loss=2.5081, acc=13.62%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5081, acc=13.62%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5101, acc=13.68%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5159, acc=13.63%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5218, acc=13.59%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5256, acc=13.56%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5224, acc=13.56%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5208, acc=13.51%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5194, acc=13.46%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5194, acc=13.41%]\u001b[A\n",
            "Testing:  81%|███████████████████▍    | 279/345 [00:03<00:00, 79.76batch/s, loss=2.5199, acc=13.37%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5199, acc=13.37%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5203, acc=13.32%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5204, acc=13.28%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5193, acc=13.28%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5171, acc=13.24%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5149, acc=13.19%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5099, acc=13.38%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5068, acc=13.44%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5056, acc=13.40%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5047, acc=13.35%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5034, acc=13.40%]\u001b[A\n",
            "Testing:  83%|████████████████████    | 288/345 [00:03<00:00, 78.23batch/s, loss=2.5013, acc=13.48%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.5013, acc=13.48%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4982, acc=13.52%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4981, acc=13.48%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4976, acc=13.43%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4971, acc=13.40%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4977, acc=13.37%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4960, acc=13.37%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4937, acc=13.33%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4914, acc=13.28%]\u001b[A\n",
            "Testing:  87%|████████████████████▊   | 299/345 [00:03<00:00, 84.23batch/s, loss=2.4922, acc=13.26%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.4922, acc=13.26%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.4914, acc=13.27%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.4884, acc=13.42%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.4854, acc=13.70%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.4825, acc=13.97%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.4860, acc=13.97%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.4905, acc=13.94%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.4953, acc=13.90%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.5001, acc=13.85%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.5054, acc=13.81%]\u001b[A\n",
            "Testing:  89%|█████████████████████▍  | 308/345 [00:03<00:00, 83.77batch/s, loss=2.5007, acc=14.03%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.5007, acc=14.03%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4983, acc=14.01%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4960, acc=13.96%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4926, acc=14.06%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4889, acc=14.22%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4915, acc=14.19%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4920, acc=14.15%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4904, acc=14.11%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4893, acc=14.06%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4887, acc=14.09%]\u001b[A\n",
            "Testing:  92%|██████████████████████  | 318/345 [00:03<00:00, 86.14batch/s, loss=2.4874, acc=14.06%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4874, acc=14.06%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4849, acc=14.06%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4828, acc=14.02%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4811, acc=14.01%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4793, acc=14.09%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4774, acc=14.08%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4758, acc=14.03%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4727, acc=14.10%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4684, acc=14.25%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4668, acc=14.21%]\u001b[A\n",
            "Testing:  95%|██████████████████████▊ | 328/345 [00:03<00:00, 88.67batch/s, loss=2.4652, acc=14.16%]\u001b[A\n",
            "Testing:  98%|███████████████████████▌| 338/345 [00:03<00:00, 91.32batch/s, loss=2.4652, acc=14.16%]\u001b[A\n",
            "Testing:  98%|███████████████████████▌| 338/345 [00:03<00:00, 91.32batch/s, loss=2.4615, acc=14.33%]\u001b[A\n",
            "Testing:  98%|███████████████████████▌| 338/345 [00:03<00:00, 91.32batch/s, loss=2.4574, acc=14.58%]\u001b[A\n",
            "Testing:  98%|███████████████████████▌| 338/345 [00:03<00:00, 91.32batch/s, loss=2.4541, acc=14.83%]\u001b[A\n",
            "Testing:  98%|███████████████████████▌| 338/345 [00:03<00:00, 91.32batch/s, loss=2.4508, acc=15.03%]\u001b[A\n",
            "Testing:  98%|███████████████████████▌| 338/345 [00:03<00:00, 91.32batch/s, loss=2.4464, acc=15.21%]\u001b[A\n",
            "Testing:  98%|███████████████████████▌| 338/345 [00:03<00:00, 91.32batch/s, loss=2.4436, acc=15.35%]\u001b[A\n",
            "Testing: 100%|████████████████████████| 345/345 [00:03<00:00, 88.81batch/s, loss=2.4440, acc=15.35%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TEST RESULTS\n",
            "==================================================\n",
            "Test Loss: 2.4440\n",
            "Test Accuracy: 0.1535 (1690/11012)\n",
            "\n",
            "Per-Class Accuracy:\n",
            " Blueball: 0.0946 (190/2008)\n",
            " Box: 0.1678 (243/1448)\n",
            " Pencilcase: 0.1359 (189/1391)\n",
            " Pinkball: 0.1807 (358/1981)\n",
            " StuffedAnimal: 0.1446 (204/1411)\n",
            " Tennis: 0.2677 (378/1412)\n",
            " Waterbottle: 0.0940 (128/1361)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate on test set\n",
        "test_results = test_model(\n",
        "    model=model,\n",
        "    test_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    class_names=class_names,\n",
        "    device=device,\n",
        "    checkpoint_dir=config['checkpoint_dir']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "hz-ZIzCAASQK",
        "outputId": "155ede34-e8ec-4ffd-97c5-76d7b4715bff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▄▄▄▃▃▃▃▃▃▂▁</td></tr><tr><td>train_acc</td><td>▁▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▂▂▁</td></tr><tr><td>val_acc</td><td>▁▅▆▆▅▆▆▆▆▆▇█▇█▇██▇▇▇</td></tr><tr><td>val_loss</td><td>▁▃▆▅▇▃▄▆▅▅▅▆▅▆▇▆▆▆▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate</td><td>2e-05</td></tr><tr><td>train_acc</td><td>45.55686</td></tr><tr><td>train_loss</td><td>1.47727</td></tr><tr><td>val_acc</td><td>27.07661</td></tr><tr><td>val_loss</td><td>2.21938</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">12run</strong> at: <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/4unnsyut' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/4unnsyut</a><br> View project at: <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 21 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_201801-4unnsyut/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nhVxRpgASQK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}