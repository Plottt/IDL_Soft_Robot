{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contact-based Object Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories for classification\n",
    "CATEGORIES = {\n",
    "    'Blueball': 0,\n",
    "    'Box': 1,\n",
    "    'Pencilcase': 2,\n",
    "    'Pinkball': 3,\n",
    "    'StuffedAnimal': 4,\n",
    "    'Tennis': 5,\n",
    "    'Waterbottle': 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Dictionary\n",
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'lr': 0.001,\n",
    "    'epochs': 20,\n",
    "    'data_dir': \"/Users/benlee/Documents/college/CMU/Spring 2025/IDL/Project/IDL_code/IDL_Data\",\n",
    "    'checkpoint_dir': \"/Users/benlee/Documents/college/CMU/Spring 2025/IDL/Project/IDL_code/checkpoint\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContactWindowDataset(Dataset):\n",
    "    def __init__(self, data_dir: str, labels: Dict[str, int] = None, window_size: int = 50, step_size: int = 10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (str): Directory containing the .txt files\n",
    "            labels (Dict[str, int]): Dictionary mapping categories to class labels\n",
    "            window_size (int): Size of the sliding window (smaller = more samples)\n",
    "            step_size (int): Step size for the sliding window (smaller = more samples)\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.file_paths = list(self.data_dir.glob(\"*.txt\"))\n",
    "        self.labels = labels or {}\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        # Lists to store all windows and their labels\n",
    "        self.features_list = []\n",
    "        self.labels_list = []\n",
    "        self.file_indices = []  # To keep track of which file each window came from\n",
    "        \n",
    "        # Process all files\n",
    "        print(\"Processing files and extracting windows:\")\n",
    "        for file_idx, file_path in enumerate(self.file_paths):\n",
    "            # Get category from filename\n",
    "            category = re.sub(r\"\\d+\", \"\", file_path.stem)\n",
    "            label = self.labels.get(category, -1)\n",
    "            \n",
    "            # Load and process the file\n",
    "            df = self._parse_file(file_path)\n",
    "            \n",
    "            # Create windows\n",
    "            windows_from_file = 0\n",
    "            for start_idx in range(0, len(df) - window_size + 1, step_size):\n",
    "                window = df.iloc[start_idx:start_idx + window_size]\n",
    "                \n",
    "                # Extract features from window\n",
    "                features = self._extract_features(window)\n",
    "                \n",
    "                self.features_list.append(features)\n",
    "                self.labels_list.append(label)\n",
    "                self.file_indices.append(file_idx)\n",
    "                windows_from_file += 1\n",
    "            \n",
    "            print(f\"  {file_path.name}: {windows_from_file} windows generated from {len(df)} datapoints\")\n",
    "        \n",
    "        # Convert lists to tensors for efficiency\n",
    "        self.features = torch.FloatTensor(self.features_list)\n",
    "        self.labels = torch.LongTensor(self.labels_list)\n",
    "        self.file_indices = torch.LongTensor(self.file_indices)\n",
    "        \n",
    "        # Print summary statistics\n",
    "        self._print_dataset_stats()\n",
    "        \n",
    "    def _parse_file(self, file_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"Parse a single data file\"\"\"\n",
    "        df = pd.read_csv(file_path, header=None, skiprows=1)\n",
    "        columns = [\n",
    "            'timestamp_pc', 'timestamp_micro',\n",
    "            'x', 'y', 'angle_1', 'angle_2',\n",
    "            'contact_1_left', 'contact_1_right',\n",
    "            'contact_2_left', 'contact_2_right'\n",
    "        ]\n",
    "        df = pd.DataFrame(df.values, columns=columns)\n",
    "        return df\n",
    "    \n",
    "    def _extract_features(self, window: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Extract features from a window of data\"\"\"\n",
    "        # Basic statistical features\n",
    "        features = np.array([\n",
    "            window['contact_1_left'].mean(),\n",
    "            window['contact_1_right'].mean(),\n",
    "            window['contact_2_left'].mean(),\n",
    "            window['contact_2_right'].mean(),\n",
    "            window['x'].max() - window['x'].min(),\n",
    "            window['y'].max() - window['y'].min(),\n",
    "            window['angle_1'].std(),\n",
    "            window['angle_2'].std(),\n",
    "            # Additional features for more information\n",
    "            window['contact_1_left'].std(),\n",
    "            window['contact_1_right'].std(),\n",
    "            window['contact_2_left'].std(),\n",
    "            window['contact_2_right'].std(),\n",
    "            window['angle_1'].mean(),\n",
    "            window['angle_2'].mean()\n",
    "        ])\n",
    "        return features\n",
    "    \n",
    "    def _print_dataset_stats(self):\n",
    "        \"\"\"Print statistics about the dataset\"\"\"\n",
    "        total_windows = len(self.features)\n",
    "        unique_files = len(torch.unique(self.file_indices))\n",
    "        \n",
    "        # Count samples per category\n",
    "        category_counts = {}\n",
    "        for label in self.labels_list:\n",
    "            category_name = list(CATEGORIES.keys())[list(CATEGORIES.values()).index(label)]\n",
    "            category_counts[category_name] = category_counts.get(category_name, 0) + 1\n",
    "        \n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Total number of windows: {total_windows}\")\n",
    "        print(f\"Total number of files: {unique_files}\")\n",
    "        print(f\"Average windows per file: {total_windows / unique_files:.2f}\")\n",
    "        print(\"\\nSamples per category:\")\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"  {category}: {count} windows\")\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.features)\n",
    "        \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split dataset while preventing data leakage\n",
    "def split_dataset_by_files(dataset, train_ratio=0.7, val_ratio=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Split the dataset by files to prevent data leakage.\n",
    "    Windows from the same file will stay in the same split.\n",
    "    \"\"\"\n",
    "    # Get unique file indices\n",
    "    unique_files = torch.unique(dataset.file_indices).tolist()\n",
    "    \n",
    "    # Shuffle the files\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(unique_files)\n",
    "    \n",
    "    # Split files into train, val, test\n",
    "    n_files = len(unique_files)\n",
    "    train_files = unique_files[:int(train_ratio * n_files)]\n",
    "    val_files = unique_files[int(train_ratio * n_files):int((train_ratio + val_ratio) * n_files)]\n",
    "    test_files = unique_files[int((train_ratio + val_ratio) * n_files):]\n",
    "    \n",
    "    # Get indices for each split\n",
    "    train_indices = [i for i, file_idx in enumerate(dataset.file_indices) if file_idx in train_files]\n",
    "    val_indices = [i for i, file_idx in enumerate(dataset.file_indices) if file_idx in val_files]\n",
    "    test_indices = [i for i, file_idx in enumerate(dataset.file_indices) if file_idx in test_files]\n",
    "    \n",
    "    return train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files and extracting windows:\n",
      "  Pencilcase4.txt: 482 windows generated from 4916 datapoints\n",
      "  Pinkball3.txt: 467 windows generated from 4760 datapoints\n",
      "  Pinkball2.txt: 460 windows generated from 4695 datapoints\n",
      "  Pencilcase5.txt: 531 windows generated from 5404 datapoints\n",
      "  Pinkball1.txt: 481 windows generated from 4901 datapoints\n",
      "  Pencilcase2.txt: 615 windows generated from 6249 datapoints\n",
      "  Pinkball5.txt: 473 windows generated from 4828 datapoints\n",
      "  Pinkball4.txt: 456 windows generated from 4658 datapoints\n",
      "  Pencilcase3.txt: 602 windows generated from 6119 datapoints\n",
      "  Pencilcase1.txt: 504 windows generated from 5135 datapoints\n",
      "  Pinkball6.txt: 458 windows generated from 4679 datapoints\n",
      "  Box4.txt: 483 windows generated from 4926 datapoints\n",
      "  Box5.txt: 530 windows generated from 5399 datapoints\n",
      "  Box1.txt: 509 windows generated from 5187 datapoints\n",
      "  Box2.txt: 467 windows generated from 4766 datapoints\n",
      "  Box3.txt: 484 windows generated from 4939 datapoints\n",
      "  Blueball6.txt: 470 windows generated from 4792 datapoints\n",
      "  Waterbottle4.txt: 538 windows generated from 5471 datapoints\n",
      "  Blueball5.txt: 470 windows generated from 4790 datapoints\n",
      "  Blueball4.txt: 471 windows generated from 4802 datapoints\n",
      "  Waterbottle5.txt: 485 windows generated from 4948 datapoints\n",
      "  Waterbottle1.txt: 488 windows generated from 4971 datapoints\n",
      "  Blueball1.txt: 449 windows generated from 4581 datapoints\n",
      "  Waterbottle2.txt: 466 windows generated from 4750 datapoints\n",
      "  Blueball3.txt: 456 windows generated from 4656 datapoints\n",
      "  Blueball2.txt: 469 windows generated from 4781 datapoints\n",
      "  Waterbottle3.txt: 467 windows generated from 4768 datapoints\n",
      "  StuffedAnimal1.txt: 464 windows generated from 4732 datapoints\n",
      "  Tennis1.txt: 500 windows generated from 5093 datapoints\n",
      "  Tennis3.txt: 493 windows generated from 5025 datapoints\n",
      "  StuffedAnimal2.txt: 507 windows generated from 5169 datapoints\n",
      "  StuffedAnimal3.txt: 510 windows generated from 5192 datapoints\n",
      "  Tennis2.txt: 631 windows generated from 6405 datapoints\n",
      "  Tennis5.txt: 466 windows generated from 4756 datapoints\n",
      "  StuffedAnimal4.txt: 491 windows generated from 5008 datapoints\n",
      "  StuffedAnimal5.txt: 352 windows generated from 3615 datapoints\n",
      "  Tennis4.txt: 524 windows generated from 5330 datapoints\n",
      "\n",
      "Dataset Statistics:\n",
      "Total number of windows: 18169\n",
      "Total number of files: 37\n",
      "Average windows per file: 491.05\n",
      "\n",
      "Samples per category:\n",
      "  Pencilcase: 2734 windows\n",
      "  Pinkball: 2795 windows\n",
      "  Box: 2473 windows\n",
      "  Blueball: 2785 windows\n",
      "  Waterbottle: 2444 windows\n",
      "  StuffedAnimal: 2324 windows\n",
      "  Tennis: 2614 windows\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset with smaller window and step size to maximize samples\n",
    "window_size = 100   # Smaller window size generates more samples\n",
    "step_size = 10     # Smaller step size creates more overlapping windows\n",
    "dataset = ContactWindowDataset(\n",
    "    data_dir=config[\"data_dir\"], \n",
    "    labels=CATEGORIES, \n",
    "    window_size=window_size, \n",
    "    step_size=step_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Splits:\n",
      "Training set size: 12423 windows\n",
      "Validation set size: 2919 windows\n",
      "Test set size: 2827 windows\n",
      "\n",
      "Sample batch:\n",
      "Batch features shape: torch.Size([32, 14])\n",
      "Batch labels shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset by files to prevent data leakage\n",
    "train_indices, val_indices, test_indices = split_dataset_by_files(dataset)\n",
    "\n",
    "# Create subset datasets\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "# Print split information\n",
    "print(\"\\nDataset Splits:\")\n",
    "print(f\"Training set size: {len(train_dataset)} windows\")\n",
    "print(f\"Validation set size: {len(val_dataset)} windows\")\n",
    "print(f\"Test set size: {len(test_dataset)} windows\")\n",
    "\n",
    "# Example of accessing a batch\n",
    "for features, labels in train_loader:\n",
    "    print(f\"\\nSample batch:\")\n",
    "    print(f\"Batch features shape: {features.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "       BatchNorm1d-1                   [-1, 14]              28\n",
      "            Linear-2                  [-1, 256]           3,840\n",
      "       BatchNorm1d-3                  [-1, 256]             512\n",
      "              GELU-4                  [-1, 256]               0\n",
      "           Dropout-5                  [-1, 256]               0\n",
      "            Linear-6                  [-1, 512]         131,584\n",
      "       BatchNorm1d-7                  [-1, 512]           1,024\n",
      "              GELU-8                  [-1, 512]               0\n",
      "            Linear-9                  [-1, 512]         262,656\n",
      "      BatchNorm1d-10                  [-1, 512]           1,024\n",
      "             GELU-11                  [-1, 512]               0\n",
      "          Dropout-12                  [-1, 512]               0\n",
      "           Linear-13                  [-1, 256]         131,328\n",
      "      BatchNorm1d-14                  [-1, 256]             512\n",
      "             GELU-15                  [-1, 256]               0\n",
      "           Linear-16                  [-1, 128]          32,896\n",
      "      BatchNorm1d-17                  [-1, 128]             256\n",
      "             GELU-18                  [-1, 128]               0\n",
      "           Linear-19                    [-1, 7]             903\n",
      "================================================================\n",
      "Total params: 566,563\n",
      "Trainable params: 566,563\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 2.16\n",
      "Estimated Total Size (MB): 2.21\n",
      "----------------------------------------------------------------\n",
      "Model moved to MPS device\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Define the Neural Network Model for Object Classification\n",
    "class ContactClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ContactClassifier, self).__init__()\n",
    "        \n",
    "        # Specialized feature extraction pathway for contact data\n",
    "        self.model = torch.nn.Sequential(\n",
    "            # Input normalization layer\n",
    "            torch.nn.BatchNorm1d(input_size),\n",
    "            \n",
    "            # Initial feature expansion - smaller width but better feature extraction\n",
    "            torch.nn.Linear(input_size, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.2),  # Reduced dropout to prevent information loss\n",
    "            \n",
    "            # Deeper feature processing (keeping original structure but improving flow)\n",
    "            torch.nn.Linear(256, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.GELU(),\n",
    "            \n",
    "            # Refinement layer - not in original but helps with feature separation\n",
    "            torch.nn.Linear(512, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            \n",
    "            # Added skip connection internally by splitting flow and rejoining\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.BatchNorm1d(256),\n",
    "            torch.nn.GELU(),\n",
    "            \n",
    "            # Squeeze down to focused features\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            torch.nn.GELU(),\n",
    "            \n",
    "            # Final classification layer\n",
    "            torch.nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Proper weight initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                # Kaiming initialization works better with GELU\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 14  # Number of features in your dataset\n",
    "num_classes = len(CATEGORIES)  # Number of object categories\n",
    "model = ContactClassifier(input_size, num_classes)\n",
    "\n",
    "# Try CPU first for compatibility with BatchNorm1d \n",
    "device = torch.device('cpu')  # Change to 'mps' after checking summary\n",
    "model.to(device)\n",
    "summary(model, (input_size,))\n",
    "\n",
    "# After checking summary, move to MPS if available\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        model = model.to('mps')\n",
    "        device = torch.device('mps')\n",
    "        print(\"Model moved to MPS device\")\n",
    "    print(f\"Using device: {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error moving to MPS: {e}\")\n",
    "    print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/var/folders/_k/bg77t7g55md3qh0s5xdb7tg40000gn/T/ipykernel_34221/1789591295.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/opt/miniconda3/lib/python3.9/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define CrossEntropyLoss as the criterion\n",
    "# Standard loss function for multi-class classification problems\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizer with AdamW (Adam with weight decay)\n",
    "# We pass all model parameters and set the learning rate\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0001,\n",
    "    weight_decay=1e-5  # Adding a small weight decay for regularization\n",
    ")\n",
    "\n",
    "# Learning rate scheduler to reduce learning rate when training plateaus\n",
    "# ReduceLROnPlateau reduces learning rate when validation loss stops improving\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',  # Monitor minimum validation loss\n",
    "    factor=0.5,  # Reduce learning rate by half when plateau is detected\n",
    "    patience=5,  # Wait for 5 epochs without improvement before reducing LR\n",
    "    min_lr=1e-6,  # Don't reduce learning rate below this threshold\n",
    "    verbose=True  # Print message when learning rate is reduced\n",
    ")\n",
    "\n",
    "# Mixed Precision, if you need it\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            acc=\"{:.04f}\".format(float(correct / total)),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr']))\n",
    "        )\n",
    "        batch_bar.update()\n",
    "        \n",
    "        # Memory management\n",
    "        del x, y, outputs, loss\n",
    "        if hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    batch_bar.close()\n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc = correct / total *100\n",
    "    \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, class_names, device):\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    for i, data in enumerate(val_loader):\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "            # Store predictions and targets for confusion matrix\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            batch_bar.set_postfix(\n",
    "                loss=\"{:.04f}\".format(float(total_loss / (i + 1))), \n",
    "                acc=\"{:.04f}\".format(float(correct / total))\n",
    "            )\n",
    "            batch_bar.update()\n",
    "        \n",
    "        # Memory management\n",
    "        del x, y, outputs, loss\n",
    "        if hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    batch_bar.close()\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    val_acc = correct / total *100\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/benlee/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"78d5988d9f05a421bc74d044c3cd9afc3b918020\") # API Key is in your wandb account, under settings (wandb.ai/settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/benlee/Documents/college/CMU/Spring 2025/IDL/Project/IDL_code/wandb/run-20250311_211000-cutkkks0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/cutkkks0' target=\"_blank\">04run</a></strong> to <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/cutkkks0' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/cutkkks0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "run = wandb.init(\n",
    "    name = \"04run\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = False, ### Allows reinitalizing runs when you re-run this cell\n",
    "    #id = \"\", ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"object_classification\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|███████████████████████| 389/389 [00:04<00:00, 92.91batch/s, loss=3.8367, acc=17.01%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.8367, Train Accuracy: 0.1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 133.52batch/s, loss=2.5456, acc=13.74%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.5456, Validation Accuracy: 0.1374\n",
      "Saved best model with validation loss: 2.5456 and accuracy: 0.1374\n",
      "Saved model for epoch 1\n",
      "End of Epoch 1/20\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 119.07batch/s, loss=2.7348, acc=19.10%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.7348, Train Accuracy: 0.1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 148.04batch/s, loss=2.2968, acc=13.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.2968, Validation Accuracy: 0.1333\n",
      "Saved best model with validation loss: 2.2968 and accuracy: 0.1333\n",
      "Saved model for epoch 2\n",
      "End of Epoch 2/20\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 121.11batch/s, loss=2.3092, acc=21.15%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3092, Train Accuracy: 0.2115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 147.39batch/s, loss=2.1488, acc=12.50%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1488, Validation Accuracy: 0.1250\n",
      "Saved best model with validation loss: 2.1488 and accuracy: 0.1250\n",
      "Saved model for epoch 3\n",
      "End of Epoch 3/20\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 120.05batch/s, loss=2.1141, acc=21.49%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.1141, Train Accuracy: 0.2149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 140.92batch/s, loss=2.0605, acc=12.44%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0605, Validation Accuracy: 0.1244\n",
      "Saved best model with validation loss: 2.0605 and accuracy: 0.1244\n",
      "Saved model for epoch 4\n",
      "End of Epoch 4/20\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 122.90batch/s, loss=2.0236, acc=22.14%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.0236, Train Accuracy: 0.2214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 143.03batch/s, loss=2.0531, acc=15.72%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0531, Validation Accuracy: 0.1572\n",
      "\n",
      "Per-class Validation Accuracy:\n",
      "  Pencilcase: 0.3503 (186/531)\n",
      "  Pinkball: 0.2227 (102/458)\n",
      "  StuffedAnimal: 0.0321 (32/998)\n",
      "  Tennis: 0.2318 (108/466)\n",
      "  Waterbottle: 0.0665 (31/466)\n",
      "Saved best model with validation loss: 2.0531 and accuracy: 0.1572\n",
      "Saved model for epoch 5\n",
      "End of Epoch 5/20\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 122.29batch/s, loss=1.9516, acc=23.32%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9516, Train Accuracy: 0.2332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 140.34batch/s, loss=2.0330, acc=16.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0330, Validation Accuracy: 0.1641\n",
      "Saved best model with validation loss: 2.0330 and accuracy: 0.1641\n",
      "Saved model for epoch 6\n",
      "End of Epoch 6/20\n",
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 122.11batch/s, loss=1.9089, acc=23.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.9089, Train Accuracy: 0.2355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 155.72batch/s, loss=2.0306, acc=16.82%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0306, Validation Accuracy: 0.1682\n",
      "Saved best model with validation loss: 2.0306 and accuracy: 0.1682\n",
      "Saved model for epoch 7\n",
      "End of Epoch 7/20\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 122.37batch/s, loss=1.8791, acc=24.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8791, Train Accuracy: 0.2443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 153.70batch/s, loss=2.0249, acc=17.30%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0249, Validation Accuracy: 0.1730\n",
      "Saved best model with validation loss: 2.0249 and accuracy: 0.1730\n",
      "Saved model for epoch 8\n",
      "End of Epoch 8/20\n",
      "\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 124.85batch/s, loss=1.8629, acc=24.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8629, Train Accuracy: 0.2479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 128.18batch/s, loss=2.0085, acc=17.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0085, Validation Accuracy: 0.1703\n",
      "Saved best model with validation loss: 2.0085 and accuracy: 0.1703\n",
      "Saved model for epoch 9\n",
      "End of Epoch 9/20\n",
      "\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 123.23batch/s, loss=1.8435, acc=25.48%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8435, Train Accuracy: 0.2548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 152.87batch/s, loss=2.0576, acc=11.92%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0576, Validation Accuracy: 0.1192\n",
      "\n",
      "Per-class Validation Accuracy:\n",
      "  Pencilcase: 0.3315 (176/531)\n",
      "  Pinkball: 0.2555 (117/458)\n",
      "  StuffedAnimal: 0.0080 (8/998)\n",
      "  Tennis: 0.0944 (44/466)\n",
      "  Waterbottle: 0.0064 (3/466)\n",
      "Saved model for epoch 10\n",
      "End of Epoch 10/20\n",
      "\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 120.90batch/s, loss=1.8305, acc=25.77%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8305, Train Accuracy: 0.2577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 146.72batch/s, loss=2.0471, acc=12.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0471, Validation Accuracy: 0.1285\n",
      "Saved model for epoch 11\n",
      "End of Epoch 11/20\n",
      "\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 123.22batch/s, loss=1.8268, acc=25.59%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8268, Train Accuracy: 0.2559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 145.23batch/s, loss=2.0293, acc=15.04%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0293, Validation Accuracy: 0.1504\n",
      "Saved model for epoch 12\n",
      "End of Epoch 12/20\n",
      "\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 120.04batch/s, loss=1.8133, acc=26.68%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8133, Train Accuracy: 0.2668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 140.36batch/s, loss=2.0127, acc=16.27%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0127, Validation Accuracy: 0.1627\n",
      "Saved model for epoch 13\n",
      "End of Epoch 13/20\n",
      "\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 124.03batch/s, loss=1.8042, acc=26.46%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.8042, Train Accuracy: 0.2646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 143.46batch/s, loss=2.0377, acc=13.57%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0377, Validation Accuracy: 0.1357\n",
      "Saved model for epoch 14\n",
      "End of Epoch 14/20\n",
      "\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 123.41batch/s, loss=1.7945, acc=26.74%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7945, Train Accuracy: 0.2674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 139.37batch/s, loss=2.0599, acc=14.18%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0599, Validation Accuracy: 0.1418\n",
      "\n",
      "Per-class Validation Accuracy:\n",
      "  Pencilcase: 0.4633 (246/531)\n",
      "  Pinkball: 0.2358 (108/458)\n",
      "  StuffedAnimal: 0.0220 (22/998)\n",
      "  Tennis: 0.0687 (32/466)\n",
      "  Waterbottle: 0.0129 (6/466)\n",
      "Saved model for epoch 15\n",
      "End of Epoch 15/20\n",
      "\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 122.79batch/s, loss=1.7824, acc=28.05%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7824, Train Accuracy: 0.2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 143.03batch/s, loss=2.0497, acc=14.97%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0497, Validation Accuracy: 0.1497\n",
      "Saved model for epoch 16\n",
      "End of Epoch 16/20\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 123.44batch/s, loss=1.7813, acc=27.29%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7813, Train Accuracy: 0.2729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 142.95batch/s, loss=2.0364, acc=16.31%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0364, Validation Accuracy: 0.1631\n",
      "Saved model for epoch 17\n",
      "End of Epoch 17/20\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 124.58batch/s, loss=1.7704, acc=27.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7704, Train Accuracy: 0.2787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 145.78batch/s, loss=2.0040, acc=18.98%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0040, Validation Accuracy: 0.1898\n",
      "Saved best model with validation loss: 2.0040 and accuracy: 0.1898\n",
      "Saved model for epoch 18\n",
      "End of Epoch 18/20\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 123.35batch/s, loss=1.7730, acc=27.79%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7730, Train Accuracy: 0.2779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 146.08batch/s, loss=2.0357, acc=18.53%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0357, Validation Accuracy: 0.1853\n",
      "Saved model for epoch 19\n",
      "End of Epoch 19/20\n",
      "\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████| 389/389 [00:03<00:00, 123.66batch/s, loss=1.7647, acc=28.00%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.7647, Train Accuracy: 0.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████████████████| 92/92 [00:00<00:00, 141.24batch/s, loss=2.0398, acc=15.55%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0398, Validation Accuracy: 0.1555\n",
      "\n",
      "Per-class Validation Accuracy:\n",
      "  Pencilcase: 0.4482 (238/531)\n",
      "  Pinkball: 0.2227 (102/458)\n",
      "  StuffedAnimal: 0.0641 (64/998)\n",
      "  Tennis: 0.0880 (41/466)\n",
      "  Waterbottle: 0.0193 (9/466)\n",
      "Saved model for epoch 20\n",
      "End of Epoch 20/20\n",
      "\n",
      "Training complete! Best validation accuracy: 0.1898\n"
     ]
    }
   ],
   "source": [
    "# Create checkpoint directory if it doesn't exist\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "\n",
    "# Initialize best metrics tracking\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0\n",
    "class_names = list(CATEGORIES.keys())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config['epochs']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
    "    \n",
    "    # Training loop with progress bar\n",
    "    model.train()\n",
    "    epoch_train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=\"Training\", unit=\"batch\", ncols=100)\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(train_bar):\n",
    "        # Move data to device\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        train_bar.set_postfix({\n",
    "            'loss': f\"{epoch_train_loss/(batch_idx+1):.4f}\",\n",
    "            'acc': f\"{100.*correct/total:.2f}%\"\n",
    "        })\n",
    "        \n",
    "        # Free memory\n",
    "        del X_batch, y_batch, outputs, loss\n",
    "        if device.type == 'mps' and hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    # Calculate final training metrics\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    print(f\"Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    \n",
    "    # Validation loop with progress bar\n",
    "    model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # For per-class accuracy\n",
    "    class_correct = [0] * len(class_names)\n",
    "    class_total = [0] * len(class_names)\n",
    "    \n",
    "    val_bar = tqdm(val_loader, desc=\"Validation\", unit=\"batch\", ncols=100)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X_val_batch, y_val_batch) in enumerate(val_bar):\n",
    "            # Move data to device\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_val_batch)\n",
    "            loss = criterion(outputs, y_val_batch)\n",
    "            \n",
    "            # Track metrics\n",
    "            epoch_val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_val_batch.size(0)\n",
    "            correct += (predicted == y_val_batch).sum().item()\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            for c in range(len(class_names)):\n",
    "                class_mask = (y_val_batch == c)\n",
    "                class_correct[c] += (predicted[class_mask] == c).sum().item()\n",
    "                class_total[c] += class_mask.sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            val_bar.set_postfix({\n",
    "                'loss': f\"{epoch_val_loss/(batch_idx+1):.4f}\", \n",
    "                'acc': f\"{100.*correct/total:.2f}%\"\n",
    "            })\n",
    "            \n",
    "            # Free memory\n",
    "            del X_val_batch, y_val_batch, outputs, loss\n",
    "            if device.type == 'mps' and hasattr(torch.mps, 'empty_cache'):\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "    # Calculate final validation metrics\n",
    "    epoch_val_loss /= len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    # Print per-class accuracy every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"\\nPer-class Validation Accuracy:\")\n",
    "        per_class_acc = {}\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            if class_total[i] > 0:\n",
    "                accuracy = class_correct[i] / class_total[i]\n",
    "                print(f\"  {class_name}: {accuracy:.4f} ({class_correct[i]}/{class_total[i]})\")\n",
    "                per_class_acc[f\"val_acc_{class_name}\"] = accuracy\n",
    "        # Log per-class metrics to WandB\n",
    "        wandb.log(per_class_acc, step=epoch)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(epoch_val_loss)\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save best model based on validation loss\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        best_val_acc = val_acc\n",
    "        \n",
    "        # Save best model\n",
    "        best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': epoch_val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, best_model_path)\n",
    "        wandb.save(best_model_path)  # Save the model to WandB\n",
    "        print(f\"Saved best model with validation loss: {best_val_loss:.4f} and accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Save the model for every epoch\n",
    "    last_model_path = os.path.join(config['checkpoint_dir'], f'model_epoch_{epoch+1}.pth')\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "    wandb.save(last_model_path)  # Save the model to WandB\n",
    "    print(f\"Saved model for epoch {epoch+1}\")\n",
    "    \n",
    "    # Logging metrics to WandB\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': epoch_train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': epoch_val_loss,\n",
    "        'val_acc': val_acc,\n",
    "        'learning_rate': curr_lr\n",
    "    }, step=epoch)\n",
    "    \n",
    "    print(f\"End of Epoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "# Final message\n",
    "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, class_names, device, checkpoint_dir=None):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test dataset and generate detailed performance metrics\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Store all predictions and ground truth for analysis\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []  # Store probabilities for confidence analysis\n",
    "    \n",
    "    # Per-class statistics\n",
    "    class_correct = {class_name: 0 for class_name in class_names}\n",
    "    class_total = {class_name: 0 for class_name in class_names}\n",
    "    \n",
    "    # Create progress bar\n",
    "    test_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\", ncols=100)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_bar:\n",
    "            # Get inputs and labels\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Calculate loss and accuracy\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            \n",
    "            # Update total counts\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            # Update per-class counts\n",
    "            for i in range(targets.size(0)):\n",
    "                label = targets[i].item()\n",
    "                pred = predicted[i].item()\n",
    "                class_name = class_names[label]\n",
    "                class_total[class_name] += 1\n",
    "                if pred == label:\n",
    "                    class_correct[class_name] += 1\n",
    "            \n",
    "            # Store predictions and targets for later analysis\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            test_bar.set_postfix({\n",
    "                'loss': f\"{test_loss/total:.4f}\",\n",
    "                'acc': f\"{100.0*correct/total:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = correct / total\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracy = {name: class_correct[name]/class_total[name] if class_total[name] > 0 else 0 \n",
    "                     for name in class_names}\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    classification_rep = classification_report(all_targets, all_preds, \n",
    "                                              target_names=class_names, output_dict=True)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TEST RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc:.4f} ({correct}/{total})\")\n",
    "    print(\"\\nPer-Class Accuracy:\")\n",
    "    for class_name in class_names:\n",
    "        print(f\"  {class_name}: {class_accuracy[class_name]:.4f} ({class_correct[class_name]}/{class_total[class_name]})\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_targets, all_preds, target_names=class_names))\n",
    "    \n",
    "    # Return comprehensive metrics dictionary\n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'test_accuracy': test_acc,\n",
    "        'class_accuracy': class_accuracy,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': classification_rep,\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets,\n",
    "        'probabilities': all_probs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/bg77t7g55md3qh0s5xdb7tg40000gn/T/ipykernel_34221/2044790258.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████████████████████| 89/89 [00:01<00:00, 84.66batch/s, loss=1.9309, acc=20.91%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEST RESULTS\n",
      "==================================================\n",
      "Test Loss: 61.3322\n",
      "Test Accuracy: 0.2091 (591/2827)\n",
      "\n",
      "Per-Class Accuracy:\n",
      "  Blueball: 0.1567 (144/919)\n",
      "  Box: 0.6916 (323/467)\n",
      "  Pencilcase: 0.0000 (0/0)\n",
      "  Pinkball: 0.0768 (35/456)\n",
      "  StuffedAnimal: 0.0000 (0/0)\n",
      "  Tennis: 0.1680 (84/500)\n",
      "  Waterbottle: 0.0103 (5/485)\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Blueball       0.34      0.16      0.21       919\n",
      "          Box       0.35      0.69      0.46       467\n",
      "   Pencilcase       0.00      0.00      0.00         0\n",
      "     Pinkball       0.10      0.08      0.09       456\n",
      "StuffedAnimal       0.00      0.00      0.00         0\n",
      "       Tennis       0.27      0.17      0.21       500\n",
      "  Waterbottle       0.17      0.01      0.02       485\n",
      "\n",
      "     accuracy                           0.21      2827\n",
      "    macro avg       0.17      0.16      0.14      2827\n",
      " weighted avg       0.26      0.21      0.20      2827\n",
      "\n",
      "\n",
      "Final test accuracy: 0.2091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load the best model (optional - if you saved a checkpoint)\n",
    "best_model_path = f\"{config['checkpoint_dir']}/best_model.pth\"\n",
    "if os.path.exists(best_model_path):\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded best model from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = test_model(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    class_names=class_names,\n",
    "    device=device,\n",
    "    checkpoint_dir=config['checkpoint_dir']\n",
    ")\n",
    "\n",
    "# You can now access detailed metrics\n",
    "print(f\"\\nFinal test accuracy: {test_results['test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>learning_rate</td><td>██████████████▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▂▄▄▄▅▅▆▆▆▇▆▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▃▂▂▂▅▅▆▆▆▁▂▄▅▃▃▄▅██▅</td></tr><tr><td>val_acc_Pencilcase</td><td>▂▁█▇</td></tr><tr><td>val_acc_Pinkball</td><td>▁█▄▁</td></tr><tr><td>val_acc_StuffedAnimal</td><td>▄▁▃█</td></tr><tr><td>val_acc_Tennis</td><td>█▂▁▂</td></tr><tr><td>val_acc_Waterbottle</td><td>█▁▂▃</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▁▁▁▁▂▂▁▁▁▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>learning_rate</td><td>5e-05</td></tr><tr><td>train_acc</td><td>0.28005</td></tr><tr><td>train_loss</td><td>1.76472</td></tr><tr><td>val_acc</td><td>0.15553</td></tr><tr><td>val_acc_Pencilcase</td><td>0.44821</td></tr><tr><td>val_acc_Pinkball</td><td>0.22271</td></tr><tr><td>val_acc_StuffedAnimal</td><td>0.06413</td></tr><tr><td>val_acc_Tennis</td><td>0.08798</td></tr><tr><td>val_acc_Waterbottle</td><td>0.01931</td></tr><tr><td>val_loss</td><td>2.03983</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">04run</strong> at: <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/cutkkks0' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/cutkkks0</a><br> View project at: <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 21 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_211000-cutkkks0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
