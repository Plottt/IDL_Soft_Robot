{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmX6adzDL6IV",
        "outputId": "7d3929ff-05ac-420d-f560-7fb16edf9fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so2Ihy3KKMb6",
        "outputId": "35caa598-3034-4e77-f7ae-25b4e23307fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import os\n",
        "from torchsummary import summary\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import wandb\n",
        "import torch.nn.functional as F\n",
        "import hashlib\n",
        "from typing import Dict, Tuple\n",
        "import random\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHpFZC4vKPdS",
        "outputId": "ffceb66f-4d53-43e2-c51b-f850ae965045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwfQWq5Stp4Y",
        "outputId": "f899ea36-bf25-4d64-ebf1-0c733ebed554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "    'batch_size': 64,            # Increased batch size for better gradient estimates\n",
        "    'lr': 0.001,                 # Learning rate\n",
        "    'weight_decay': 1e-4,        # L2 regularization\n",
        "    'epochs': 10,                # More epochs with early stopping\n",
        "    'input_dim': 22,             # Your feature dimension\n",
        "    'num_classes': 7,            # Number of object classes\n",
        "    'hidden_dims': [512,256,128],           # Hidden dimension\n",
        "    'num_layers': 2,             # Number of LSTM layers\n",
        "    'dropout': 0.3,              # Dropout rate\n",
        "    'patience': 7,               # Early stopping patience\n",
        "    'seq_len': 2500,             # Your sequence length\n",
        "    'checkpoint_dir': \"/content/drive/MyDrive/IDL/Checkpoint\",\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5H88G2o5KMb-"
      },
      "outputs": [],
      "source": [
        "# Define category mapping\n",
        "CATEGORIES = {\n",
        "    'Blueball': 0,\n",
        "    'Box': 1,\n",
        "    'Pencilcase': 2,\n",
        "    'Pinkball': 3,\n",
        "    'StuffedAnimal': 4,\n",
        "    'Tennis': 5,\n",
        "    'Waterbottle': 6,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuINLJVKKMb_"
      },
      "outputs": [],
      "source": [
        "# Path to the folder containing the dataset files\n",
        "folder_path = \"/content/drive/MyDrive/IDL/IDL_Data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXeSE84nKMb_"
      },
      "outputs": [],
      "source": [
        "# Stats trackers\n",
        "total_count = 0\n",
        "kept_count = 0\n",
        "valid_file_count = 0\n",
        "skipped_due_to_missing_waypoints = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWAborBwKMb_"
      },
      "outputs": [],
      "source": [
        "WAYPOINTS = [\n",
        "    (30, -30), (30, 30), (15, -30), (15, 30),\n",
        "    (0, -30), (0, 30), (-15, -30), (-15, 30),\n",
        "    (-30, -30), (-30, 30), (-30, -30), (30, -30),\n",
        "    (30, 30), (-30, 30)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVuq0D5TKMb_"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load and label dataset\n",
        "def load_and_label_file(file_path, file_name):\n",
        "    global total_count\n",
        "    category = next((key for key in CATEGORIES if key in file_name), None)\n",
        "    if category is None:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    data = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            if len(parts) == 10:\n",
        "                try:\n",
        "                    timestamp = parts[0]\n",
        "                    microsec = int(parts[1])\n",
        "                    x = float(parts[2])\n",
        "                    y = float(parts[3])\n",
        "                    x_target = float(parts[4])\n",
        "                    y_target = float(parts[5])\n",
        "                    pwm1 = int(parts[6])\n",
        "                    pwm2 = int(parts[7])\n",
        "                    pwm3 = int(parts[8])\n",
        "                    pwm4 = int(parts[9])\n",
        "                    total_count += 1\n",
        "\n",
        "                    data.append([\n",
        "                        timestamp, microsec, x, y, x_target, y_target,\n",
        "                        pwm1, pwm2, pwm3, pwm4, category, CATEGORIES[category]\n",
        "                    ])\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "    return pd.DataFrame(data, columns=[\n",
        "        \"timestamp\", \"microseconds\", \"x\", \"y\", \"x_target\", \"y_target\",\n",
        "        \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"category\", \"label\"\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naFDKYoGKMb_"
      },
      "outputs": [],
      "source": [
        "# Step 2: Assign sequential waypoint numbers\n",
        "def assign_sequential_waypoints(df, tol=1.0):\n",
        "    df = df.reset_index(drop=True)\n",
        "    wp_index = 0\n",
        "    assigned_wp = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        x_t, y_t = df.loc[i, \"x_target\"], df.loc[i, \"y_target\"]\n",
        "        current_expected = WAYPOINTS[wp_index]\n",
        "\n",
        "        if np.isclose(x_t, current_expected[0], atol=tol) and np.isclose(y_t, current_expected[1], atol=tol):\n",
        "            assigned_wp.append(wp_index)\n",
        "        else:\n",
        "            if wp_index + 1 < len(WAYPOINTS):\n",
        "                next_expected = WAYPOINTS[wp_index + 1]\n",
        "                if np.isclose(x_t, next_expected[0], atol=tol) and np.isclose(y_t, next_expected[1], atol=tol):\n",
        "                    wp_index += 1\n",
        "                    assigned_wp.append(wp_index)\n",
        "                else:\n",
        "                    assigned_wp.append(wp_index)\n",
        "            else:\n",
        "                assigned_wp.append(wp_index)\n",
        "\n",
        "    df[\"waypoint_number\"] = assigned_wp\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfXzeHC4KMb_"
      },
      "outputs": [],
      "source": [
        "# Step 3: Filter out rows where y <= 0\n",
        "def filter_by_y(df):\n",
        "    global kept_count\n",
        "    filtered = df[df[\"y\"] > 15].reset_index(drop=True)\n",
        "    kept_count += len(filtered)\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogu08rbDKMcA"
      },
      "outputs": [],
      "source": [
        "def process_file(file_path, file_name):\n",
        "    global valid_file_count\n",
        "\n",
        "    # Step 1: Load and label\n",
        "    df = load_and_label_file(file_path, file_name)\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Step 2: Assign waypoint numbers\n",
        "    df = assign_sequential_waypoints(df)\n",
        "\n",
        "    # Show how many waypoints existed before filtering\n",
        "    waypoint_count_before = df[\"waypoint_number\"].nunique()\n",
        "    print(f\"\\n{file_name} → {waypoint_count_before} waypoints BEFORE filtering\")\n",
        "\n",
        "    # Step 3: Filter out rows where y ≤ 0\n",
        "    df = filter_by_y(df)\n",
        "\n",
        "    # Show how many remain after filtering\n",
        "    waypoint_count_after = df[\"waypoint_number\"].nunique()\n",
        "    print(f\"{file_name} → {waypoint_count_after} waypoints AFTER filtering\")\n",
        "\n",
        "    # Count as valid if any data was kept\n",
        "    if not df.empty:\n",
        "        valid_file_count += 1\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8u06Sh1KMcA"
      },
      "outputs": [],
      "source": [
        "# print(all_data.head(1350))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuHhrzRwKMcB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg1SVbqJkMaR",
        "outputId": "5d4e6e02-1adf-4f35-f51c-47d8ffd5efa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Per-class file counts:\n",
            "  Blueball        → 30 total files\n",
            "  Box             → 30 total files\n",
            "  Pencilcase      → 29 total files\n",
            "  Pinkball        → 30 total files\n",
            "  StuffedAnimal   → 30 total files\n",
            "  Tennis          → 30 total files\n",
            "  Waterbottle     → 30 total files\n",
            "\n",
            "✅ Final split file counts:\n",
            "Train: 132\n",
            "Val:   42\n",
            "Test:  35\n",
            "Processing train files\n",
            "\n",
            "📌 StuffedAnimal21.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal21.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal24.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal24.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal25.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal25.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal2.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal2.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal13.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal13.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal28.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal28.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal12.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal12.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal22.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal22.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal29.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal29.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal20.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal20.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal4.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal4.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal1.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal30.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal30.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal5.txt → 10 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal5.txt → 9 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal6.txt → 13 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal3.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal3.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal15.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal15.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal17.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal17.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal8.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle22.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle22.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle13.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle13.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle23.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle23.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle17.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle17.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle28.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle28.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle14.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle14.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle24.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle24.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle21.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle21.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle27.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle27.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle20.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle20.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle25.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle25.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle2.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle4.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle26.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle26.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle12.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle12.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis14.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis14.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis21.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis21.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis12.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis12.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis27.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis27.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis3.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis25.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis25.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis13.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis13.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis5.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis16.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis16.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis24.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis24.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis2.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis30.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis30.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis15.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis15.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis18.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis18.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis29.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis29.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis11.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis11.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis23.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis23.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball3.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball25.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball25.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball2.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball17.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball17.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball18.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball18.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball22.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball22.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball27.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball27.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball14.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball14.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball24.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball24.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball4.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball12.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball12.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball5.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball21.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball21.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball29.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball29.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball30.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball30.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball13.txt → 13 waypoints BEFORE filtering\n",
            "📌 Pinkball13.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball20.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball20.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball30.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball30.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball5.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball21.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball21.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball4.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball29.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball29.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball11.txt → 10 waypoints BEFORE filtering\n",
            "📌 Blueball11.txt → 9 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball15.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball15.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball22.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball22.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball23.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball23.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball18.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball18.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball3.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball27.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball27.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball14.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball14.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase26.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase26.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase20.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase20.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase18.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase18.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase16.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase16.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase21.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase21.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase17.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase17.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase2.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase3.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase27.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase27.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase14.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase14.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase13.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase13.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase25.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase25.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase4.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase15.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase15.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase11.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase11.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box21.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box21.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box23.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box23.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box5.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box15.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box15.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box29.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box29.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box14.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box14.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box3.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box19.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box19.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box26.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box26.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box13.txt → 10 waypoints BEFORE filtering\n",
            "📌 Box13.txt → 9 waypoints AFTER filtering\n",
            "\n",
            "📌 Box22.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box22.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box28.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box28.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box24.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box24.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box12.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box12.txt → 12 waypoints AFTER filtering\n",
            "Processing validation files\n",
            "\n",
            "📌 StuffedAnimal7.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal19.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal19.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal27.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal27.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal14.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal14.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal9.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal23.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal23.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle29.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle29.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle5.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle16.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle16.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle18.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle18.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle19.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle19.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis28.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis28.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis22.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis22.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis4.txt → 13 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball6.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball6.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball9.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball9.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball11.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball11.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball16.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball16.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball26.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball26.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball13.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball13.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball24.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball24.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball19.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball19.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball2.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball16.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball16.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball25.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball25.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase24.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase24.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase12.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase12.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase30.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase30.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase23.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase23.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase8.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase8.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box2.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box2.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box27.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box27.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box16.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box16.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box11.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box11.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box20.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box20.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box10.txt → 12 waypoints AFTER filtering\n",
            "Processing test files\n",
            "\n",
            "📌 StuffedAnimal16.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal16.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal11.txt → 13 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal11.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal18.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal18.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal26.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal26.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 StuffedAnimal10.txt → 14 waypoints BEFORE filtering\n",
            "📌 StuffedAnimal10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle3.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle3.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle30.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle30.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle1.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle1.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle11.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle11.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Waterbottle15.txt → 14 waypoints BEFORE filtering\n",
            "📌 Waterbottle15.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis20.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis20.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis17.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis17.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis26.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis26.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis10.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis10.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Tennis19.txt → 14 waypoints BEFORE filtering\n",
            "📌 Tennis19.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball20.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball20.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball15.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball15.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball23.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball23.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball19.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball19.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pinkball28.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pinkball28.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball26.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball26.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball7.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball7.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball12.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball12.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball28.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball28.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Blueball17.txt → 14 waypoints BEFORE filtering\n",
            "📌 Blueball17.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase22.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase22.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase28.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase28.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase5.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase5.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase29.txt → 14 waypoints BEFORE filtering\n",
            "📌 Pencilcase29.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Pencilcase19.txt → 13 waypoints BEFORE filtering\n",
            "📌 Pencilcase19.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box17.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box17.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box30.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box30.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box18.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box18.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box4.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box4.txt → 12 waypoints AFTER filtering\n",
            "\n",
            "📌 Box25.txt → 14 waypoints BEFORE filtering\n",
            "📌 Box25.txt → 12 waypoints AFTER filtering\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict, Counter\n",
        "\n",
        "def build_datasets(data_dir: str):\n",
        "    data_dir = Path(data_dir)\n",
        "    file_paths = list(data_dir.glob(\"*.txt\"))\n",
        "    random.seed(42)\n",
        "\n",
        "    class_to_files = defaultdict(list)\n",
        "    for file_path in file_paths:\n",
        "        for class_name in CATEGORIES:\n",
        "            if class_name in file_path.name:\n",
        "                class_to_files[class_name].append(file_path)\n",
        "                break\n",
        "    train_files, val_files, test_files = [], [], []\n",
        "    for class_name, files in class_to_files.items():\n",
        "        random.shuffle(files)\n",
        "        n = len(files)\n",
        "        train_split = int(0.65 * n)\n",
        "        val_split = int(0.85 * n)\n",
        "        train_files += files[:train_split]\n",
        "        val_files += files[train_split:val_split]\n",
        "        test_files += files[val_split:]\n",
        "\n",
        "    print(\"Per-class file counts:\")\n",
        "    for cls in CATEGORIES:\n",
        "        print(f\"  {cls:<15} → {len(class_to_files[cls])} total files\")\n",
        "\n",
        "    print(\"Final split file counts:\")\n",
        "    print(f\"Train: {len(train_files)}\")\n",
        "    print(f\"Val:   {len(val_files)}\")\n",
        "    print(f\"Test:  {len(test_files)}\")\n",
        "\n",
        "    def process_file_list(file_list):\n",
        "        dfs = []\n",
        "        for fp in file_list:\n",
        "            df = process_file(fp, fp.name)\n",
        "            if not df.empty:\n",
        "                dfs.append(df)\n",
        "        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
        "\n",
        "    print(\"Processing train files\")\n",
        "    train_df = process_file_list(train_files)\n",
        "    print(\"Processing validation files\")\n",
        "    val_df = process_file_list(val_files)\n",
        "    print(\"Processing test files\")\n",
        "    test_df = process_file_list(test_files)\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "# Run everything\n",
        "train_df, val_df, test_df = build_datasets(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4MdJKhSsJg5"
      },
      "outputs": [],
      "source": [
        "def extract_enhanced_features(df):\n",
        "    \"\"\"Extract additional meaningful features from the raw sensor data\"\"\"\n",
        "    # Create a copy to avoid modifying the original\n",
        "    enhanced_df = df.copy()\n",
        "\n",
        "    # Compute deltas (changes between consecutive measurements)\n",
        "    enhanced_df['x_delta'] = enhanced_df.groupby(['category', 'waypoint_number'])['x'].diff().fillna(0)\n",
        "    enhanced_df['y_delta'] = enhanced_df.groupby(['category', 'waypoint_number'])['y'].diff().fillna(0)\n",
        "\n",
        "    # Distance to target (error signal that drives the controller)\n",
        "    enhanced_df['x_error'] = enhanced_df['x_target'] - enhanced_df['x']\n",
        "    enhanced_df['y_error'] = enhanced_df['y_target'] - enhanced_df['y']\n",
        "\n",
        "    # Magnitude of error and movement\n",
        "    enhanced_df['error_magnitude'] = np.sqrt(enhanced_df['x_error']**2 + enhanced_df['y_error']**2)\n",
        "    enhanced_df['movement_magnitude'] = np.sqrt(enhanced_df['x_delta']**2 + enhanced_df['y_delta']**2)\n",
        "\n",
        "    # Control effort features (sum and differences of PWM signals)\n",
        "    enhanced_df['total_pwm'] = enhanced_df['pwm1'] + enhanced_df['pwm2'] + enhanced_df['pwm3'] + enhanced_df['pwm4']\n",
        "    enhanced_df['pwm_x_diff'] = enhanced_df['pwm1'] - enhanced_df['pwm3']  # Assuming these control x-axis\n",
        "    enhanced_df['pwm_y_diff'] = enhanced_df['pwm2'] - enhanced_df['pwm4']  # Assuming these control y-axis\n",
        "\n",
        "    # Interaction features (product of error and control)\n",
        "    enhanced_df['x_control_response'] = enhanced_df['x_error'] * enhanced_df['pwm_x_diff']\n",
        "    enhanced_df['y_control_response'] = enhanced_df['y_error'] * enhanced_df['pwm_y_diff']\n",
        "\n",
        "    # Time derivatives of error (how quickly error is changing)\n",
        "    enhanced_df['x_error_delta'] = enhanced_df.groupby(['category', 'waypoint_number'])['x_error'].diff().fillna(0)\n",
        "    enhanced_df['y_error_delta'] = enhanced_df.groupby(['category', 'waypoint_number'])['y_error'].diff().fillna(0)\n",
        "\n",
        "    return enhanced_df\n",
        "\n",
        "# Process your dataframes with the enhanced features\n",
        "train_df_enhanced = extract_enhanced_features(train_df)\n",
        "val_df_enhanced = extract_enhanced_features(val_df)\n",
        "test_df_enhanced = extract_enhanced_features(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi8NwL56sOo8"
      },
      "outputs": [],
      "source": [
        "class EnhancedWindowedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, seq_len=2000, augment=False, noise_scale=0.02, time_warp_scale=0.1):\n",
        "        self.seq_len = seq_len\n",
        "        self.df = df.reset_index(drop=True)  # Use the enhanced dataframe\n",
        "        self.augment = augment\n",
        "        self.noise_scale = noise_scale\n",
        "        self.time_warp_scale = time_warp_scale\n",
        "\n",
        "        # Select all features including the new engineered features\n",
        "        feature_columns = [\n",
        "            # Original features\n",
        "            \"x\", \"y\", \"x_target\", \"y_target\", \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"waypoint_number\",\n",
        "            # New engineered features\n",
        "            \"x_delta\", \"y_delta\", \"x_error\", \"y_error\", \"error_magnitude\", \"movement_magnitude\",\n",
        "            \"total_pwm\", \"pwm_x_diff\", \"pwm_y_diff\", \"x_control_response\", \"y_control_response\",\n",
        "            \"x_error_delta\", \"y_error_delta\"\n",
        "        ]\n",
        "\n",
        "        # Filter to only include columns that exist in the dataframe\n",
        "        self.feature_columns = [col for col in feature_columns if col in self.df.columns]\n",
        "        self.features = self.df[self.feature_columns].values.astype(np.float32)\n",
        "\n",
        "        # Normalize features\n",
        "        self.feature_means = np.mean(self.features, axis=0)\n",
        "        self.feature_stds = np.std(self.features, axis=0) + 1e-6  # Avoid division by zero\n",
        "        self.features = (self.features - self.feature_means) / self.feature_stds\n",
        "\n",
        "        # Label per row\n",
        "        self.labels = self.df[\"label\"].values.astype(np.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) - self.seq_len + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.features[idx:idx + self.seq_len]  # (seq_len, input_dim)\n",
        "        y = self.labels[idx + self.seq_len - 1]\n",
        "\n",
        "        # Apply augmentation if enabled\n",
        "        if self.augment and np.random.random() > 0.5:\n",
        "            # Get dimensions once at the start\n",
        "            seq_len, feat_dim = x.shape\n",
        "\n",
        "            # Add random noise\n",
        "            noise = np.random.normal(0, self.noise_scale, x.shape)\n",
        "            x = x + noise\n",
        "\n",
        "            # Time warping (randomly stretch or compress parts of the sequence)\n",
        "            if np.random.random() > 0.7:\n",
        "                time_indices = np.arange(seq_len)\n",
        "                warp = np.sin(np.linspace(0, 3*np.pi, seq_len)) * self.time_warp_scale\n",
        "                warped_indices = np.clip(time_indices + warp, 0, seq_len-1).astype(int)\n",
        "                x = x[warped_indices, :]\n",
        "\n",
        "            # Random feature masking (occasionally zero out features)\n",
        "            if np.random.random() > 0.8:\n",
        "                mask_idx = np.random.choice(feat_dim, size=int(feat_dim * 0.1), replace=False)\n",
        "                x[:, mask_idx] = 0\n",
        "\n",
        "        x_tensor = torch.tensor(x, dtype=torch.float32)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "        return x_tensor, y_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho-BZBjrsVuz"
      },
      "outputs": [],
      "source": [
        "seq_len = config['seq_len']  # Adjust based on your needs\n",
        "\n",
        "# Create datasets with augmentation\n",
        "train_dataset = EnhancedWindowedDataset(train_df_enhanced, seq_len=seq_len, augment=True)\n",
        "val_dataset = EnhancedWindowedDataset(val_df_enhanced, seq_len=seq_len)\n",
        "test_dataset = EnhancedWindowedDataset(test_df_enhanced, seq_len=seq_len)\n",
        "\n",
        "# Create data loaders with smaller batch size\n",
        "batch_size = 16  # Smaller batch size for better regularization\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lURaiGrsHZN",
        "outputId": "5855b781-8e3d-4b12-cb5d-978e0cc55f1d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TactileEnsembleModel                     [32, 7]                   --\n",
              "├─CNNFeatureExtractor: 1-1               [32, 39936]               --\n",
              "│    └─Conv1d: 2-1                       [32, 64, 2500]            7,104\n",
              "│    └─BatchNorm1d: 2-2                  [32, 64, 2500]            128\n",
              "│    └─MaxPool1d: 2-3                    [32, 64, 625]             --\n",
              "│    └─Conv1d: 2-4                       [32, 128, 625]            41,088\n",
              "│    └─BatchNorm1d: 2-5                  [32, 128, 625]            256\n",
              "│    └─MaxPool1d: 2-6                    [32, 128, 156]            --\n",
              "│    └─Conv1d: 2-7                       [32, 256, 156]            164,096\n",
              "│    └─BatchNorm1d: 2-8                  [32, 256, 156]            512\n",
              "├─StatisticalFeatureExtractor: 1-2       [32, 132]                 --\n",
              "├─Dropout: 1-3                           [32, 40068]               --\n",
              "├─MLPClassifier: 1-4                     [32, 7]                   --\n",
              "│    └─Sequential: 2-9                   [32, 7]                   --\n",
              "│    │    └─Linear: 3-1                  [32, 512]                 20,515,328\n",
              "│    │    └─ReLU: 3-2                    [32, 512]                 --\n",
              "│    │    └─Dropout: 3-3                 [32, 512]                 --\n",
              "│    │    └─Linear: 3-4                  [32, 256]                 131,328\n",
              "│    │    └─ReLU: 3-5                    [32, 256]                 --\n",
              "│    │    └─Dropout: 3-6                 [32, 256]                 --\n",
              "│    │    └─Linear: 3-7                  [32, 128]                 32,896\n",
              "│    │    └─ReLU: 3-8                    [32, 128]                 --\n",
              "│    │    └─Dropout: 3-9                 [32, 128]                 --\n",
              "│    │    └─Linear: 3-10                 [32, 7]                   903\n",
              "==========================================================================================\n",
              "Total params: 20,893,639\n",
              "Trainable params: 20,893,639\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 2.87\n",
              "==========================================================================================\n",
              "Input size (MB): 7.04\n",
              "Forward/backward pass size (MB): 143.56\n",
              "Params size (MB): 83.57\n",
              "Estimated Total Size (MB): 234.17\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class CNNFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_channels, seq_len):\n",
        "        super(CNNFeatureExtractor, self).__init__()\n",
        "\n",
        "        # CNN layers for extracting temporal patterns\n",
        "        self.conv1 = nn.Conv1d(input_channels, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.conv3 = nn.Conv1d(128, 256, kernel_size=5, stride=1, padding=2)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        # Pooling to reduce dimensionality\n",
        "        self.pool = nn.MaxPool1d(kernel_size=4, stride=4)\n",
        "\n",
        "        # Calculate output size after convolutions and pooling\n",
        "        output_len = seq_len // 16  # After 2 pooling layers (4×4=16)\n",
        "        self.output_size = 256 * output_len if output_len > 0 else 256\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, input_channels]\n",
        "        # Convert to [batch_size, input_channels, seq_len] for CNN\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply CNN layers\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "        # Flatten for MLP\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class StatisticalFeatureExtractor(nn.Module):\n",
        "    def __init__(self, seq_len, input_dim):\n",
        "        super(StatisticalFeatureExtractor, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, input_dim]\n",
        "\n",
        "        # Calculate statistical features\n",
        "        mean = torch.mean(x, dim=1)  # Mean across sequence\n",
        "        std = torch.std(x, dim=1)    # Standard deviation\n",
        "        max_vals, _ = torch.max(x, dim=1)  # Maximum values\n",
        "        min_vals, _ = torch.min(x, dim=1)  # Minimum values\n",
        "\n",
        "        # Calculate deltas (changes between timesteps)\n",
        "        deltas = x[:, 1:, :] - x[:, :-1, :]\n",
        "        mean_deltas = torch.mean(deltas, dim=1)\n",
        "        std_deltas = torch.std(deltas, dim=1)\n",
        "\n",
        "        # Concatenate all features\n",
        "        features = torch.cat([mean, std, max_vals, min_vals, mean_deltas, std_deltas], dim=1)\n",
        "        return features\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.3):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Create hidden layers\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, num_classes))\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)\n",
        "\n",
        "class TactileEnsembleModel(nn.Module):\n",
        "    def __init__(self, input_dim, seq_len, num_classes, hidden_dims=[512, 256, 128], dropout=0.3):\n",
        "        super(TactileEnsembleModel, self).__init__()\n",
        "\n",
        "        # CNN feature extractor\n",
        "        self.cnn_extractor = CNNFeatureExtractor(input_dim, seq_len)\n",
        "        cnn_output_size = self.cnn_extractor.output_size\n",
        "\n",
        "        # Statistical feature extractor\n",
        "        self.stat_extractor = StatisticalFeatureExtractor(seq_len, input_dim)\n",
        "        stat_output_size = input_dim * 6  # mean, std, max, min, mean_delta, std_delta\n",
        "\n",
        "        # Combined feature size\n",
        "        combined_size = cnn_output_size + stat_output_size\n",
        "\n",
        "        # MLP classifier\n",
        "        self.classifier = MLPClassifier(combined_size, hidden_dims, num_classes, dropout)\n",
        "\n",
        "        # Regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract CNN features\n",
        "        cnn_features = self.cnn_extractor(x)\n",
        "\n",
        "        # Extract statistical features\n",
        "        stat_features = self.stat_extractor(x)\n",
        "\n",
        "        # Combine features\n",
        "        combined = torch.cat([cnn_features, stat_features], dim=1)\n",
        "        combined = self.dropout(combined)\n",
        "\n",
        "        # Final classification\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        # Return both logits and softmax probabilities\n",
        "        return logits, F.softmax(logits, dim=1)\n",
        "\n",
        "model = TactileEnsembleModel(\n",
        "    input_dim=config['input_dim'],\n",
        "    seq_len=config['seq_len'],\n",
        "    num_classes=config['num_classes'],\n",
        "    hidden_dims=config['hidden_dims'],\n",
        "    dropout=config['dropout']\n",
        ").to(config['device'])\n",
        "\n",
        "summary(model, input_size=(32, config['seq_len'], config['input_dim']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VElzMZvKMcB"
      },
      "outputs": [],
      "source": [
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiPYAO1zKMcB"
      },
      "outputs": [],
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsUsF7_osqsP"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = float('inf')\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bfr3msGHstv-"
      },
      "outputs": [],
      "source": [
        "def train_model_with_regularization(model, train_loader, criterion, optimizer, device,\n",
        "                                   weight_decay=1e-4, gradient_clip=1.0, mixup_alpha=0.2):\n",
        "    model.train()\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        x, y = data\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Move data to device first\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Apply Mixup (data augmentation technique)\n",
        "        if mixup_alpha > 0:\n",
        "            lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "            index = torch.randperm(batch_size).to(device)\n",
        "            x = lam * x + (1 - lam) * x[index]\n",
        "\n",
        "        # Forward pass - now returns logits and probabilities\n",
        "        logits, probs = model(x)\n",
        "\n",
        "        # Compute loss using logits (not probabilities)\n",
        "        if mixup_alpha > 0:\n",
        "            loss = lam * criterion(logits, y) + (1 - lam) * criterion(logits, y[index])\n",
        "        else:\n",
        "            loss = criterion(logits, y)\n",
        "\n",
        "        # Add L2 regularization explicitly if needed\n",
        "        l2_reg = torch.tensor(0., device=device)\n",
        "        for param in model.parameters():\n",
        "            l2_reg += torch.norm(param, 2)\n",
        "        loss += weight_decay * l2_reg\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute accuracy using probabilities\n",
        "        _, predicted = torch.max(probs.data, 1)\n",
        "        correct = (predicted == y).sum().item()\n",
        "        acc = 100.0 * correct / batch_size\n",
        "\n",
        "        loss_m.update(loss.item())\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(loss_m.avg)),\n",
        "            acc=\"{:.04f}%\".format(float(acc_m.avg)),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, logits, probs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    return loss_m.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZO5iNDzKMcC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate_model(model, val_loader, criterion, class_names, device):\n",
        "    model.eval()\n",
        "    loss_m = AverageMeter()\n",
        "    acc_m = AverageMeter()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "        x, y = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass - now returns logits and probabilities\n",
        "        logits, probs = model(x)\n",
        "\n",
        "        # Compute loss using logits\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        # Get predictions from probabilities\n",
        "        _, predicted = torch.max(probs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(y.cpu().numpy())\n",
        "\n",
        "        # Calculate accuracy\n",
        "        correct = (predicted == y).sum().item()\n",
        "        acc = 100.0 * correct / y.size(0)\n",
        "\n",
        "        loss_m.update(loss.item())\n",
        "        acc_m.update(acc)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(loss_m.avg)),\n",
        "            acc=\"{:.04f}%\".format(float(acc_m.avg))\n",
        "        )\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, logits, probs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    if class_names:\n",
        "        print(\"\\nPer-class Validation Accuracy:\")\n",
        "        per_class_acc = {}\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            class_mask = (np.array(all_targets) == i)\n",
        "            if np.sum(class_mask) > 0:\n",
        "                class_correct = np.sum((np.array(all_preds)[class_mask] == i))\n",
        "                class_total = np.sum(class_mask)\n",
        "                acc_percent = 100 * class_correct / class_total\n",
        "                print(f\"  {class_name}: {acc_percent:.4f}% ({class_correct}/{class_total})\")\n",
        "                per_class_acc[f\"val_acc_{class_name}\"] = acc_percent\n",
        "\n",
        "    return loss_m.avg, acc_m.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0vNzj8FKMcC"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metrics, epoch, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'metrics': metrics\n",
        "    }, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTQYwAK1syMp",
        "outputId": "40071ee7-2667-49ee-98cf-f335d2059b33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "# Use AdamW optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=config['lr'],\n",
        "    weight_decay=config['weight_decay']\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Early stopping initialization\n",
        "early_stopping = EarlyStopping(\n",
        "    patience=7,\n",
        "    verbose=True,\n",
        "    path=os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWEXTMbuKMcC",
        "outputId": "1adc9d89-cf09-4031-f0af-f6704df4ae5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "gon3xS9EKMcC",
        "outputId": "bdd2b04c-d908-458b-ddbf-35d0076f9b8c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250424_210412-qyp74dqk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/qyp74dqk' target=\"_blank\">CNN+MLP Ensemble</a></strong> to <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/qyp74dqk' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/qyp74dqk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name=\"CNN+MLP Ensemble\",\n",
        "    project=\"object_classification\",\n",
        "    config={\n",
        "        'architecture': 'BiLSTM-Attention',\n",
        "        'input_dim': config['input_dim'],\n",
        "        'seq_len': config['seq_len'],\n",
        "        'batch_size': config['batch_size'],\n",
        "        'learning_rate': config['lr'],\n",
        "        'weight_decay': config['weight_decay'],\n",
        "        'dropout': config['dropout'],\n",
        "        'hidden_dims': config['hidden_dims'],\n",
        "        'num_layers': config['num_layers']\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4pPEOUzs7WI",
        "outputId": "e3dd041d-2bcf-489e-ceb3-0a2e645736e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.8845, Train Accuracy: 59.02%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Per-class Validation Accuracy:\n",
            "  Blueball: 68.8592% (15609/22668)\n",
            "  Box: 65.2853% (14210/21766)\n",
            "  Pencilcase: 44.8167% (9913/22119)\n",
            "  Pinkball: 57.2613% (11529/20134)\n",
            "  StuffedAnimal: 35.4703% (6938/19560)\n",
            "  Tennis: 60.7053% (13978/23026)\n",
            "  Waterbottle: 45.6078% (10119/22187)\n",
            "Validation Loss: 1.6734, Validation Accuracy: 54.33%\n",
            "EarlyStopping counter: 1 out of 7\n",
            "Saved best model with validation loss: 1.6734 and accuracy: 54.33%\n"
          ]
        },
        {
          "ename": "Error",
          "evalue": "You must call wandb.init() before wandb.log()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-2f8817aa4535>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Log metrics to wandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     wandb.log({\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "best_val_acc = 0\n",
        "epochs = 3\n",
        "class_names = list(CATEGORIES.keys())\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc = train_model_with_regularization(\n",
        "        model,\n",
        "        train_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        device,\n",
        "        weight_decay=1e-3,\n",
        "        gradient_clip=1.0,\n",
        "        mixup_alpha=0.2\n",
        "    )\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "    # Validate\n",
        "    val_loss, val_acc = validate_model(model, val_loader, criterion, class_names, device)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping(val_loss, model)\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "    # Save checkpoint\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_val_loss = val_loss\n",
        "        print(f\"Saved best model with validation loss: {best_val_loss:.4f} and accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    # Log metrics to wandb\n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_loss': val_loss,\n",
        "        'val_acc': val_acc,\n",
        "        'learning_rate': optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    print(f\"End of Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prB3uz0aKMcC"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_model(model, test_loader, criterion, class_names, device):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "\n",
        "    class_correct = {class_name: 0 for class_name in class_names}\n",
        "    class_total = {class_name: 0 for class_name in class_names}\n",
        "\n",
        "    for data in test_loader:\n",
        "        inputs, targets = data\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass - now returns logits and probabilities\n",
        "        logits, probs = model(inputs)\n",
        "\n",
        "        # Compute loss using logits\n",
        "        loss = criterion(logits, targets)\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Get predictions from probabilities\n",
        "        _, predicted = torch.max(probs, 1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "        for i in range(targets.size(0)):\n",
        "            label = targets[i].item()\n",
        "            pred = predicted[i].item()\n",
        "            class_name = class_names[label]\n",
        "            class_total[class_name] += 1\n",
        "            if pred == label:\n",
        "                class_correct[class_name] += 1\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = correct / total\n",
        "\n",
        "    class_accuracy = {\n",
        "        name: class_correct[name]/class_total[name] if class_total[name] > 0 else 0\n",
        "        for name in class_names\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TEST RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc:.4f} ({correct}/{total})\")\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    for class_name in class_names:\n",
        "        print(f\" {class_name}: {class_accuracy[class_name]:.4f} ({class_correct[class_name]}/{class_total[class_name]})\")\n",
        "\n",
        "    return {\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_acc,\n",
        "        'class_accuracy': class_accuracy,\n",
        "        'predictions': all_preds,\n",
        "        'targets': all_targets,\n",
        "        'probabilities': all_probs\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "id": "BUyHcRfQUhBI",
        "outputId": "85d74c00-9e08-4656-de2c-e4800a9e38a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTrain:   8%|▊         | 2492/30809 [02:08<24:05, 19.59it/s, acc=58.9812%, loss=0.9027, lr=0.001000]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "TEST RESULTS\n",
            "==================================================\n",
            "Test Loss: 1.7264\n",
            "Test Accuracy: 0.5286 (69905/132234)\n",
            "\n",
            "Per-Class Accuracy:\n",
            " Blueball: 0.5231 (9085/17368)\n",
            " Box: 0.2100 (3738/17804)\n",
            " Pencilcase: 0.4521 (10100/22339)\n",
            " Pinkball: 0.9693 (17471/18024)\n",
            " StuffedAnimal: 0.7700 (14789/19207)\n",
            " Tennis: 0.3635 (7163/19703)\n",
            " Waterbottle: 0.4249 (7559/17789)\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>learning_rate</td><td>▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_acc</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>val_acc</td><td>▁█</td></tr><tr><td>val_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>learning_rate</td><td>0.001</td></tr><tr><td>test_accuracy</td><td>0.52865</td></tr><tr><td>test_loss</td><td>1.72642</td></tr><tr><td>train_acc</td><td>58.24069</td></tr><tr><td>train_loss</td><td>0.93832</td></tr><tr><td>val_acc</td><td>60.69439</td></tr><tr><td>val_loss</td><td>1.46313</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CNN+MLP Ensemble</strong> at: <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/qyp74dqk' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification/runs/qyp74dqk</a><br> View project at: <a href='https://wandb.ai/donggul-carnegie-mellon-university/object_classification' target=\"_blank\">https://wandb.ai/donggul-carnegie-mellon-university/object_classification</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250424_210412-qyp74dqk/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Accuracy: 0.53%\n"
          ]
        }
      ],
      "source": [
        "# model.load_state_dict(torch.load(os.path.join(config['checkpoint_dir'], 'best_model.pth')))\n",
        "\n",
        "# Test the model\n",
        "test_results = test_model(model, test_loader, criterion, class_names, device)\n",
        "\n",
        "# Log final test results to wandb\n",
        "wandb.log({\n",
        "    'test_loss': test_results['test_loss'],\n",
        "    'test_accuracy': test_results['test_accuracy'],\n",
        "    'confusion_matrix': wandb.plot.confusion_matrix(\n",
        "        probs=None,\n",
        "        y_true=test_results['targets'],\n",
        "        preds=test_results['predictions'],\n",
        "        class_names=class_names\n",
        "    )\n",
        "})\n",
        "\n",
        "# Finish wandb run\n",
        "wandb.finish()\n",
        "\n",
        "print(f\"Final Test Accuracy: {test_results['test_accuracy']:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_cJpaH4UjbX"
      },
      "outputs": [],
      "source": [
        "# You can visualize the confusion matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_confusion_matrix(conf_matrix, class_names):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        conf_matrix,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap='Blues',\n",
        "        xticklabels=class_names,\n",
        "        yticklabels=class_names\n",
        "    )\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# Plot the confusion matrix\n",
        "if detailed_results:\n",
        "    plot_confusion_matrix(detailed_results['confusion_matrix'], class_names)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}