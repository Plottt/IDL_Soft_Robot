{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11410933,"sourceType":"datasetVersion","datasetId":7146059}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, random_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport os\nfrom torchsummary import summary\nfrom torchinfo import summary\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nimport wandb\nimport torch.nn.functional as F\nimport hashlib\nfrom typing import Dict, Tuple\nimport random\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", device)","metadata":{"id":"so2Ihy3KKMb6","outputId":"5e5eeb39-3e3a-42ce-de86-1ba13f2c1a9b"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"config = {\n    'batch_size': 16,            # Reduced batch size for better regularization\n    'lr': 0.001,                # Lower learning rate for more stable training\n    'epochs': 10,                # Increased epochs (early stopping will prevent overfitting)\n    'input_dim': 22,             # Expanded input dimension for engineered features\n    'num_classes': 7,            # Same number of classes\n    'hidden_dim': 128,           # Reduced hidden dimension (more efficient)\n    'num_blocks': 2,             # Reduced number of blocks\n    'checkpoint_dir': \"/kaggle/working/\",\n    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n\n    # New parameters for regularization and training\n    'weight_decay': 1e-4,        # L2 regularization strength\n    'dropout': 0.4,              # Increased dropout rate to reduce overfitting\n    'patience': 7,               # Early stopping patience\n    'mixup_alpha': 0.2,          # Mixup augmentation strength\n    'gradient_clip': 1.0,        # Gradient clipping value\n    'seq_len': 2000,             # Sequence length for windowed data (reduced from 2000)\n    'label_smoothing': 0.1       # Label smoothing for loss function\n}\n\nprint(\"Device: \", device)","metadata":{"id":"uwfQWq5Stp4Y","outputId":"7348069c-19bb-4a81-c055-4f08c89789cb"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration Dictionary\n# config = {\n#     'batch_size': 32,\n#     'lr': 0.001,\n#     'epochs': 10,\n#     'input_dim': 9,\n#     'num_classes': 7,\n#     'hidden_dim': 512,\n#     'num_blocks': 3,\n#     'checkpoint_dir': \"/content/drive/MyDrive/IDL/Checkpoint\",\n#     'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n# }","metadata":{"id":"vH6wKOAvKMb-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define category mapping\nCATEGORIES = {\n    'Blueball': 0,\n    'Box': 1,\n    'Pencilcase': 2,\n    'Pinkball': 3,\n    'StuffedAnimal': 4,\n    'Tennis': 5,\n    'Waterbottle': 6,\n}","metadata":{"id":"5H88G2o5KMb-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Path to the folder containing the dataset files\nfolder_path = \"/kaggle/input/idl-dataset/IDL_Data\"","metadata":{"id":"MuINLJVKKMb_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stats trackers\ntotal_count = 0\nkept_count = 0\nvalid_file_count = 0\nskipped_due_to_missing_waypoints = 0","metadata":{"id":"qXeSE84nKMb_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WAYPOINTS = [\n    (30, -30), (30, 30), (15, -30), (15, 30),\n    (0, -30), (0, 30), (-15, -30), (-15, 30),\n    (-30, -30), (-30, 30), (-30, -30), (30, -30),\n    (30, 30), (-30, 30)\n]\n","metadata":{"id":"FWAborBwKMb_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Load and label dataset\ndef load_and_label_file(file_path, file_name):\n    global total_count\n    category = next((key for key in CATEGORIES if key in file_name), None)\n    if category is None:\n        return pd.DataFrame()\n\n    data = []\n    with open(file_path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split(',')\n            if len(parts) == 10:\n                try:\n                    timestamp = parts[0]\n                    microsec = int(parts[1])\n                    x = float(parts[2])\n                    y = float(parts[3])\n                    x_target = float(parts[4])\n                    y_target = float(parts[5])\n                    pwm1 = int(parts[6])\n                    pwm2 = int(parts[7])\n                    pwm3 = int(parts[8])\n                    pwm4 = int(parts[9])\n                    total_count += 1\n\n                    data.append([\n                        timestamp, microsec, x, y, x_target, y_target,\n                        pwm1, pwm2, pwm3, pwm4, category, CATEGORIES[category]\n                    ])\n                except ValueError:\n                    continue\n\n    return pd.DataFrame(data, columns=[\n        \"timestamp\", \"microseconds\", \"x\", \"y\", \"x_target\", \"y_target\",\n        \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"category\", \"label\"\n    ])","metadata":{"id":"zVuq0D5TKMb_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Assign sequential waypoint numbers\ndef assign_sequential_waypoints(df, tol=1.0):\n    df = df.reset_index(drop=True)\n    wp_index = 0\n    assigned_wp = []\n\n    for i in range(len(df)):\n        x_t, y_t = df.loc[i, \"x_target\"], df.loc[i, \"y_target\"]\n        current_expected = WAYPOINTS[wp_index]\n\n        if np.isclose(x_t, current_expected[0], atol=tol) and np.isclose(y_t, current_expected[1], atol=tol):\n            assigned_wp.append(wp_index)\n        else:\n            if wp_index + 1 < len(WAYPOINTS):\n                next_expected = WAYPOINTS[wp_index + 1]\n                if np.isclose(x_t, next_expected[0], atol=tol) and np.isclose(y_t, next_expected[1], atol=tol):\n                    wp_index += 1\n                    assigned_wp.append(wp_index)\n                else:\n                    assigned_wp.append(wp_index)\n            else:\n                assigned_wp.append(wp_index)\n\n    df[\"waypoint_number\"] = assigned_wp\n    return df","metadata":{"id":"naFDKYoGKMb_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Filter out rows where y <= 0\ndef filter_by_y(df):\n    global kept_count\n    filtered = df[df[\"y\"] > 15].reset_index(drop=True)\n    kept_count += len(filtered)\n    return filtered","metadata":{"id":"MfXzeHC4KMb_"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_file(file_path, file_name):\n    global valid_file_count\n\n    # Step 1: Load and label\n    df = load_and_label_file(file_path, file_name)\n    if df.empty:\n        return pd.DataFrame()\n\n    # Step 2: Assign waypoint numbers\n    df = assign_sequential_waypoints(df)\n\n    # üìå Show how many waypoints existed before filtering\n    waypoint_count_before = df[\"waypoint_number\"].nunique()\n    print(f\"\\nüìå {file_name} ‚Üí {waypoint_count_before} waypoints BEFORE filtering\")\n\n    # Step 3: Filter out rows where y ‚â§ 0\n    df = filter_by_y(df)\n\n    # üìå Show how many remain after filtering\n    waypoint_count_after = df[\"waypoint_number\"].nunique()\n    print(f\"üìå {file_name} ‚Üí {waypoint_count_after} waypoints AFTER filtering\")\n\n    # Count as valid if any data was kept\n    if not df.empty:\n        valid_file_count += 1\n\n    return df","metadata":{"id":"Ogu08rbDKMcA"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Step 5: Process all .txt files\n# all_data = pd.DataFrame()\n\n# for file_name in os.listdir(folder_path):\n#     if not file_name.endswith(\".txt\") or file_name.startswith(\".\"):\n#         continue\n#     file_path = os.path.join(folder_path, file_name)\n#     df = process_file(file_path, file_name)\n#     if not df.empty:\n#         all_data = pd.concat([all_data, df], ignore_index=True)\n\n# # Step 6: Summary\n# print(\"\\nüìÑ Summary:\")\n# print(f\"Total files scanned: {len([f for f in os.listdir(folder_path) if f.endswith('.txt')])}\")\n# print(f\"‚úÖ Files with 14 valid waypoints: {valid_file_count}\")\n# print(f\"‚ö†Ô∏è Skipped due to missing waypoints: {skipped_due_to_missing_waypoints}\")\n# print(f\"üìä Data points before filtering: {total_count}\")\n# print(f\"‚úÖ Data points after filtering: {kept_count}\")\n# print(f\"üö´ Dropped data points: {total_count - kept_count}\")","metadata":{"id":"7CBO_ta8KMcA"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(all_data.head(1350))","metadata":{"id":"B8u06Sh1KMcA"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np","metadata":{"id":"yuHhrzRwKMcB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class ObjectSensorDataset(Dataset):\n#     def __init__(self, df):\n#         features = df[[\"x\", \"y\", \"x_target\", \"y_target\", \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"microseconds\"]].values\n#         labels = df[\"label\"].values\n\n#         self.X = torch.tensor(features, dtype=torch.float32)\n#         self.y = torch.tensor(labels, dtype=torch.long)\n\n#     def __len__(self):\n#         return len(self.y)\n\n#     def __getitem__(self, idx):\n#         return self.X[idx], self.y[idx]","metadata":{"id":"6qjmfJ-JKMcB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Normalize features before split\n# features_to_scale = [\"x\", \"y\", \"x_target\", \"y_target\", \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"microseconds\"]\n# scaler = StandardScaler()\n# all_data[features_to_scale] = scaler.fit_transform(all_data[features_to_scale])\n\n# # üß™ Split into train/val/test with stratified sampling\n# train_df, temp_df = train_test_split(\n#     all_data, test_size=0.3, stratify=all_data[\"label\"], random_state=42\n# )\n# val_df, test_df = train_test_split(\n#     temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42\n# )\n\n# # üì¶ Create datasets\n# train_dataset = ObjectSensorDataset(train_df)\n# val_dataset = ObjectSensorDataset(val_df)\n# test_dataset = ObjectSensorDataset(test_df)","metadata":{"id":"p13o2gWwkDee"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict, Counter\n\ndef build_datasets(data_dir: str):\n    data_dir = Path(data_dir)\n    file_paths = list(data_dir.glob(\"*.txt\"))\n    random.seed(42)\n\n    # 1. Group files by object class\n    class_to_files = defaultdict(list)\n    for file_path in file_paths:\n        for class_name in CATEGORIES:\n            if class_name in file_path.name:\n                class_to_files[class_name].append(file_path)\n                break\n\n    # 2. Stratified split (each class in train/val/test)\n    train_files, val_files, test_files = [], [], []\n    for class_name, files in class_to_files.items():\n        random.shuffle(files)\n        n = len(files)\n        train_split = int(0.65 * n)\n        val_split = int(0.85 * n)\n        train_files += files[:train_split]\n        val_files += files[train_split:val_split]\n        test_files += files[val_split:]\n\n    print(\"üîç Per-class file counts:\")\n    for cls in CATEGORIES:\n        print(f\"  {cls:<15} ‚Üí {len(class_to_files[cls])} total files\")\n\n    print(\"\\n‚úÖ Final split file counts:\")\n    print(f\"Train: {len(train_files)}\")\n    print(f\"Val:   {len(val_files)}\")\n    print(f\"Test:  {len(test_files)}\")\n\n    # 3. Process each split\n    def process_file_list(file_list):\n        dfs = []\n        for fp in file_list:\n            df = process_file(fp, fp.name)\n            if not df.empty:\n                dfs.append(df)\n        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n\n    print(\"Processing train files\")\n    train_df = process_file_list(train_files)\n    print(\"Processing validation files\")\n    val_df = process_file_list(val_files)\n    print(\"Processing test files\")\n    test_df = process_file_list(test_files)\n\n    return train_df, val_df, test_df\n\n\n# üì¶ Run everything\ntrain_df, val_df, test_df = build_datasets(folder_path)","metadata":{"id":"vg1SVbqJkMaR","outputId":"75347ae7-9520-4993-c9b1-cfaceb441823"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_enhanced_features(df):\n    \"\"\"Extract additional meaningful features from the raw sensor data\"\"\"\n    # Create a copy to avoid modifying the original\n    enhanced_df = df.copy()\n\n    # Compute deltas (changes between consecutive measurements)\n    enhanced_df['x_delta'] = enhanced_df.groupby(['category', 'waypoint_number'])['x'].diff().fillna(0)\n    enhanced_df['y_delta'] = enhanced_df.groupby(['category', 'waypoint_number'])['y'].diff().fillna(0)\n\n    # Distance to target (error signal that drives the controller)\n    enhanced_df['x_error'] = enhanced_df['x_target'] - enhanced_df['x']\n    enhanced_df['y_error'] = enhanced_df['y_target'] - enhanced_df['y']\n\n    # Magnitude of error and movement\n    enhanced_df['error_magnitude'] = np.sqrt(enhanced_df['x_error']**2 + enhanced_df['y_error']**2)\n    enhanced_df['movement_magnitude'] = np.sqrt(enhanced_df['x_delta']**2 + enhanced_df['y_delta']**2)\n\n    # Control effort features (sum and differences of PWM signals)\n    enhanced_df['total_pwm'] = enhanced_df['pwm1'] + enhanced_df['pwm2'] + enhanced_df['pwm3'] + enhanced_df['pwm4']\n    enhanced_df['pwm_x_diff'] = enhanced_df['pwm1'] - enhanced_df['pwm3']  # Assuming these control x-axis\n    enhanced_df['pwm_y_diff'] = enhanced_df['pwm2'] - enhanced_df['pwm4']  # Assuming these control y-axis\n\n    # Interaction features (product of error and control)\n    enhanced_df['x_control_response'] = enhanced_df['x_error'] * enhanced_df['pwm_x_diff']\n    enhanced_df['y_control_response'] = enhanced_df['y_error'] * enhanced_df['pwm_y_diff']\n\n    # Time derivatives of error (how quickly error is changing)\n    enhanced_df['x_error_delta'] = enhanced_df.groupby(['category', 'waypoint_number'])['x_error'].diff().fillna(0)\n    enhanced_df['y_error_delta'] = enhanced_df.groupby(['category', 'waypoint_number'])['y_error'].diff().fillna(0)\n\n    return enhanced_df\n\n# Process your dataframes with the enhanced features\ntrain_df_enhanced = extract_enhanced_features(train_df)\nval_df_enhanced = extract_enhanced_features(val_df)\ntest_df_enhanced = extract_enhanced_features(test_df)","metadata":{"id":"K4MdJKhSsJg5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EnhancedWindowedDataset(torch.utils.data.Dataset):\n    def __init__(self, df, seq_len=2000, augment=False, noise_scale=0.02, time_warp_scale=0.1):\n        self.seq_len = seq_len\n        self.df = df.reset_index(drop=True)  # Use the enhanced dataframe\n        self.augment = augment\n        self.noise_scale = noise_scale\n        self.time_warp_scale = time_warp_scale\n\n        # Select all features including the new engineered features\n        feature_columns = [\n            # Original features\n            \"x\", \"y\", \"x_target\", \"y_target\", \"pwm1\", \"pwm2\", \"pwm3\", \"pwm4\", \"waypoint_number\",\n            # New engineered features\n            \"x_delta\", \"y_delta\", \"x_error\", \"y_error\", \"error_magnitude\", \"movement_magnitude\",\n            \"total_pwm\", \"pwm_x_diff\", \"pwm_y_diff\", \"x_control_response\", \"y_control_response\",\n            \"x_error_delta\", \"y_error_delta\"\n        ]\n\n        # Filter to only include columns that exist in the dataframe\n        self.feature_columns = [col for col in feature_columns if col in self.df.columns]\n        self.features = self.df[self.feature_columns].values.astype(np.float32)\n\n        # Normalize features\n        self.feature_means = np.mean(self.features, axis=0)\n        self.feature_stds = np.std(self.features, axis=0) + 1e-6  # Avoid division by zero\n        self.features = (self.features - self.feature_means) / self.feature_stds\n\n        # Label per row\n        self.labels = self.df[\"label\"].values.astype(np.int64)\n\n    def __len__(self):\n        return len(self.df) - self.seq_len + 1\n\n    def __getitem__(self, idx):\n        x = self.features[idx:idx + self.seq_len]  # (seq_len, input_dim)\n        y = self.labels[idx + self.seq_len - 1]\n\n        # Apply augmentation if enabled\n        if self.augment and np.random.random() > 0.5:\n            # Get dimensions once at the start\n            seq_len, feat_dim = x.shape\n\n            # Add random noise\n            noise = np.random.normal(0, self.noise_scale, x.shape)\n            x = x + noise\n\n            # Time warping (randomly stretch or compress parts of the sequence)\n            if np.random.random() > 0.7:\n                time_indices = np.arange(seq_len)\n                warp = np.sin(np.linspace(0, 3*np.pi, seq_len)) * self.time_warp_scale\n                warped_indices = np.clip(time_indices + warp, 0, seq_len-1).astype(int)\n                x = x[warped_indices, :]\n\n            # Random feature masking (occasionally zero out features)\n            if np.random.random() > 0.8:\n                mask_idx = np.random.choice(feat_dim, size=int(feat_dim * 0.1), replace=False)\n                x[:, mask_idx] = 0\n\n        x_tensor = torch.tensor(x, dtype=torch.float32)\n        y_tensor = torch.tensor(y, dtype=torch.long)\n        return x_tensor, y_tensor","metadata":{"id":"mi8NwL56sOo8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seq_len = config['seq_len']  # Adjust based on your needs\n\n# Create datasets with augmentation\ntrain_dataset = EnhancedWindowedDataset(train_df_enhanced, seq_len=seq_len, augment=True)\nval_dataset = EnhancedWindowedDataset(val_df_enhanced, seq_len=seq_len)\ntest_dataset = EnhancedWindowedDataset(test_df_enhanced, seq_len=seq_len)\n\n# Create data loaders with smaller batch size\nbatch_size = 16  # Smaller batch size for better regularization\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"Ho-BZBjrsVuz"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# class CNNBlock(nn.Module):\n#     def __init__(self, in_channels, out_channels, kernel_size=3, dropout=0.2):\n#         super(CNNBlock, self).__init__()\n#         self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2)\n#         self.bn = nn.BatchNorm1d(out_channels)\n#         self.relu = nn.ReLU()\n#         self.dropout = nn.Dropout(dropout)\n\n#     def forward(self, x):\n#         return self.dropout(self.relu(self.bn(self.conv(x))))\n\nclass CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, dropout=0.2):\n        super(CNNBlock, self).__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size//2)\n        self.bn = nn.BatchNorm1d(out_channels)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        \n        # For skip connection when dimensions don't match\n        self.use_residual = (in_channels == out_channels)\n        if not self.use_residual:\n            self.residual_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n            self.residual_bn = nn.BatchNorm1d(out_channels)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        # Apply skip connection if possible\n        if self.use_residual:\n            out = out + residual\n        else:\n            # When dimensions don't match, transform the residual\n            transformed_residual = self.residual_bn(self.residual_conv(residual))\n            out = out + transformed_residual\n            \n        return out\n\nclass CNNModel(nn.Module):\n    def __init__(self, input_dim, num_classes, dropout=0.3):\n        super(CNNModel, self).__init__()\n\n        self.conv_layers = nn.Sequential(\n            CNNBlock(input_dim, 32, kernel_size=5, dropout=dropout/2),\n            CNNBlock(32, 64, kernel_size=5, dropout=dropout/2),\n            CNNBlock(64, 128, kernel_size=3, dropout=dropout),\n            nn.AdaptiveAvgPool1d(output_size=1)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(32, num_classes)\n        )\n\n    def forward(self, x):\n        # x shape: [batch_size, seq_len, features]\n        x = x.permute(0, 2, 1)  # [batch_size, features, seq_len]\n        x = self.conv_layers(x)\n        x = self.classifier(x)\n        return x\n\n# class RNNModel(nn.Module):\n#     def __init__(self, input_dim, num_classes, hidden_dim=128, num_layers=2, dropout=0.3):\n#         super(RNNModel, self).__init__()\n\n#         self.lstm = nn.LSTM(\n#             input_size=input_dim,\n#             hidden_size=hidden_dim,\n#             num_layers=num_layers,\n#             batch_first=True,\n#             dropout=dropout if num_layers > 1 else 0,\n#             bidirectional=True\n#         )\n\n#         self.attention = nn.Sequential(\n#             nn.Linear(hidden_dim*2, 1),  # *2 for bidirectional\n#             nn.Tanh()\n#         )\n\n#         self.classifier = nn.Sequential(\n#             nn.Linear(hidden_dim*2, hidden_dim),\n#             nn.ReLU(),\n#             nn.Dropout(dropout),\n#             nn.Linear(hidden_dim, num_classes)\n#         )\n\n#     def forward(self, x):\n#         # x shape: [batch_size, seq_len, features]\n#         lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_dim*2]\n\n#         # Attention mechanism\n#         attention_weights = self.attention(lstm_out).squeeze(-1)  # [batch_size, seq_len]\n#         attention_weights = F.softmax(attention_weights, dim=1).unsqueeze(1)  # [batch_size, 1, seq_len]\n\n#         # Apply attention weights\n#         context = torch.bmm(attention_weights, lstm_out).squeeze(1)  # [batch_size, hidden_dim*2]\n\n#         # Classification\n#         output = self.classifier(context)\n#         return output\n\nclass RNNModel(nn.Module):\n    def __init__(self, input_dim, num_classes, hidden_dim=128, num_layers=2, dropout=0.3):\n        super(RNNModel, self).__init__()\n\n        self.lstm = nn.LSTM(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0,\n            bidirectional=True\n        )\n\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_dim*2, 1),  # *2 for bidirectional\n            nn.Tanh()\n        )\n\n        # First layer of classifier\n        self.fc1 = nn.Linear(hidden_dim*2, hidden_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        \n        # Second layer of classifier\n        self.fc2 = nn.Linear(hidden_dim, num_classes)\n        \n        # For skip connection in attention-to-output\n        self.shortcut = nn.Linear(hidden_dim*2, num_classes)\n\n    def forward(self, x):\n        # x shape: [batch_size, seq_len, features]\n        lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_dim*2]\n\n        # Attention mechanism\n        attention_weights = self.attention(lstm_out).squeeze(-1)  # [batch_size, seq_len]\n        attention_weights = F.softmax(attention_weights, dim=1).unsqueeze(1)  # [batch_size, 1, seq_len]\n\n        # Apply attention weights\n        context = torch.bmm(attention_weights, lstm_out).squeeze(1)  # [batch_size, hidden_dim*2]\n\n        # Main path\n        x1 = self.fc1(context)\n        x1 = self.relu(x1)\n        x1 = self.dropout(x1)\n        x1 = self.fc2(x1)\n        \n        # Skip connection path\n        x2 = self.shortcut(context)\n        \n        # Combine main and skip paths\n        output = x1 + x2\n        \n        return output\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, input_dim, num_classes, dropout=0.3):\n        super(EnsembleModel, self).__init__()\n\n        # Component models\n        self.cnn_model = CNNModel(input_dim, num_classes, dropout)\n        self.rnn_model = RNNModel(input_dim, num_classes, dropout=dropout)\n\n        # Meta-learner (combination weights)\n        self.meta_learner = nn.Sequential(\n            nn.Linear(num_classes*2, num_classes),\n            nn.Dropout(dropout/2)\n        )\n\n    def forward(self, x):\n        # Get predictions from individual models\n        cnn_out = self.cnn_model(x)\n        rnn_out = self.rnn_model(x)\n\n        # Concatenate predictions\n        combined = torch.cat((cnn_out, rnn_out), dim=1)\n\n        # Meta-learner combines predictions\n        final_out = self.meta_learner(combined)\n        probs = F.softmax(final_out, dim=1)\n\n        return {\"feats\": combined, \"out\": probs}\n\n\ninput_dim = train_dataset.features.shape[1]  # Number of features including engineered ones\nnum_classes = config['num_classes']  # Number of object classes\ndropout = config['dropout']\n\nmodel = EnsembleModel(\n    input_dim=input_dim,\n    num_classes=num_classes,\n    dropout=dropout\n).to(device)\n\n# Print model summary\nfrom torchinfo import summary\nsummary(model, input_data=torch.zeros(batch_size, seq_len, input_dim).to(device))","metadata":{"id":"7lURaiGrsHZN","outputId":"5e47b23d-fa0b-43c0-ca22-07e497909011"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"id":"2VElzMZvKMcB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res","metadata":{"id":"EiPYAO1zKMcB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = float('inf')\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_loss, model):\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decreases.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"id":"GsUsF7_osqsP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def train_model(model, train_loader, criterion, optimizer, device):\n#     model.train()\n#     loss_m = AverageMeter()\n#     acc_m = AverageMeter()\n#     batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n\n#     for i, data in enumerate(train_loader):\n#         optimizer.zero_grad()\n#         x, y = data\n#         x, y = x.to(device), y.to(device)\n#         outputs = model(x)\n#         loss = criterion(outputs['out'], y)\n#         loss.backward()\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n#         optimizer.step()\n\n#         acc = accuracy(outputs['out'], y)[0].item()\n#         loss_m.update(loss.item())\n#         acc_m.update(acc)\n\n#         batch_bar.set_postfix(\n#             loss=\"{:.04f}\".format(float(loss_m.avg)),\n#             acc=\"{:.04f}%\".format(float(acc_m.avg)),\n#             lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr']))\n#         )\n#         batch_bar.update()\n\n#         del x, y, outputs, loss\n#         torch.cuda.empty_cache()\n\n#     batch_bar.close()\n#     return loss_m.avg, acc_m.avg","metadata":{"id":"DLwyZLQLKMcB"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model_with_regularization(model, train_loader, criterion, optimizer, device,\n                                   weight_decay=1e-4, gradient_clip=1.0, mixup_alpha=0.2):\n    model.train()\n    loss_m = AverageMeter()\n    acc_m = AverageMeter()\n    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n\n    for i, data in enumerate(train_loader):\n        optimizer.zero_grad()\n        x, y = data\n        batch_size = x.size(0)\n\n        # Move data to device first\n        x, y = x.to(device), y.to(device)\n\n        # Apply Mixup (data augmentation technique)\n        if mixup_alpha > 0:\n            lam = np.random.beta(mixup_alpha, mixup_alpha)\n            index = torch.randperm(batch_size).to(device)\n            x = lam * x + (1 - lam) * x[index]\n\n        # Forward pass\n        outputs = model(x)\n\n        # Compute loss\n        if mixup_alpha > 0:\n            loss = lam * criterion(outputs['out'], y) + (1 - lam) * criterion(outputs['out'], y[index])\n        else:\n            loss = criterion(outputs['out'], y)\n\n        # Add L2 regularization explicitly if needed (in addition to weight_decay in optimizer)\n        l2_reg = torch.tensor(0., device=device)\n        for param in model.parameters():\n            l2_reg += torch.norm(param, 2)\n        loss += weight_decay * l2_reg\n\n        # Backward pass and optimization\n        loss.backward()\n\n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n\n        optimizer.step()\n\n        # Compute accuracy\n        acc = accuracy(outputs['out'], y)[0].item()\n        loss_m.update(loss.item())\n        acc_m.update(acc)\n\n        batch_bar.set_postfix(\n            loss=\"{:.04f}\".format(float(loss_m.avg)),\n            acc=\"{:.04f}%\".format(float(acc_m.avg)),\n            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr']))\n        )\n        batch_bar.update()\n\n        del x, y, outputs, loss\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n    return loss_m.avg, acc_m.avg","metadata":{"id":"Bfr3msGHstv-"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef validate_model(model, val_loader, criterion, class_names, device):\n    model.eval()\n    loss_m = AverageMeter()\n    acc_m = AverageMeter()\n    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n\n    all_preds = []\n    all_targets = []\n\n    for i, data in enumerate(val_loader):\n        x, y = data\n        x, y = x.to(device), y.to(device)\n        outputs = model(x)\n        loss = criterion(outputs['out'], y)\n\n        acc = accuracy(outputs['out'], y)[0].item()\n\n        _, predicted = torch.max(outputs['out'], 1)\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(y.cpu().numpy())\n\n        loss_m.update(loss.item())\n        acc_m.update(acc)\n\n        batch_bar.set_postfix(\n            loss=\"{:.04f}\".format(float(loss_m.avg)),\n            acc=\"{:.04f}%\".format(float(acc_m.avg))\n        )\n        batch_bar.update()\n\n        del x, y, outputs, loss\n        torch.cuda.empty_cache()\n\n    batch_bar.close()\n\n    if class_names:\n        print(\"\\nPer-class Validation Accuracy:\")\n        per_class_acc = {}\n        for i, class_name in enumerate(class_names):\n            class_mask = (np.array(all_targets) == i)\n            if np.sum(class_mask) > 0:\n                class_correct = np.sum((np.array(all_preds)[class_mask] == i))\n                class_total = np.sum(class_mask)\n                acc_percent = 100 * class_correct / class_total\n                print(f\"  {class_name}: {acc_percent:.4f}% ({class_correct}/{class_total})\")\n                per_class_acc[f\"val_acc_{class_name}\"] = acc_percent\n\n    return loss_m.avg, acc_m.avg","metadata":{"id":"VZO5iNDzKMcC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_model(model, optimizer, scheduler, metrics, epoch, path):\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n        'metrics': metrics\n    }, path)","metadata":{"id":"g0vNzj8FKMcC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n# Use AdamW optimizer with weight decay\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=config['lr'],  # Lower learning rate\n    weight_decay=config['weight_decay']\n)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',\n    factor=0.5,\n    patience=3,\n    min_lr=1e-6,\n    verbose=True\n)\n\n# Early stopping initialization\nearly_stopping = EarlyStopping(\n    patience=3,\n    verbose=True,\n    path=os.path.join(config['checkpoint_dir'], 'best_model.pth')\n)","metadata":{"id":"rTQYwAK1syMp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # from sklearn.utils.class_weight import compute_class_weight\n# # import numpy as np\n\n# # class_weights = compute_class_weight(\n# #     class_weight='balanced',\n# #     classes=np.unique(train_df['label']),\n# #     y=train_df['label']\n# # )\n# # class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n\n# criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n# # # Define CrossEntropyLoss as the criterion\n# # criterion = nn.CrossEntropyLoss(\n# #     label_smoothing=0.1\n# # )\n\n# # Initialize optimizer with AdamW\n# # optimizer = torch.optim.AdamW(\n# #     model.parameters(),\n# #     lr=config['lr'],\n# #     weight_decay=1e-4\n# # )\n\n# optimizer = torch.optim.SGD(\n#     model.parameters(),\n#     lr=config['lr'],\n#     momentum=0.9,\n#     weight_decay=1e-4\n# )\n\n# # Learning rate scheduler\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n#     optimizer,\n#     mode='min',\n#     factor=0.5,\n#     patience=3,\n#     min_lr=1e-6,\n#     verbose=True\n# )","metadata":{"id":"3naM25ADKMcC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\n\n# Intialize wandb\nwandb.login(key=\"\") # API Key is in your wandb account, under settings (wandb.ai/settings)","metadata":{"id":"tWEXTMbuKMcC","outputId":"602c565c-ca50-4eb4-c90c-e9aaa0a3bdbf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run = wandb.init(\n    name=\"ensemble_model\",\n    project=\"object_classification\",\n    config={\n        'architecture': 'CNN-RNN-Ensemble',\n        'input_dim': input_dim,\n        'seq_len': seq_len,\n        'batch_size': batch_size,\n        'learning_rate': optimizer.param_groups[0]['lr'],\n        'weight_decay': 1e-4,\n        'dropout': dropout,\n        'mixup_alpha': 0.2,\n        'gradient_clip': 1.0\n    }\n)\n","metadata":{"id":"gon3xS9EKMcC","outputId":"53d2546b-25f8-4f38-d919-9fe875474a86"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_val_loss = float('inf')\nbest_val_acc = 0\nepochs = 10\nclass_names = list(CATEGORIES.keys())\n\nfor epoch in range(epochs):\n    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n\n    # Train\n    train_loss, train_acc = train_model_with_regularization(\n        model,\n        train_loader,\n        criterion,\n        optimizer,\n        device,\n        weight_decay=1e-3,\n        gradient_clip=1.0,\n        mixup_alpha=0.2\n    )\n    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n\n    # Validate\n    val_loss, val_acc = validate_model(model, val_loader, criterion, class_names, device)\n    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n\n    # Update learning rate\n    scheduler.step(val_loss)\n\n    # Early stopping\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\"Early stopping triggered\")\n        break\n\n    # Save checkpoint\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_val_loss = val_loss\n        print(f\"Saved best model with validation loss: {best_val_loss:.4f} and accuracy: {best_val_acc:.2f}%\")\n\n    # Log metrics to wandb\n    wandb.log({\n        'epoch': epoch + 1,\n        'train_loss': train_loss,\n        'train_acc': train_acc,\n        'val_loss': val_loss,\n        'val_acc': val_acc,\n        'learning_rate': optimizer.param_groups[0]['lr']\n    })\n\n    print(f\"End of Epoch {epoch+1}/{epochs}\")\n\nprint(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")","metadata":{"id":"t4pPEOUzs7WI","outputId":"cce2b01a-5e2e-4124-f2ac-d06ebc820aab"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Training Loop\n# best_val_loss = float('inf')\n# best_val_acc = 0\n# class_names = list(CATEGORIES.keys())\n\n# for epoch in range(config['epochs']):\n#     print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n\n#     train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, config['device'])\n#     print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n\n#     val_loss, val_acc = validate_model(model, val_loader, criterion, class_names, config['device'])\n#     print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n\n#     scheduler.step(val_loss)\n#     curr_lr = optimizer.param_groups[0]['lr']\n\n#     if val_loss < best_val_loss:\n#         best_val_loss = val_loss\n#         best_val_acc = val_acc\n#         best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n#         torch.save({\n#             'epoch': epoch,\n#             'model_state_dict': model.state_dict(),\n#             'optimizer_state_dict': optimizer.state_dict(),\n#             'val_loss': val_loss,\n#             'val_acc': val_acc,\n#         }, best_model_path)\n#         wandb.save(best_model_path)\n#         print(f\"Saved best model with validation loss: {best_val_loss:.4f} and accuracy: {best_val_acc:.2f}%\")\n\n#     last_model_path = os.path.join(config['checkpoint_dir'], f'model_epoch_{epoch+1}.pth')\n#     torch.save(model.state_dict(), last_model_path)\n#     wandb.save(last_model_path)\n#     print(f\"Saved model for epoch {epoch+1}\")\n\n#     wandb.log({\n#         'epoch': epoch + 1,\n#         'train_loss': train_loss,\n#         'train_acc': train_acc,\n#         'val_loss': val_loss,\n#         'val_acc': val_acc,\n#         'learning_rate': curr_lr\n#     }, step=epoch)\n\n#     print(f\"End of Epoch {epoch+1}/{config['epochs']}\")\n\n# print(f\"\\nTraining complete! Best validation accuracy: {best_val_acc:.2f}%\")","metadata":{"id":"C8hA-tExKMcC","outputId":"e0db2e31-d7ec-4c87-fe99-7a655cb229d2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef test_model(model, test_loader, criterion, class_names, device, checkpoint_dir=None):\n    model.eval()\n    test_loss = 0.0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_targets = []\n    all_probs = []\n\n    class_correct = {class_name: 0 for class_name in class_names}\n    class_total = {class_name: 0 for class_name in class_names}\n\n    for data in test_loader:\n        inputs, targets = data\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        outputs_for_loss = outputs['out'] if isinstance(outputs, dict) and 'out' in outputs else outputs\n        loss = criterion(outputs_for_loss, targets)\n        test_loss += loss.item() * inputs.size(0)\n\n        probs = torch.nn.functional.softmax(outputs_for_loss, dim=1)\n        _, predicted = torch.max(outputs_for_loss, 1)\n\n        total += targets.size(0)\n        correct += (predicted == targets).sum().item()\n\n        for i in range(targets.size(0)):\n            label = targets[i].item()\n            pred = predicted[i].item()\n            class_name = class_names[label]\n            class_total[class_name] += 1\n            if pred == label:\n                class_correct[class_name] += 1\n\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())\n        all_probs.extend(probs.cpu().numpy())\n\n    test_loss /= len(test_loader.dataset)\n    test_acc = correct / total\n\n    class_accuracy = {\n        name: class_correct[name]/class_total[name] if class_total[name] > 0 else 0\n        for name in class_names\n    }\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"TEST RESULTS\")\n    print(\"=\"*50)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc:.4f} ({correct}/{total})\")\n    print(\"\\nPer-Class Accuracy:\")\n    for class_name in class_names:\n        print(f\" {class_name}: {class_accuracy[class_name]:.4f} ({class_correct[class_name]}/{class_total[class_name]})\")\n\n    return {\n        'test_loss': test_loss,\n        'test_accuracy': test_acc,\n        'class_accuracy': class_accuracy,\n        'predictions': all_preds,\n        'targets': all_targets,\n        'probabilities': all_probs\n    }\n","metadata":{"id":"prB3uz0aKMcC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.load_state_dict(torch.load(os.path.join(config['checkpoint_dir'], 'best_model.pth')))\n\n# Test the model\ntest_results = test_model(model, test_loader, criterion, class_names, device)\n\n# Log final test results to wandb\nwandb.log({\n    'test_loss': test_results['test_loss'],\n    'test_accuracy': test_results['test_accuracy'],\n    'confusion_matrix': wandb.plot.confusion_matrix(\n        probs=None,\n        y_true=test_results['targets'],\n        preds=test_results['predictions'],\n        class_names=class_names\n    )\n})\n\n# Finish wandb run\nwandb.finish()\n\nprint(f\"Final Test Accuracy: {test_results['test_accuracy']:.2f}%\")","metadata":{"id":"BUyHcRfQUhBI","outputId":"aae07e5e-431b-4f25-9f4e-f0fe34da0229"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"x_cJpaH4UjbX"},"outputs":[],"execution_count":null}]}